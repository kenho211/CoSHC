{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L16UMaFg6hUn"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qo51z1qY3mwg",
        "outputId": "d64cbe97-3883-4c57-932f-51743bf9192b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhxYqtRb8wAN",
        "outputId": "c91d3a62-1353-4a68-ffe2-19dcdf45d460"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/MyDrive/CoSHC/data/python.zip\n",
            "   creating: /content/data/python/\n",
            "   creating: /content/data/python/final/\n",
            "   creating: /content/data/python/final/jsonl/\n",
            "   creating: /content/data/python/final/jsonl/train/\n",
            "  inflating: /content/data/python/final/jsonl/train/python_train_9.jsonl.gz  \n",
            "  inflating: /content/data/python/final/jsonl/train/python_train_12.jsonl.gz  \n",
            "  inflating: /content/data/python/final/jsonl/train/python_train_10.jsonl.gz  \n",
            "  inflating: /content/data/python/final/jsonl/train/python_train_0.jsonl.gz  \n",
            "  inflating: /content/data/python/final/jsonl/train/python_train_6.jsonl.gz  \n",
            "  inflating: /content/data/python/final/jsonl/train/python_train_2.jsonl.gz  \n",
            "  inflating: /content/data/python/final/jsonl/train/python_train_4.jsonl.gz  \n",
            "  inflating: /content/data/python/final/jsonl/train/python_train_8.jsonl.gz  \n",
            "  inflating: /content/data/python/final/jsonl/train/python_train_11.jsonl.gz  \n",
            "  inflating: /content/data/python/final/jsonl/train/python_train_5.jsonl.gz  \n",
            "  inflating: /content/data/python/final/jsonl/train/python_train_13.jsonl.gz  \n",
            "  inflating: /content/data/python/final/jsonl/train/python_train_3.jsonl.gz  \n",
            "  inflating: /content/data/python/final/jsonl/train/python_train_1.jsonl.gz  \n",
            "  inflating: /content/data/python/final/jsonl/train/python_train_7.jsonl.gz  \n",
            "   creating: /content/data/python/final/jsonl/test/\n",
            "  inflating: /content/data/python/final/jsonl/test/python_test_0.jsonl.gz  \n",
            "   creating: /content/data/python/final/jsonl/valid/\n",
            "  inflating: /content/data/python/final/jsonl/valid/python_valid_0.jsonl.gz  \n",
            "  inflating: /content/data/python_dedupe_definitions_v2.pkl  \n",
            "  inflating: /content/data/python_licenses.pkl  \n",
            "/content/data\n",
            "python\n",
            "gzip: python/final/jsonl/test/python_test_0.jsonl.gz: No such file or directory\n",
            "gzip: python/final/jsonl/valid/python_valid_0.jsonl.gz: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "LANG = \"python\"\n",
        "\n",
        "!mkdir /content/data\n",
        "!unzip /content/drive/MyDrive/CoSHC/data/{LANG}.zip -d /content/data\n",
        "\n",
        "!cp -r /content/drive/MyDrive/CoSHC/data/{LANG} /content/data/\n",
        "!cp /content/drive/MyDrive/CoSHC/data/preprocess.py /content/data/preprocess.py\n",
        "\n",
        "%cd /content/data\n",
        "!python preprocess.py --languages {LANG}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nfhj-UYIzjya"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/NVIDIA/apex\n",
        "# %cd /content/apex\n",
        "\n",
        "# !pip install -v --disable-pip-version-check --no-build-isolation --no-cache-dir ./\n",
        "# if pip >= 23.1 (ref: https://pip.pypa.io/en/stable/news/#v23-1) which supports multiple `--config-settings` with the same key...\n",
        "# !pip install -v --disable-pip-version-check --no-cache-dir --no-build-isolation --config-settings \"--build-option=--cpp_ext\" --config-settings \"--build-option=--cuda_ext\" ./"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Jr9volNv17tZ",
        "outputId": "b632878b-3c24-4d73-e44b-f862292612fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.49.0)\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (5.29.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.6.2.2\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers tensorboardX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upoazzBg2He1",
        "outputId": "845cd7c8-f6ee-4cf6-e534-44973974ce2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'CoSHC'...\n",
            "remote: Enumerating objects: 245, done.\u001b[K\n",
            "remote: Counting objects: 100% (18/18), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 245 (delta 10), reused 14 (delta 6), pack-reused 227 (from 1)\u001b[K\n",
            "Receiving objects: 100% (245/245), 71.22 KiB | 1.45 MiB/s, done.\n",
            "Resolving deltas: 100% (159/159), done.\n",
            "/content/CoSHC/CodeBERT\n",
            "Branch 'codebert' set up to track remote branch 'codebert' from 'origin'.\n",
            "Switched to a new branch 'codebert'\n"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "GITHUB_PAT = userdata.get('GITHUB_PAT')\n",
        "GITHUB_USERNAME = userdata.get('GITHUB_USERNAME')\n",
        "REPO_NAME = \"CoSHC\"\n",
        "\n",
        "%cd /content\n",
        "!git clone https://$GITHUB_USERNAME:$GITHUB_PAT@github.com/$GITHUB_USERNAME/$REPO_NAME\n",
        "%cd /content/$REPO_NAME/CodeBERT\n",
        "\n",
        "!git checkout codebert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4y9IVolltuRO",
        "outputId": "bcdaf260-b3aa-4ecb-8e58-998cefff4a5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects:  14% (1/7)\u001b[K\rremote: Counting objects:  28% (2/7)\u001b[K\rremote: Counting objects:  42% (3/7)\u001b[K\rremote: Counting objects:  57% (4/7)\u001b[K\rremote: Counting objects:  71% (5/7)\u001b[K\rremote: Counting objects:  85% (6/7)\u001b[K\rremote: Counting objects: 100% (7/7)\u001b[K\rremote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1/1)\u001b[K\rremote: Compressing objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 4 (delta 3), reused 4 (delta 3), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects:  25% (1/4)\rUnpacking objects:  50% (2/4)\rUnpacking objects:  75% (3/4)\rUnpacking objects: 100% (4/4)\rUnpacking objects: 100% (4/4), 351 bytes | 351.00 KiB/s, done.\n",
            "From https://github.com/kenho211/CoSHC\n",
            "   4be18f7..da63171  codebert   -> origin/codebert\n",
            "Updating 4be18f7..da63171\n",
            "Fast-forward\n",
            " CodeBERT/run_coshc.py | 31 \u001b[32m+\u001b[m\u001b[31m------------------------------\u001b[m\n",
            " 1 file changed, 1 insertion(+), 30 deletions(-)\n"
          ]
        }
      ],
      "source": [
        "## for hotfix\n",
        "!git pull"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "brWg2BExLVj1",
        "outputId": "76fd76b9-764d-44fd-ce89-96dad20e7f39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-25 16:00:29.155732: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-03-25 16:00:29.173931: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1742918429.195450   25337 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1742918429.202133   25337 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-25 16:00:29.224204: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "args: %s Namespace(train_data_file='/content/data/python/train.jsonl', output_dir='/content/drive/MyDrive/CoSHC/CodeBERT/models/python_coshc', eval_data_file='/content/data/python/valid.jsonl', test_data_file='/content/data/python/test.jsonl', codebase_file='/content/data/python/codebase.jsonl', model_name_or_path='/content/drive/MyDrive/CoSHC/CodeBERT/models/python_baseline/checkpoint-best-mrr/model.bin', config_name='microsoft/codebert-base', tokenizer_name='microsoft/codebert-base', nl_length=128, code_length=256, do_train=True, do_eval=False, do_test=False, train_batch_size=32, eval_batch_size=64, learning_rate=1e-05, max_grad_norm=1.0, num_train_epochs=1, seed=123456, hash_dim=128, hash_epochs=10, do_embed=True, embedding_dir='/content/drive/MyDrive/CoSHC/CodeBERT/code_embedding/python', class_epochs=5, num_clusters=10, beta=0.6, eta=0.4, mu=1.5, lambda1=0.1, lambda2=0.1, total_recall=100, alpha_init=1.0, cluster_file='clusters.pkl', hash_file='hashes.bin', debug=False)\n",
            "03/25/2025 16:00:32 - INFO - __main__ -   device: cuda, n_gpu: 1\n",
            "03/25/2025 16:00:33 - INFO - __main__ -   Loading pretrained CodeBERT model from /content/drive/MyDrive/CoSHC/CodeBERT/models/python_baseline/checkpoint-best-mrr/model.bin\n",
            "03/25/2025 16:00:33 - INFO - __main__ -   output_dir: /content/drive/MyDrive/CoSHC/CodeBERT/models/python_coshc\n",
            "03/25/2025 16:00:35 - INFO - __main__ -   Base model loaded\n",
            "03/25/2025 16:00:35 - INFO - __main__ -   Code embeddings already exist in /content/drive/MyDrive/CoSHC/CodeBERT/code_embedding/python\n",
            "03/25/2025 16:00:35 - INFO - __main__ -   CoSHC model loaded\n",
            "03/25/2025 16:00:37 - INFO - __main__ -   Code clustering completed\n",
            "03/25/2025 16:00:37 - INFO - __main__ -   Classification module initialized\n",
            "03/25/2025 16:02:17 - INFO - __main__ -   Epoch 0, Batch 100, Avg Loss: 2.2862\n",
            "03/25/2025 16:03:10 - INFO - __main__ -   Epoch 0, Batch 200, Avg Loss: 2.2667\n",
            "03/25/2025 16:04:04 - INFO - __main__ -   Epoch 0, Batch 300, Avg Loss: 2.2452\n",
            "03/25/2025 16:04:58 - INFO - __main__ -   Epoch 0, Batch 400, Avg Loss: 2.2246\n",
            "03/25/2025 16:05:52 - INFO - __main__ -   Epoch 0, Batch 500, Avg Loss: 2.2054\n",
            "03/25/2025 16:06:45 - INFO - __main__ -   Epoch 0, Batch 600, Avg Loss: 2.1867\n",
            "03/25/2025 16:07:31 - INFO - __main__ -   Epoch 0 completed, Avg Loss: 2.1700\n",
            "03/25/2025 16:08:25 - INFO - __main__ -   Epoch 1, Batch 100, Avg Loss: 2.0225\n",
            "03/25/2025 16:09:19 - INFO - __main__ -   Epoch 1, Batch 200, Avg Loss: 2.0050\n",
            "03/25/2025 16:10:13 - INFO - __main__ -   Epoch 1, Batch 300, Avg Loss: 1.9860\n",
            "03/25/2025 16:11:06 - INFO - __main__ -   Epoch 1, Batch 400, Avg Loss: 1.9692\n",
            "03/25/2025 16:12:00 - INFO - __main__ -   Epoch 1, Batch 500, Avg Loss: 1.9526\n",
            "03/25/2025 16:12:54 - INFO - __main__ -   Epoch 1, Batch 600, Avg Loss: 1.9364\n",
            "03/25/2025 16:13:40 - INFO - __main__ -   Epoch 1 completed, Avg Loss: 1.9221\n",
            "03/25/2025 16:14:34 - INFO - __main__ -   Epoch 2, Batch 100, Avg Loss: 1.7971\n",
            "03/25/2025 16:15:28 - INFO - __main__ -   Epoch 2, Batch 200, Avg Loss: 1.7820\n",
            "03/25/2025 16:16:21 - INFO - __main__ -   Epoch 2, Batch 300, Avg Loss: 1.7646\n",
            "03/25/2025 16:17:15 - INFO - __main__ -   Epoch 2, Batch 400, Avg Loss: 1.7493\n",
            "03/25/2025 16:18:09 - INFO - __main__ -   Epoch 2, Batch 500, Avg Loss: 1.7352\n",
            "03/25/2025 16:19:03 - INFO - __main__ -   Epoch 2, Batch 600, Avg Loss: 1.7213\n",
            "03/25/2025 16:19:49 - INFO - __main__ -   Epoch 2 completed, Avg Loss: 1.7090\n",
            "03/25/2025 16:20:43 - INFO - __main__ -   Epoch 3, Batch 100, Avg Loss: 1.5991\n",
            "03/25/2025 16:21:37 - INFO - __main__ -   Epoch 3, Batch 200, Avg Loss: 1.5874\n",
            "03/25/2025 16:22:31 - INFO - __main__ -   Epoch 3, Batch 300, Avg Loss: 1.5720\n",
            "03/25/2025 16:23:25 - INFO - __main__ -   Epoch 3, Batch 400, Avg Loss: 1.5588\n",
            "03/25/2025 16:24:18 - INFO - __main__ -   Epoch 3, Batch 500, Avg Loss: 1.5470\n",
            "03/25/2025 16:25:12 - INFO - __main__ -   Epoch 3, Batch 600, Avg Loss: 1.5348\n",
            "03/25/2025 16:25:58 - INFO - __main__ -   Epoch 3 completed, Avg Loss: 1.5241\n",
            "03/25/2025 16:26:52 - INFO - __main__ -   Epoch 4, Batch 100, Avg Loss: 1.4308\n",
            "03/25/2025 16:27:46 - INFO - __main__ -   Epoch 4, Batch 200, Avg Loss: 1.4208\n",
            "03/25/2025 16:28:40 - INFO - __main__ -   Epoch 4, Batch 300, Avg Loss: 1.4072\n",
            "03/25/2025 16:29:34 - INFO - __main__ -   Epoch 4, Batch 400, Avg Loss: 1.3961\n",
            "03/25/2025 16:30:27 - INFO - __main__ -   Epoch 4, Batch 500, Avg Loss: 1.3861\n",
            "03/25/2025 16:31:21 - INFO - __main__ -   Epoch 4, Batch 600, Avg Loss: 1.3759\n",
            "03/25/2025 16:32:07 - INFO - __main__ -   Epoch 4 completed, Avg Loss: 1.3667\n",
            "03/25/2025 16:32:07 - INFO - __main__ -   Training hashing module\n",
            "03/25/2025 16:36:40 - INFO - run -   *** Example ***\n",
            "03/25/2025 16:36:40 - INFO - run -   idx: 0\n",
            "03/25/2025 16:36:40 - INFO - run -   code_tokens: ['<s>', 'def', '_split', '_', 'ph', 'yl', 'ogen', 'y', '_(', '_p', '_,', '_level', '_=', '_\"', 's', '\"', '_)', '_:', '_level', '_=', '_level', '_+', '_\"', '__', '\"', '_result', '_=', '_p', '_.', '_split', '_(', '_level', '_)', '_return', '_result', '_[', '_0', '_]', '_+', '_level', '_+', '_result', '_[', '_1', '_]', '_.', '_split', '_(', '_\"', ';\"', '_)', '_[', '_0', '_]', '</s>']\n",
            "03/25/2025 16:36:40 - INFO - run -   code_ids: 0 9232 3462 1215 3792 4360 11575 219 36 181 2156 672 5457 22 29 113 4839 4832 672 5457 672 2055 22 30529 113 898 5457 181 479 3462 36 672 4839 671 898 646 321 27779 2055 672 2055 898 646 112 27779 479 3462 36 22 42777 4839 646 321 27779 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "03/25/2025 16:36:40 - INFO - run -   nl_tokens: ['<s>', 'Return', '_either', '_the', '_full', '_or', '_trunc', 'ated', '_version', '_of', '_a', '_Q', 'I', 'IME', '_-', '_formatted', '_tax', 'onomy', '_string', '_.', '</s>']\n",
            "03/25/2025 16:36:40 - INFO - run -   nl_ids: 0 42555 1169 5 455 50 43064 1070 1732 9 10 1209 100 28417 111 46625 629 38217 6755 479 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "03/25/2025 16:36:40 - INFO - run -   *** Example ***\n",
            "03/25/2025 16:36:40 - INFO - run -   idx: 1\n",
            "03/25/2025 16:36:40 - INFO - run -   code_tokens: ['<s>', 'def', '_ensure', '_', 'dir', '_(', '_d', '_)', '_:', '_if', '_not', '_os', '_.', '_path', '_.', '_exists', '_(', '_d', '_)', '_:', '_try', '_:', '_os', '_.', '_m', 'aked', 'irs', '_(', '_d', '_)', '_except', '_O', 'SE', 'r', 'ror', '_as', '_o', 'e', '_:', '_#', '_should', '_not', '_happen', '_with', '_os', '.', 'm', 'aked', 'irs', '_#', '_EN', 'O', 'ENT', ':', '_No', '_such', '_file', '_or', '_directory', '_if', '_os', '_.', '_err', 'no', '_==', '_err', 'no', '_.', '_EN', 'O', 'ENT', '_:', '_msg', '_=', '_tw', 'dd', '_(', '_\"\"\"', 'One', '_or', '_more', '_directories', '_in', '_the', '_path', '_({', '})', '_do', '_not', '_exist', '.', '_If', 'Ċ', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_you', '_are', '_specifying', '_a', '_new', '_directory', '_for', '_output', ',', '_please', '_ensure', 'Ċ', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_all', '_other', '_directories', '_in', '_the', '_path', '_currently', '_exist', '.\"', '\"\"', '_)', '_return', '_msg', '_.', '_format', '_(', '_d', '_)', '_else', '_:', '_msg', '_=', '_tw', 'dd', '_(', '_\"\"\"', 'An', '_error', '_occurred', '_trying', '_to', '_create', '_the', '_output', '_directory', 'Ċ', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_({', '})', '_with', '_message', ':', '_{}', '\"\"\"', '_)', '_return', '_msg', '_.', '_format', '_(', '_d', '_,', '_o', 'e', '_.', '_stre', 'r', 'ror', '_)', '</s>']\n",
            "03/25/2025 16:36:40 - INFO - run -   code_ids: 0 9232 1306 1215 41292 36 385 4839 4832 114 45 11988 479 2718 479 8785 36 385 4839 4832 860 4832 11988 479 475 8435 21098 36 385 4839 4682 384 3388 338 21929 25 1021 242 4832 849 197 45 1369 19 11988 4 119 8435 21098 849 13245 673 5382 35 440 215 2870 50 31826 114 11988 479 22379 2362 45994 22379 2362 479 13245 673 5382 4832 49049 5457 11901 16134 36 49434 3762 50 55 44472 11 5 2718 49698 49424 109 45 5152 4 318 50118 1437 1437 1437 1437 1437 1437 1437 1437 1437 1437 1437 1437 1437 1437 1437 1437 1437 1437 1437 1437 1437 1437 1437 1437 1437 1437 47 32 39140 10 92 31826 13 4195 6 2540 1306 50118 1437 1437 1437 1437 1437 1437 1437 1437 1437 1437 1437 1437 1437 1437 1437 1437 1437 1437 1437 1437 1437 1437 1437 1437 1437 1437 70 97 44472 11 5 2718 855 5152 72 48149 4839 671 49049 479 7390 36 385 4839 1493 4832 49049 5457 11901 16134 36 49434 4688 5849 2756 667 7 1045 5 4195 31826 50118 1437 1437 1437 1437 1437 1437 1437 1437 1437 1437 1437 1437 1437 1437 1437 1437 1437 1437 1437 1437 1437 1437 1437 1437 1437 1437 49698 49424 19 1579 35 49153 49849 4839 671 49049 479 7390 36 385 2156 1021 242 479 22246 338 21929 4839 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "03/25/2025 16:36:40 - INFO - run -   nl_tokens: ['<s>', 'Check', '_to', '_make', '_sure', '_the', '_supplied', '_directory', '_path', '_does', '_not', '_exist', '_if', '_so', '_create', '_it', '_.', '_The', '_method', '_catches', '_O', 'SE', 'r', 'ror', '_exceptions', '_and', '_returns', '_a', '_descriptive', '_message', '_instead', '_of', '_re', '_-', '_raising', '_the', '_error', '_.', '</s>']\n",
            "03/25/2025 16:36:40 - INFO - run -   nl_ids: 0 26615 7 146 686 5 12359 31826 2718 473 45 5152 114 98 1045 24 479 20 5448 8758 384 3388 338 21929 18286 8 2886 10 42690 1579 1386 9 769 111 3282 5 5849 479 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "03/25/2025 16:36:40 - INFO - run -   *** Example ***\n",
            "03/25/2025 16:36:40 - INFO - run -   idx: 2\n",
            "03/25/2025 16:36:40 - INFO - run -   code_tokens: ['<s>', 'def', '_file', '_', 'handle', '_(', '_fn', 'h', '_,', '_mode', '_=', '_\"', 'r', 'U', '\"', '_)', '_:', '_handle', '_=', '_None', '_if', '_is', 'instance', '_(', '_fn', 'h', '_,', '_file', '_)', '_:', '_if', '_fn', 'h', '_.', '_closed', '_:', '_raise', '_Value', 'Error', '_(', '_\"', 'Input', '_file', '_is', '_closed', '.\"', '_)', '_handle', '_=', '_fn', 'h', '_el', 'if', '_is', 'instance', '_(', '_fn', 'h', '_,', '_str', '_)', '_:', '_handle', '_=', '_open', '_(', '_fn', 'h', '_,', '_mode', '_)', '_return', '_handle', '</s>']\n",
            "03/25/2025 16:36:40 - INFO - run -   code_ids: 0 9232 2870 1215 26628 36 48930 298 2156 5745 5457 22 338 791 113 4839 4832 3679 5457 9291 114 16 48768 36 48930 298 2156 2870 4839 4832 114 48930 298 479 1367 4832 1693 11714 30192 36 22 48214 2870 16 1367 72 4839 3679 5457 48930 298 1615 1594 16 48768 36 48930 298 2156 7031 4839 4832 3679 5457 490 36 48930 298 2156 5745 4839 671 3679 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "03/25/2025 16:36:40 - INFO - run -   nl_tokens: ['<s>', 'T', 'akes', '_either', '_a', '_file', '_path', '_or', '_an', '_open', '_file', '_handle', '_checks', '_validity', '_and', '_returns', '_an', '_open', '_file', '_handle', '_or', '_raises', '_an', '_appropriate', '_Exception', '_.', '</s>']\n",
            "03/25/2025 16:36:40 - INFO - run -   nl_ids: 0 565 5556 1169 10 2870 2718 50 41 490 2870 3679 6240 25295 8 2886 41 490 2870 3679 50 7700 41 3901 47617 479 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "03/25/2025 16:36:49 - INFO - __main__ -   Epoch 0, Batch 50, Avg Loss: 145.8596\n",
            "03/25/2025 16:36:56 - INFO - __main__ -   Epoch 0, Batch 100, Avg Loss: 143.4471\n",
            "03/25/2025 16:37:03 - INFO - __main__ -   Epoch 0, Batch 150, Avg Loss: 132.0543\n",
            "03/25/2025 16:37:10 - INFO - __main__ -   Epoch 0, Batch 200, Avg Loss: 113.4817\n",
            "03/25/2025 16:37:17 - INFO - __main__ -   Epoch 0, Batch 250, Avg Loss: 100.2239\n",
            "03/25/2025 16:37:24 - INFO - __main__ -   Epoch 0, Batch 300, Avg Loss: 90.1479\n",
            "03/25/2025 16:37:31 - INFO - __main__ -   Epoch 0, Batch 350, Avg Loss: 82.8339\n",
            "03/25/2025 16:37:38 - INFO - __main__ -   Epoch 0, Batch 400, Avg Loss: 77.3764\n",
            "03/25/2025 16:37:45 - INFO - __main__ -   Epoch 0, Batch 450, Avg Loss: 74.0200\n",
            "03/25/2025 16:37:52 - INFO - __main__ -   Epoch 0, Batch 500, Avg Loss: 70.4573\n",
            "03/25/2025 16:37:59 - INFO - __main__ -   Epoch 0, Batch 550, Avg Loss: 67.7866\n",
            "03/25/2025 16:38:06 - INFO - __main__ -   Epoch 0, Batch 600, Avg Loss: 65.5058\n",
            "03/25/2025 16:38:13 - INFO - __main__ -   Epoch 0, Batch 650, Avg Loss: 64.2956\n",
            "03/25/2025 16:38:20 - INFO - __main__ -   Epoch 0, Batch 700, Avg Loss: 62.4543\n",
            "03/25/2025 16:38:27 - INFO - __main__ -   Epoch 0, Batch 750, Avg Loss: 60.9056\n",
            "03/25/2025 16:38:34 - INFO - __main__ -   Epoch 0, Batch 800, Avg Loss: 59.6296\n",
            "03/25/2025 16:38:41 - INFO - __main__ -   Epoch 0, Batch 850, Avg Loss: 58.4824\n",
            "03/25/2025 16:38:48 - INFO - __main__ -   Epoch 0, Batch 900, Avg Loss: 57.4257\n",
            "03/25/2025 16:38:55 - INFO - __main__ -   Epoch 0, Batch 950, Avg Loss: 56.5681\n",
            "03/25/2025 16:39:02 - INFO - __main__ -   Epoch 0, Batch 1000, Avg Loss: 56.0523\n",
            "03/25/2025 16:39:09 - INFO - __main__ -   Epoch 0, Batch 1050, Avg Loss: 55.2405\n",
            "03/25/2025 16:39:16 - INFO - __main__ -   Epoch 0, Batch 1100, Avg Loss: 54.7146\n",
            "03/25/2025 16:39:23 - INFO - __main__ -   Epoch 0, Batch 1150, Avg Loss: 54.0972\n",
            "03/25/2025 16:39:30 - INFO - __main__ -   Epoch 0, Batch 1200, Avg Loss: 53.6739\n",
            "03/25/2025 16:39:37 - INFO - __main__ -   Epoch 0, Batch 1250, Avg Loss: 53.1368\n",
            "03/25/2025 16:39:44 - INFO - __main__ -   Epoch 0, Batch 1300, Avg Loss: 52.6057\n",
            "03/25/2025 16:39:51 - INFO - __main__ -   Epoch 0, Batch 1350, Avg Loss: 52.0551\n",
            "03/25/2025 16:39:58 - INFO - __main__ -   Epoch 0, Batch 1400, Avg Loss: 51.6891\n",
            "03/25/2025 16:40:05 - INFO - __main__ -   Epoch 0, Batch 1450, Avg Loss: 51.2919\n",
            "03/25/2025 16:40:12 - INFO - __main__ -   Epoch 0, Batch 1500, Avg Loss: 50.8854\n",
            "03/25/2025 16:40:19 - INFO - __main__ -   Epoch 0, Batch 1550, Avg Loss: 50.5788\n",
            "03/25/2025 16:40:26 - INFO - __main__ -   Epoch 0, Batch 1600, Avg Loss: 50.2690\n",
            "03/25/2025 16:40:33 - INFO - __main__ -   Epoch 0, Batch 1650, Avg Loss: 50.0282\n",
            "03/25/2025 16:40:40 - INFO - __main__ -   Epoch 0, Batch 1700, Avg Loss: 49.8843\n",
            "03/25/2025 16:40:47 - INFO - __main__ -   Epoch 0, Batch 1750, Avg Loss: 49.5933\n",
            "03/25/2025 16:40:54 - INFO - __main__ -   Epoch 0, Batch 1800, Avg Loss: 49.2288\n",
            "03/25/2025 16:41:01 - INFO - __main__ -   Epoch 0, Batch 1850, Avg Loss: 49.0026\n",
            "03/25/2025 16:41:08 - INFO - __main__ -   Epoch 0, Batch 1900, Avg Loss: 48.8927\n",
            "03/25/2025 16:41:15 - INFO - __main__ -   Epoch 0, Batch 1950, Avg Loss: 48.7576\n",
            "03/25/2025 16:41:22 - INFO - __main__ -   Epoch 0, Batch 2000, Avg Loss: 48.8840\n",
            "03/25/2025 16:41:29 - INFO - __main__ -   Epoch 0, Batch 2050, Avg Loss: 48.7441\n",
            "03/25/2025 16:41:36 - INFO - __main__ -   Epoch 0, Batch 2100, Avg Loss: 48.5574\n",
            "03/25/2025 16:41:43 - INFO - __main__ -   Epoch 0, Batch 2150, Avg Loss: 48.3718\n",
            "03/25/2025 16:41:50 - INFO - __main__ -   Epoch 0, Batch 2200, Avg Loss: 48.1904\n",
            "03/25/2025 16:41:57 - INFO - __main__ -   Epoch 0, Batch 2250, Avg Loss: 47.9668\n",
            "03/25/2025 16:42:04 - INFO - __main__ -   Epoch 0, Batch 2300, Avg Loss: 47.7611\n",
            "03/25/2025 16:42:11 - INFO - __main__ -   Epoch 0, Batch 2350, Avg Loss: 47.5414\n",
            "03/25/2025 16:42:18 - INFO - __main__ -   Epoch 0, Batch 2400, Avg Loss: 47.3182\n",
            "03/25/2025 16:42:25 - INFO - __main__ -   Epoch 0, Batch 2450, Avg Loss: 47.0631\n",
            "03/25/2025 16:42:32 - INFO - __main__ -   Epoch 0, Batch 2500, Avg Loss: 46.8021\n",
            "03/25/2025 16:42:39 - INFO - __main__ -   Epoch 0, Batch 2550, Avg Loss: 46.6865\n",
            "03/25/2025 16:42:46 - INFO - __main__ -   Epoch 0, Batch 2600, Avg Loss: 47.1670\n",
            "03/25/2025 16:42:53 - INFO - __main__ -   Epoch 0, Batch 2650, Avg Loss: 47.2696\n",
            "03/25/2025 16:43:00 - INFO - __main__ -   Epoch 0, Batch 2700, Avg Loss: 47.0344\n",
            "03/25/2025 16:43:07 - INFO - __main__ -   Epoch 0, Batch 2750, Avg Loss: 46.8151\n",
            "03/25/2025 16:43:14 - INFO - __main__ -   Epoch 0, Batch 2800, Avg Loss: 46.5780\n",
            "03/25/2025 16:43:21 - INFO - __main__ -   Epoch 0, Batch 2850, Avg Loss: 46.3631\n",
            "03/25/2025 16:43:28 - INFO - __main__ -   Epoch 0, Batch 2900, Avg Loss: 46.1435\n",
            "03/25/2025 16:43:35 - INFO - __main__ -   Epoch 0, Batch 2950, Avg Loss: 45.9189\n",
            "03/25/2025 16:43:42 - INFO - __main__ -   Epoch 0, Batch 3000, Avg Loss: 45.7149\n",
            "03/25/2025 16:43:49 - INFO - __main__ -   Epoch 0, Batch 3050, Avg Loss: 45.5109\n",
            "03/25/2025 16:43:56 - INFO - __main__ -   Epoch 0, Batch 3100, Avg Loss: 45.3184\n",
            "03/25/2025 16:44:03 - INFO - __main__ -   Epoch 0, Batch 3150, Avg Loss: 45.1819\n",
            "03/25/2025 16:44:10 - INFO - __main__ -   Epoch 0, Batch 3200, Avg Loss: 44.9833\n",
            "03/25/2025 16:44:17 - INFO - __main__ -   Epoch 0, Batch 3250, Avg Loss: 44.8165\n",
            "03/25/2025 16:44:24 - INFO - __main__ -   Epoch 0, Batch 3300, Avg Loss: 44.9364\n",
            "03/25/2025 16:44:31 - INFO - __main__ -   Epoch 0, Batch 3350, Avg Loss: 44.8086\n",
            "03/25/2025 16:44:38 - INFO - __main__ -   Epoch 0, Batch 3400, Avg Loss: 44.7241\n",
            "03/25/2025 16:44:45 - INFO - __main__ -   Epoch 0, Batch 3450, Avg Loss: 44.6097\n",
            "03/25/2025 16:44:52 - INFO - __main__ -   Epoch 0, Batch 3500, Avg Loss: 44.4798\n",
            "03/25/2025 16:44:59 - INFO - __main__ -   Epoch 0, Batch 3550, Avg Loss: 44.4257\n",
            "03/25/2025 16:45:06 - INFO - __main__ -   Epoch 0, Batch 3600, Avg Loss: 44.3033\n",
            "03/25/2025 16:45:13 - INFO - __main__ -   Epoch 0, Batch 3650, Avg Loss: 44.2336\n",
            "03/25/2025 16:45:20 - INFO - __main__ -   Epoch 0, Batch 3700, Avg Loss: 44.1001\n",
            "03/25/2025 16:45:27 - INFO - __main__ -   Epoch 0, Batch 3750, Avg Loss: 43.9902\n",
            "03/25/2025 16:45:34 - INFO - __main__ -   Epoch 0, Batch 3800, Avg Loss: 43.8525\n",
            "03/25/2025 16:45:41 - INFO - __main__ -   Epoch 0, Batch 3850, Avg Loss: 43.7454\n",
            "03/25/2025 16:45:48 - INFO - __main__ -   Epoch 0, Batch 3900, Avg Loss: 43.6093\n",
            "03/25/2025 16:45:55 - INFO - __main__ -   Epoch 0, Batch 3950, Avg Loss: 43.4835\n",
            "03/25/2025 16:46:02 - INFO - __main__ -   Epoch 0, Batch 4000, Avg Loss: 43.3592\n",
            "03/25/2025 16:46:09 - INFO - __main__ -   Epoch 0, Batch 4050, Avg Loss: 43.2260\n",
            "03/25/2025 16:46:16 - INFO - __main__ -   Epoch 0, Batch 4100, Avg Loss: 43.0814\n",
            "03/25/2025 16:46:23 - INFO - __main__ -   Epoch 0, Batch 4150, Avg Loss: 42.9461\n",
            "03/25/2025 16:46:30 - INFO - __main__ -   Epoch 0, Batch 4200, Avg Loss: 43.0090\n",
            "03/25/2025 16:46:37 - INFO - __main__ -   Epoch 0, Batch 4250, Avg Loss: 42.8759\n",
            "03/25/2025 16:46:44 - INFO - __main__ -   Epoch 0, Batch 4300, Avg Loss: 42.8164\n",
            "03/25/2025 16:46:51 - INFO - __main__ -   Epoch 0, Batch 4350, Avg Loss: 42.6798\n",
            "03/25/2025 16:46:58 - INFO - __main__ -   Epoch 0, Batch 4400, Avg Loss: 42.5522\n",
            "03/25/2025 16:47:05 - INFO - __main__ -   Epoch 0, Batch 4450, Avg Loss: 42.4787\n",
            "03/25/2025 16:47:12 - INFO - __main__ -   Epoch 0, Batch 4500, Avg Loss: 42.3509\n",
            "03/25/2025 16:47:19 - INFO - __main__ -   Epoch 0, Batch 4550, Avg Loss: 42.2737\n",
            "03/25/2025 16:47:26 - INFO - __main__ -   Epoch 0, Batch 4600, Avg Loss: 42.1628\n",
            "03/25/2025 16:47:33 - INFO - __main__ -   Epoch 0, Batch 4650, Avg Loss: 42.0833\n",
            "03/25/2025 16:47:40 - INFO - __main__ -   Epoch 0, Batch 4700, Avg Loss: 41.9748\n",
            "03/25/2025 16:47:47 - INFO - __main__ -   Epoch 0, Batch 4750, Avg Loss: 41.9276\n",
            "03/25/2025 16:47:54 - INFO - __main__ -   Epoch 0, Batch 4800, Avg Loss: 41.9074\n",
            "03/25/2025 16:48:01 - INFO - __main__ -   Epoch 0, Batch 4850, Avg Loss: 41.8412\n",
            "03/25/2025 16:48:08 - INFO - __main__ -   Epoch 0, Batch 4900, Avg Loss: 41.7439\n",
            "03/25/2025 16:48:15 - INFO - __main__ -   Epoch 0, Batch 4950, Avg Loss: 41.6350\n",
            "03/25/2025 16:48:22 - INFO - __main__ -   Epoch 0, Batch 5000, Avg Loss: 41.5427\n",
            "03/25/2025 16:48:29 - INFO - __main__ -   Epoch 0, Batch 5050, Avg Loss: 41.4175\n",
            "03/25/2025 16:48:36 - INFO - __main__ -   Epoch 0, Batch 5100, Avg Loss: 41.3030\n",
            "03/25/2025 16:48:43 - INFO - __main__ -   Epoch 0, Batch 5150, Avg Loss: 41.1999\n",
            "03/25/2025 16:48:50 - INFO - __main__ -   Epoch 0, Batch 5200, Avg Loss: 41.1034\n",
            "03/25/2025 16:48:57 - INFO - __main__ -   Epoch 0, Batch 5250, Avg Loss: 40.9933\n",
            "03/25/2025 16:49:04 - INFO - __main__ -   Epoch 0, Batch 5300, Avg Loss: 40.9347\n",
            "03/25/2025 16:49:11 - INFO - __main__ -   Epoch 0, Batch 5350, Avg Loss: 40.8281\n",
            "03/25/2025 16:49:18 - INFO - __main__ -   Epoch 0, Batch 5400, Avg Loss: 40.7232\n",
            "03/25/2025 16:49:25 - INFO - __main__ -   Epoch 0, Batch 5450, Avg Loss: 40.6704\n",
            "03/25/2025 16:49:32 - INFO - __main__ -   Epoch 0, Batch 5500, Avg Loss: 40.5845\n",
            "03/25/2025 16:49:40 - INFO - __main__ -   Epoch 0, Batch 5550, Avg Loss: 40.5025\n",
            "03/25/2025 16:49:47 - INFO - __main__ -   Epoch 0, Batch 5600, Avg Loss: 40.4316\n",
            "03/25/2025 16:49:54 - INFO - __main__ -   Epoch 0, Batch 5650, Avg Loss: 40.4078\n",
            "03/25/2025 16:50:01 - INFO - __main__ -   Epoch 0, Batch 5700, Avg Loss: 40.3315\n",
            "03/25/2025 16:50:08 - INFO - __main__ -   Epoch 0, Batch 5750, Avg Loss: 40.2311\n",
            "03/25/2025 16:50:15 - INFO - __main__ -   Epoch 0, Batch 5800, Avg Loss: 40.1419\n",
            "03/25/2025 16:50:22 - INFO - __main__ -   Epoch 0, Batch 5850, Avg Loss: 40.0647\n",
            "03/25/2025 16:50:29 - INFO - __main__ -   Epoch 0, Batch 5900, Avg Loss: 40.0112\n",
            "03/25/2025 16:50:36 - INFO - __main__ -   Epoch 0, Batch 5950, Avg Loss: 39.9457\n",
            "03/25/2025 16:50:43 - INFO - __main__ -   Epoch 0, Batch 6000, Avg Loss: 39.9233\n",
            "03/25/2025 16:50:50 - INFO - __main__ -   Epoch 0, Batch 6050, Avg Loss: 39.8356\n",
            "03/25/2025 16:50:57 - INFO - __main__ -   Epoch 0, Batch 6100, Avg Loss: 39.7742\n",
            "03/25/2025 16:51:04 - INFO - __main__ -   Epoch 0, Batch 6150, Avg Loss: 39.6970\n",
            "03/25/2025 16:51:11 - INFO - __main__ -   Epoch 0, Batch 6200, Avg Loss: 39.6136\n",
            "03/25/2025 16:51:18 - INFO - __main__ -   Epoch 0, Batch 6250, Avg Loss: 39.5399\n",
            "03/25/2025 16:51:25 - INFO - __main__ -   Epoch 0, Batch 6300, Avg Loss: 39.5102\n",
            "03/25/2025 16:51:32 - INFO - __main__ -   Epoch 0, Batch 6350, Avg Loss: 39.4421\n",
            "03/25/2025 16:51:39 - INFO - __main__ -   Epoch 0, Batch 6400, Avg Loss: 39.3647\n",
            "03/25/2025 16:51:46 - INFO - __main__ -   Epoch 0, Batch 6450, Avg Loss: 39.2775\n",
            "03/25/2025 16:51:53 - INFO - __main__ -   Epoch 0, Batch 6500, Avg Loss: 39.2094\n",
            "03/25/2025 16:52:00 - INFO - __main__ -   Epoch 0, Batch 6550, Avg Loss: 39.1759\n",
            "03/25/2025 16:52:07 - INFO - __main__ -   Epoch 0, Batch 6600, Avg Loss: 39.0928\n",
            "03/25/2025 16:52:14 - INFO - __main__ -   Epoch 0, Batch 6650, Avg Loss: 39.0084\n",
            "03/25/2025 16:52:21 - INFO - __main__ -   Epoch 0, Batch 6700, Avg Loss: 38.9454\n",
            "03/25/2025 16:52:28 - INFO - __main__ -   Epoch 0, Batch 6750, Avg Loss: 38.8797\n",
            "03/25/2025 16:52:35 - INFO - __main__ -   Epoch 0, Batch 6800, Avg Loss: 38.8056\n",
            "03/25/2025 16:52:42 - INFO - __main__ -   Epoch 0, Batch 6850, Avg Loss: 38.7007\n",
            "03/25/2025 16:52:49 - INFO - __main__ -   Epoch 0, Batch 6900, Avg Loss: 38.6296\n",
            "03/25/2025 16:52:56 - INFO - __main__ -   Epoch 0, Batch 6950, Avg Loss: 38.5505\n",
            "03/25/2025 16:53:03 - INFO - __main__ -   Epoch 0, Batch 7000, Avg Loss: 38.4979\n",
            "03/25/2025 16:53:10 - INFO - __main__ -   Epoch 0, Batch 7050, Avg Loss: 38.4430\n",
            "03/25/2025 16:53:17 - INFO - __main__ -   Epoch 0, Batch 7100, Avg Loss: 38.3806\n",
            "03/25/2025 16:53:24 - INFO - __main__ -   Epoch 0, Batch 7150, Avg Loss: 38.3181\n",
            "03/25/2025 16:53:31 - INFO - __main__ -   Epoch 0, Batch 7200, Avg Loss: 38.2585\n",
            "03/25/2025 16:53:38 - INFO - __main__ -   Epoch 0, Batch 7250, Avg Loss: 38.2115\n",
            "03/25/2025 16:53:45 - INFO - __main__ -   Epoch 0, Batch 7300, Avg Loss: 38.1360\n",
            "03/25/2025 16:53:52 - INFO - __main__ -   Epoch 0, Batch 7350, Avg Loss: 38.0763\n",
            "03/25/2025 16:53:59 - INFO - __main__ -   Epoch 0, Batch 7400, Avg Loss: 38.0276\n",
            "03/25/2025 16:54:06 - INFO - __main__ -   Epoch 0, Batch 7450, Avg Loss: 37.9968\n",
            "03/25/2025 16:54:13 - INFO - __main__ -   Epoch 0, Batch 7500, Avg Loss: 37.9404\n",
            "03/25/2025 16:54:20 - INFO - __main__ -   Epoch 0, Batch 7550, Avg Loss: 37.8693\n",
            "03/25/2025 16:54:27 - INFO - __main__ -   Epoch 0, Batch 7600, Avg Loss: 37.8365\n",
            "03/25/2025 16:54:34 - INFO - __main__ -   Epoch 0, Batch 7650, Avg Loss: 37.7787\n",
            "03/25/2025 16:54:41 - INFO - __main__ -   Epoch 0, Batch 7700, Avg Loss: 37.7103\n",
            "03/25/2025 16:54:48 - INFO - __main__ -   Epoch 0, Batch 7750, Avg Loss: 37.6633\n",
            "03/25/2025 16:54:55 - INFO - __main__ -   Epoch 0, Batch 7800, Avg Loss: 37.6106\n",
            "03/25/2025 16:55:02 - INFO - __main__ -   Epoch 0, Batch 7850, Avg Loss: 37.5578\n",
            "03/25/2025 16:55:05 - INFO - __main__ -   Epoch 0 completed, Avg Loss: 37.5371\n",
            "03/25/2025 16:55:21 - INFO - __main__ -   Epoch 1, Batch 50, Avg Loss: 56.4921\n",
            "03/25/2025 16:55:28 - INFO - __main__ -   Epoch 1, Batch 100, Avg Loss: 46.8475\n",
            "03/25/2025 16:55:35 - INFO - __main__ -   Epoch 1, Batch 150, Avg Loss: 42.3186\n",
            "03/25/2025 16:55:42 - INFO - __main__ -   Epoch 1, Batch 200, Avg Loss: 39.0313\n",
            "03/25/2025 16:55:49 - INFO - __main__ -   Epoch 1, Batch 250, Avg Loss: 37.1328\n",
            "03/25/2025 16:55:56 - INFO - __main__ -   Epoch 1, Batch 300, Avg Loss: 35.3881\n",
            "03/25/2025 16:56:03 - INFO - __main__ -   Epoch 1, Batch 350, Avg Loss: 34.0026\n",
            "03/25/2025 16:56:10 - INFO - __main__ -   Epoch 1, Batch 400, Avg Loss: 32.9678\n",
            "03/25/2025 16:56:17 - INFO - __main__ -   Epoch 1, Batch 450, Avg Loss: 32.4109\n",
            "03/25/2025 16:56:24 - INFO - __main__ -   Epoch 1, Batch 500, Avg Loss: 31.7656\n",
            "03/25/2025 16:56:31 - INFO - __main__ -   Epoch 1, Batch 550, Avg Loss: 31.2330\n",
            "03/25/2025 16:56:38 - INFO - __main__ -   Epoch 1, Batch 600, Avg Loss: 30.7936\n",
            "03/25/2025 16:56:45 - INFO - __main__ -   Epoch 1, Batch 650, Avg Loss: 30.8865\n",
            "03/25/2025 16:56:52 - INFO - __main__ -   Epoch 1, Batch 700, Avg Loss: 30.3327\n",
            "03/25/2025 16:56:59 - INFO - __main__ -   Epoch 1, Batch 750, Avg Loss: 29.9133\n",
            "03/25/2025 16:57:06 - INFO - __main__ -   Epoch 1, Batch 800, Avg Loss: 29.5978\n",
            "03/25/2025 16:57:13 - INFO - __main__ -   Epoch 1, Batch 850, Avg Loss: 29.3358\n",
            "03/25/2025 16:57:20 - INFO - __main__ -   Epoch 1, Batch 900, Avg Loss: 29.1052\n",
            "03/25/2025 16:57:27 - INFO - __main__ -   Epoch 1, Batch 950, Avg Loss: 28.8927\n",
            "03/25/2025 16:57:34 - INFO - __main__ -   Epoch 1, Batch 1000, Avg Loss: 28.7891\n",
            "03/25/2025 16:57:41 - INFO - __main__ -   Epoch 1, Batch 1050, Avg Loss: 28.5490\n",
            "03/25/2025 16:57:48 - INFO - __main__ -   Epoch 1, Batch 1100, Avg Loss: 28.3785\n",
            "03/25/2025 16:57:55 - INFO - __main__ -   Epoch 1, Batch 1150, Avg Loss: 28.1712\n",
            "03/25/2025 16:58:02 - INFO - __main__ -   Epoch 1, Batch 1200, Avg Loss: 28.0849\n",
            "03/25/2025 16:58:09 - INFO - __main__ -   Epoch 1, Batch 1250, Avg Loss: 27.9146\n",
            "03/25/2025 16:58:16 - INFO - __main__ -   Epoch 1, Batch 1300, Avg Loss: 27.7388\n",
            "03/25/2025 16:58:23 - INFO - __main__ -   Epoch 1, Batch 1350, Avg Loss: 27.5513\n",
            "03/25/2025 16:58:30 - INFO - __main__ -   Epoch 1, Batch 1400, Avg Loss: 27.4347\n",
            "03/25/2025 16:58:37 - INFO - __main__ -   Epoch 1, Batch 1450, Avg Loss: 27.2812\n",
            "03/25/2025 16:58:44 - INFO - __main__ -   Epoch 1, Batch 1500, Avg Loss: 27.1081\n",
            "03/25/2025 16:58:51 - INFO - __main__ -   Epoch 1, Batch 1550, Avg Loss: 26.9706\n",
            "03/25/2025 16:58:58 - INFO - __main__ -   Epoch 1, Batch 1600, Avg Loss: 26.8441\n",
            "03/25/2025 16:59:05 - INFO - __main__ -   Epoch 1, Batch 1650, Avg Loss: 26.7570\n",
            "03/25/2025 16:59:12 - INFO - __main__ -   Epoch 1, Batch 1700, Avg Loss: 26.6247\n",
            "03/25/2025 16:59:19 - INFO - __main__ -   Epoch 1, Batch 1750, Avg Loss: 26.5211\n",
            "03/25/2025 16:59:26 - INFO - __main__ -   Epoch 1, Batch 1800, Avg Loss: 26.3787\n",
            "03/25/2025 16:59:33 - INFO - __main__ -   Epoch 1, Batch 1850, Avg Loss: 26.2872\n",
            "03/25/2025 16:59:40 - INFO - __main__ -   Epoch 1, Batch 1900, Avg Loss: 26.2491\n",
            "03/25/2025 16:59:47 - INFO - __main__ -   Epoch 1, Batch 1950, Avg Loss: 26.1712\n",
            "03/25/2025 16:59:54 - INFO - __main__ -   Epoch 1, Batch 2000, Avg Loss: 26.1464\n",
            "03/25/2025 17:00:01 - INFO - __main__ -   Epoch 1, Batch 2050, Avg Loss: 26.0726\n",
            "03/25/2025 17:00:08 - INFO - __main__ -   Epoch 1, Batch 2100, Avg Loss: 25.9909\n",
            "03/25/2025 17:00:15 - INFO - __main__ -   Epoch 1, Batch 2150, Avg Loss: 25.9096\n",
            "03/25/2025 17:00:22 - INFO - __main__ -   Epoch 1, Batch 2200, Avg Loss: 25.8133\n",
            "03/25/2025 17:00:29 - INFO - __main__ -   Epoch 1, Batch 2250, Avg Loss: 25.7105\n",
            "03/25/2025 17:00:36 - INFO - __main__ -   Epoch 1, Batch 2300, Avg Loss: 25.6318\n",
            "03/25/2025 17:00:43 - INFO - __main__ -   Epoch 1, Batch 2350, Avg Loss: 25.5446\n",
            "03/25/2025 17:00:50 - INFO - __main__ -   Epoch 1, Batch 2400, Avg Loss: 25.4637\n",
            "03/25/2025 17:00:57 - INFO - __main__ -   Epoch 1, Batch 2450, Avg Loss: 25.3778\n",
            "03/25/2025 17:01:04 - INFO - __main__ -   Epoch 1, Batch 2500, Avg Loss: 25.2884\n",
            "03/25/2025 17:01:11 - INFO - __main__ -   Epoch 1, Batch 2550, Avg Loss: 25.2431\n",
            "03/25/2025 17:01:18 - INFO - __main__ -   Epoch 1, Batch 2600, Avg Loss: 25.5044\n",
            "03/25/2025 17:01:25 - INFO - __main__ -   Epoch 1, Batch 2650, Avg Loss: 25.5621\n",
            "03/25/2025 17:01:32 - INFO - __main__ -   Epoch 1, Batch 2700, Avg Loss: 25.4774\n",
            "03/25/2025 17:01:39 - INFO - __main__ -   Epoch 1, Batch 2750, Avg Loss: 25.4000\n",
            "03/25/2025 17:01:46 - INFO - __main__ -   Epoch 1, Batch 2800, Avg Loss: 25.3171\n",
            "03/25/2025 17:01:53 - INFO - __main__ -   Epoch 1, Batch 2850, Avg Loss: 25.2361\n",
            "03/25/2025 17:02:00 - INFO - __main__ -   Epoch 1, Batch 2900, Avg Loss: 25.1442\n",
            "03/25/2025 17:02:07 - INFO - __main__ -   Epoch 1, Batch 2950, Avg Loss: 25.0538\n",
            "03/25/2025 17:02:14 - INFO - __main__ -   Epoch 1, Batch 3000, Avg Loss: 24.9812\n",
            "03/25/2025 17:02:21 - INFO - __main__ -   Epoch 1, Batch 3050, Avg Loss: 24.9003\n",
            "03/25/2025 17:02:28 - INFO - __main__ -   Epoch 1, Batch 3100, Avg Loss: 24.8215\n",
            "03/25/2025 17:02:35 - INFO - __main__ -   Epoch 1, Batch 3150, Avg Loss: 24.7557\n",
            "03/25/2025 17:02:42 - INFO - __main__ -   Epoch 1, Batch 3200, Avg Loss: 24.6719\n",
            "03/25/2025 17:02:49 - INFO - __main__ -   Epoch 1, Batch 3250, Avg Loss: 24.6042\n",
            "03/25/2025 17:02:56 - INFO - __main__ -   Epoch 1, Batch 3300, Avg Loss: 24.6567\n",
            "03/25/2025 17:03:03 - INFO - __main__ -   Epoch 1, Batch 3350, Avg Loss: 24.6072\n",
            "03/25/2025 17:03:10 - INFO - __main__ -   Epoch 1, Batch 3400, Avg Loss: 24.5863\n",
            "03/25/2025 17:03:17 - INFO - __main__ -   Epoch 1, Batch 3450, Avg Loss: 24.5295\n",
            "03/25/2025 17:03:24 - INFO - __main__ -   Epoch 1, Batch 3500, Avg Loss: 24.4921\n",
            "03/25/2025 17:03:31 - INFO - __main__ -   Epoch 1, Batch 3550, Avg Loss: 24.4748\n",
            "03/25/2025 17:03:38 - INFO - __main__ -   Epoch 1, Batch 3600, Avg Loss: 24.4233\n",
            "03/25/2025 17:03:45 - INFO - __main__ -   Epoch 1, Batch 3650, Avg Loss: 24.3862\n",
            "03/25/2025 17:03:52 - INFO - __main__ -   Epoch 1, Batch 3700, Avg Loss: 24.3444\n",
            "03/25/2025 17:03:59 - INFO - __main__ -   Epoch 1, Batch 3750, Avg Loss: 24.2953\n",
            "03/25/2025 17:04:06 - INFO - __main__ -   Epoch 1, Batch 3800, Avg Loss: 24.2502\n",
            "03/25/2025 17:04:13 - INFO - __main__ -   Epoch 1, Batch 3850, Avg Loss: 24.2195\n",
            "03/25/2025 17:04:20 - INFO - __main__ -   Epoch 1, Batch 3900, Avg Loss: 24.1775\n",
            "03/25/2025 17:04:27 - INFO - __main__ -   Epoch 1, Batch 3950, Avg Loss: 24.1213\n",
            "03/25/2025 17:04:34 - INFO - __main__ -   Epoch 1, Batch 4000, Avg Loss: 24.0687\n",
            "03/25/2025 17:04:41 - INFO - __main__ -   Epoch 1, Batch 4050, Avg Loss: 24.0305\n",
            "03/25/2025 17:04:48 - INFO - __main__ -   Epoch 1, Batch 4100, Avg Loss: 23.9812\n",
            "03/25/2025 17:04:55 - INFO - __main__ -   Epoch 1, Batch 4150, Avg Loss: 23.9259\n",
            "03/25/2025 17:05:02 - INFO - __main__ -   Epoch 1, Batch 4200, Avg Loss: 23.9498\n",
            "03/25/2025 17:05:09 - INFO - __main__ -   Epoch 1, Batch 4250, Avg Loss: 23.9036\n",
            "03/25/2025 17:05:16 - INFO - __main__ -   Epoch 1, Batch 4300, Avg Loss: 23.8750\n",
            "03/25/2025 17:05:23 - INFO - __main__ -   Epoch 1, Batch 4350, Avg Loss: 23.8220\n",
            "03/25/2025 17:05:30 - INFO - __main__ -   Epoch 1, Batch 4400, Avg Loss: 23.7779\n",
            "03/25/2025 17:05:37 - INFO - __main__ -   Epoch 1, Batch 4450, Avg Loss: 23.7555\n",
            "03/25/2025 17:05:44 - INFO - __main__ -   Epoch 1, Batch 4500, Avg Loss: 23.7135\n",
            "03/25/2025 17:05:51 - INFO - __main__ -   Epoch 1, Batch 4550, Avg Loss: 23.6881\n",
            "03/25/2025 17:05:58 - INFO - __main__ -   Epoch 1, Batch 4600, Avg Loss: 23.6421\n",
            "03/25/2025 17:06:05 - INFO - __main__ -   Epoch 1, Batch 4650, Avg Loss: 23.6149\n",
            "03/25/2025 17:06:12 - INFO - __main__ -   Epoch 1, Batch 4700, Avg Loss: 23.5663\n",
            "03/25/2025 17:06:19 - INFO - __main__ -   Epoch 1, Batch 4750, Avg Loss: 23.5421\n",
            "03/25/2025 17:06:26 - INFO - __main__ -   Epoch 1, Batch 4800, Avg Loss: 23.5403\n",
            "03/25/2025 17:06:33 - INFO - __main__ -   Epoch 1, Batch 4850, Avg Loss: 23.5146\n",
            "03/25/2025 17:06:40 - INFO - __main__ -   Epoch 1, Batch 4900, Avg Loss: 23.4815\n",
            "03/25/2025 17:06:47 - INFO - __main__ -   Epoch 1, Batch 4950, Avg Loss: 23.4484\n",
            "03/25/2025 17:06:54 - INFO - __main__ -   Epoch 1, Batch 5000, Avg Loss: 23.4202\n",
            "03/25/2025 17:07:01 - INFO - __main__ -   Epoch 1, Batch 5050, Avg Loss: 23.3783\n",
            "03/25/2025 17:07:08 - INFO - __main__ -   Epoch 1, Batch 5100, Avg Loss: 23.3460\n",
            "03/25/2025 17:07:15 - INFO - __main__ -   Epoch 1, Batch 5150, Avg Loss: 23.3146\n",
            "03/25/2025 17:07:22 - INFO - __main__ -   Epoch 1, Batch 5200, Avg Loss: 23.2709\n",
            "03/25/2025 17:07:29 - INFO - __main__ -   Epoch 1, Batch 5250, Avg Loss: 23.2271\n",
            "03/25/2025 17:07:36 - INFO - __main__ -   Epoch 1, Batch 5300, Avg Loss: 23.2051\n",
            "03/25/2025 17:07:43 - INFO - __main__ -   Epoch 1, Batch 5350, Avg Loss: 23.1588\n",
            "03/25/2025 17:07:50 - INFO - __main__ -   Epoch 1, Batch 5400, Avg Loss: 23.1152\n",
            "03/25/2025 17:07:57 - INFO - __main__ -   Epoch 1, Batch 5450, Avg Loss: 23.0945\n",
            "03/25/2025 17:08:04 - INFO - __main__ -   Epoch 1, Batch 5500, Avg Loss: 23.0590\n",
            "03/25/2025 17:08:11 - INFO - __main__ -   Epoch 1, Batch 5550, Avg Loss: 23.0277\n",
            "03/25/2025 17:08:18 - INFO - __main__ -   Epoch 1, Batch 5600, Avg Loss: 22.9941\n",
            "03/25/2025 17:08:25 - INFO - __main__ -   Epoch 1, Batch 5650, Avg Loss: 22.9812\n",
            "03/25/2025 17:08:32 - INFO - __main__ -   Epoch 1, Batch 5700, Avg Loss: 22.9521\n",
            "03/25/2025 17:08:39 - INFO - __main__ -   Epoch 1, Batch 5750, Avg Loss: 22.9057\n",
            "03/25/2025 17:08:46 - INFO - __main__ -   Epoch 1, Batch 5800, Avg Loss: 22.8645\n",
            "03/25/2025 17:08:53 - INFO - __main__ -   Epoch 1, Batch 5850, Avg Loss: 22.8285\n",
            "03/25/2025 17:09:00 - INFO - __main__ -   Epoch 1, Batch 5900, Avg Loss: 22.7962\n",
            "03/25/2025 17:09:07 - INFO - __main__ -   Epoch 1, Batch 5950, Avg Loss: 22.7705\n",
            "03/25/2025 17:09:14 - INFO - __main__ -   Epoch 1, Batch 6000, Avg Loss: 22.7561\n",
            "03/25/2025 17:09:22 - INFO - __main__ -   Epoch 1, Batch 6050, Avg Loss: 22.7213\n",
            "03/25/2025 17:09:29 - INFO - __main__ -   Epoch 1, Batch 6100, Avg Loss: 22.6948\n",
            "03/25/2025 17:09:36 - INFO - __main__ -   Epoch 1, Batch 6150, Avg Loss: 22.6623\n",
            "03/25/2025 17:09:43 - INFO - __main__ -   Epoch 1, Batch 6200, Avg Loss: 22.6337\n",
            "03/25/2025 17:09:50 - INFO - __main__ -   Epoch 1, Batch 6250, Avg Loss: 22.6130\n",
            "03/25/2025 17:09:57 - INFO - __main__ -   Epoch 1, Batch 6300, Avg Loss: 22.5944\n",
            "03/25/2025 17:10:04 - INFO - __main__ -   Epoch 1, Batch 6350, Avg Loss: 22.5720\n",
            "03/25/2025 17:10:11 - INFO - __main__ -   Epoch 1, Batch 6400, Avg Loss: 22.5472\n",
            "03/25/2025 17:10:18 - INFO - __main__ -   Epoch 1, Batch 6450, Avg Loss: 22.5186\n",
            "03/25/2025 17:10:25 - INFO - __main__ -   Epoch 1, Batch 6500, Avg Loss: 22.4914\n",
            "03/25/2025 17:10:32 - INFO - __main__ -   Epoch 1, Batch 6550, Avg Loss: 22.4927\n",
            "03/25/2025 17:10:39 - INFO - __main__ -   Epoch 1, Batch 6600, Avg Loss: 22.4616\n",
            "03/25/2025 17:10:46 - INFO - __main__ -   Epoch 1, Batch 6650, Avg Loss: 22.4357\n",
            "03/25/2025 17:10:53 - INFO - __main__ -   Epoch 1, Batch 6700, Avg Loss: 22.4145\n",
            "03/25/2025 17:11:00 - INFO - __main__ -   Epoch 1, Batch 6750, Avg Loss: 22.3939\n",
            "03/25/2025 17:11:07 - INFO - __main__ -   Epoch 1, Batch 6800, Avg Loss: 22.3706\n",
            "03/25/2025 17:11:14 - INFO - __main__ -   Epoch 1, Batch 6850, Avg Loss: 22.3342\n",
            "03/25/2025 17:11:21 - INFO - __main__ -   Epoch 1, Batch 6900, Avg Loss: 22.3140\n",
            "03/25/2025 17:11:28 - INFO - __main__ -   Epoch 1, Batch 6950, Avg Loss: 22.2798\n",
            "03/25/2025 17:11:35 - INFO - __main__ -   Epoch 1, Batch 7000, Avg Loss: 22.2668\n",
            "03/25/2025 17:11:42 - INFO - __main__ -   Epoch 1, Batch 7050, Avg Loss: 22.2456\n",
            "03/25/2025 17:11:49 - INFO - __main__ -   Epoch 1, Batch 7100, Avg Loss: 22.2196\n",
            "03/25/2025 17:11:56 - INFO - __main__ -   Epoch 1, Batch 7150, Avg Loss: 22.1994\n",
            "03/25/2025 17:12:03 - INFO - __main__ -   Epoch 1, Batch 7200, Avg Loss: 22.1778\n",
            "03/25/2025 17:12:10 - INFO - __main__ -   Epoch 1, Batch 7250, Avg Loss: 22.1711\n",
            "03/25/2025 17:12:17 - INFO - __main__ -   Epoch 1, Batch 7300, Avg Loss: 22.1428\n",
            "03/25/2025 17:12:24 - INFO - __main__ -   Epoch 1, Batch 7350, Avg Loss: 22.1169\n",
            "03/25/2025 17:12:31 - INFO - __main__ -   Epoch 1, Batch 7400, Avg Loss: 22.1013\n",
            "03/25/2025 17:12:38 - INFO - __main__ -   Epoch 1, Batch 7450, Avg Loss: 22.1008\n",
            "03/25/2025 17:12:45 - INFO - __main__ -   Epoch 1, Batch 7500, Avg Loss: 22.0751\n",
            "03/25/2025 17:12:52 - INFO - __main__ -   Epoch 1, Batch 7550, Avg Loss: 22.0435\n",
            "03/25/2025 17:12:59 - INFO - __main__ -   Epoch 1, Batch 7600, Avg Loss: 22.0328\n",
            "03/25/2025 17:13:06 - INFO - __main__ -   Epoch 1, Batch 7650, Avg Loss: 22.0055\n",
            "03/25/2025 17:13:13 - INFO - __main__ -   Epoch 1, Batch 7700, Avg Loss: 21.9787\n",
            "03/25/2025 17:13:20 - INFO - __main__ -   Epoch 1, Batch 7750, Avg Loss: 21.9790\n",
            "03/25/2025 17:13:27 - INFO - __main__ -   Epoch 1, Batch 7800, Avg Loss: 21.9632\n",
            "03/25/2025 17:13:34 - INFO - __main__ -   Epoch 1, Batch 7850, Avg Loss: 21.9398\n",
            "03/25/2025 17:13:37 - INFO - __main__ -   Epoch 1 completed, Avg Loss: 21.9296\n",
            "03/25/2025 17:13:49 - INFO - __main__ -   Epoch 2, Batch 50, Avg Loss: 19.3090\n",
            "03/25/2025 17:13:56 - INFO - __main__ -   Epoch 2, Batch 100, Avg Loss: 18.6196\n",
            "03/25/2025 17:14:03 - INFO - __main__ -   Epoch 2, Batch 150, Avg Loss: 18.3955\n",
            "03/25/2025 17:14:10 - INFO - __main__ -   Epoch 2, Batch 200, Avg Loss: 18.1165\n",
            "03/25/2025 17:14:17 - INFO - __main__ -   Epoch 2, Batch 250, Avg Loss: 18.1913\n",
            "03/25/2025 17:14:24 - INFO - __main__ -   Epoch 2, Batch 300, Avg Loss: 18.0795\n",
            "03/25/2025 17:14:31 - INFO - __main__ -   Epoch 2, Batch 350, Avg Loss: 17.8882\n",
            "03/25/2025 17:14:38 - INFO - __main__ -   Epoch 2, Batch 400, Avg Loss: 17.8316\n",
            "03/25/2025 17:14:45 - INFO - __main__ -   Epoch 2, Batch 450, Avg Loss: 17.8163\n",
            "03/25/2025 17:14:52 - INFO - __main__ -   Epoch 2, Batch 500, Avg Loss: 17.8044\n",
            "03/25/2025 17:14:59 - INFO - __main__ -   Epoch 2, Batch 550, Avg Loss: 17.8395\n",
            "03/25/2025 17:15:06 - INFO - __main__ -   Epoch 2, Batch 600, Avg Loss: 17.8296\n",
            "03/25/2025 17:15:13 - INFO - __main__ -   Epoch 2, Batch 650, Avg Loss: 17.9428\n",
            "03/25/2025 17:15:20 - INFO - __main__ -   Epoch 2, Batch 700, Avg Loss: 17.8428\n",
            "03/25/2025 17:15:27 - INFO - __main__ -   Epoch 2, Batch 750, Avg Loss: 17.8025\n",
            "03/25/2025 17:15:34 - INFO - __main__ -   Epoch 2, Batch 800, Avg Loss: 17.7611\n",
            "03/25/2025 17:15:41 - INFO - __main__ -   Epoch 2, Batch 850, Avg Loss: 17.7292\n",
            "03/25/2025 17:15:48 - INFO - __main__ -   Epoch 2, Batch 900, Avg Loss: 17.6913\n",
            "03/25/2025 17:15:55 - INFO - __main__ -   Epoch 2, Batch 950, Avg Loss: 17.6618\n",
            "03/25/2025 17:16:02 - INFO - __main__ -   Epoch 2, Batch 1000, Avg Loss: 17.6840\n",
            "03/25/2025 17:16:09 - INFO - __main__ -   Epoch 2, Batch 1050, Avg Loss: 17.6401\n",
            "03/25/2025 17:16:16 - INFO - __main__ -   Epoch 2, Batch 1100, Avg Loss: 17.5889\n",
            "03/25/2025 17:16:23 - INFO - __main__ -   Epoch 2, Batch 1150, Avg Loss: 17.5383\n",
            "03/25/2025 17:16:30 - INFO - __main__ -   Epoch 2, Batch 1200, Avg Loss: 17.5316\n",
            "03/25/2025 17:16:37 - INFO - __main__ -   Epoch 2, Batch 1250, Avg Loss: 17.4960\n",
            "03/25/2025 17:16:44 - INFO - __main__ -   Epoch 2, Batch 1300, Avg Loss: 17.4831\n",
            "03/25/2025 17:16:51 - INFO - __main__ -   Epoch 2, Batch 1350, Avg Loss: 17.4349\n",
            "03/25/2025 17:16:58 - INFO - __main__ -   Epoch 2, Batch 1400, Avg Loss: 17.4242\n",
            "03/25/2025 17:17:05 - INFO - __main__ -   Epoch 2, Batch 1450, Avg Loss: 17.4069\n",
            "03/25/2025 17:17:12 - INFO - __main__ -   Epoch 2, Batch 1500, Avg Loss: 17.3576\n",
            "03/25/2025 17:17:19 - INFO - __main__ -   Epoch 2, Batch 1550, Avg Loss: 17.3081\n",
            "03/25/2025 17:17:26 - INFO - __main__ -   Epoch 2, Batch 1600, Avg Loss: 17.2844\n",
            "03/25/2025 17:17:33 - INFO - __main__ -   Epoch 2, Batch 1650, Avg Loss: 17.2764\n",
            "03/25/2025 17:17:40 - INFO - __main__ -   Epoch 2, Batch 1700, Avg Loss: 17.2428\n",
            "03/25/2025 17:17:47 - INFO - __main__ -   Epoch 2, Batch 1750, Avg Loss: 17.2157\n",
            "03/25/2025 17:17:54 - INFO - __main__ -   Epoch 2, Batch 1800, Avg Loss: 17.1652\n",
            "03/25/2025 17:18:01 - INFO - __main__ -   Epoch 2, Batch 1850, Avg Loss: 17.1347\n",
            "03/25/2025 17:18:08 - INFO - __main__ -   Epoch 2, Batch 1900, Avg Loss: 17.1442\n",
            "03/25/2025 17:18:15 - INFO - __main__ -   Epoch 2, Batch 1950, Avg Loss: 17.1296\n",
            "03/25/2025 17:18:22 - INFO - __main__ -   Epoch 2, Batch 2000, Avg Loss: 17.1532\n",
            "03/25/2025 17:18:29 - INFO - __main__ -   Epoch 2, Batch 2050, Avg Loss: 17.1454\n",
            "03/25/2025 17:18:36 - INFO - __main__ -   Epoch 2, Batch 2100, Avg Loss: 17.1285\n",
            "03/25/2025 17:18:43 - INFO - __main__ -   Epoch 2, Batch 2150, Avg Loss: 17.1048\n",
            "03/25/2025 17:18:50 - INFO - __main__ -   Epoch 2, Batch 2200, Avg Loss: 17.0799\n",
            "03/25/2025 17:18:57 - INFO - __main__ -   Epoch 2, Batch 2250, Avg Loss: 17.0441\n",
            "03/25/2025 17:19:04 - INFO - __main__ -   Epoch 2, Batch 2300, Avg Loss: 17.0252\n",
            "03/25/2025 17:19:11 - INFO - __main__ -   Epoch 2, Batch 2350, Avg Loss: 16.9989\n",
            "03/25/2025 17:19:18 - INFO - __main__ -   Epoch 2, Batch 2400, Avg Loss: 16.9794\n",
            "03/25/2025 17:19:25 - INFO - __main__ -   Epoch 2, Batch 2450, Avg Loss: 16.9541\n",
            "03/25/2025 17:19:32 - INFO - __main__ -   Epoch 2, Batch 2500, Avg Loss: 16.9245\n",
            "03/25/2025 17:19:39 - INFO - __main__ -   Epoch 2, Batch 2550, Avg Loss: 16.9192\n",
            "03/25/2025 17:19:46 - INFO - __main__ -   Epoch 2, Batch 2600, Avg Loss: 17.1607\n",
            "03/25/2025 17:19:53 - INFO - __main__ -   Epoch 2, Batch 2650, Avg Loss: 17.2385\n",
            "03/25/2025 17:20:00 - INFO - __main__ -   Epoch 2, Batch 2700, Avg Loss: 17.2105\n",
            "03/25/2025 17:20:07 - INFO - __main__ -   Epoch 2, Batch 2750, Avg Loss: 17.1803\n",
            "03/25/2025 17:20:14 - INFO - __main__ -   Epoch 2, Batch 2800, Avg Loss: 17.1550\n",
            "03/25/2025 17:20:21 - INFO - __main__ -   Epoch 2, Batch 2850, Avg Loss: 17.1234\n",
            "03/25/2025 17:20:28 - INFO - __main__ -   Epoch 2, Batch 2900, Avg Loss: 17.0867\n",
            "03/25/2025 17:20:35 - INFO - __main__ -   Epoch 2, Batch 2950, Avg Loss: 17.0481\n",
            "03/25/2025 17:20:42 - INFO - __main__ -   Epoch 2, Batch 3000, Avg Loss: 17.0202\n",
            "03/25/2025 17:20:49 - INFO - __main__ -   Epoch 2, Batch 3050, Avg Loss: 16.9944\n",
            "03/25/2025 17:20:56 - INFO - __main__ -   Epoch 2, Batch 3100, Avg Loss: 16.9638\n",
            "03/25/2025 17:21:03 - INFO - __main__ -   Epoch 2, Batch 3150, Avg Loss: 16.9453\n",
            "03/25/2025 17:21:10 - INFO - __main__ -   Epoch 2, Batch 3200, Avg Loss: 16.9158\n",
            "03/25/2025 17:21:17 - INFO - __main__ -   Epoch 2, Batch 3250, Avg Loss: 16.8957\n",
            "03/25/2025 17:21:24 - INFO - __main__ -   Epoch 2, Batch 3300, Avg Loss: 16.9369\n",
            "03/25/2025 17:21:31 - INFO - __main__ -   Epoch 2, Batch 3350, Avg Loss: 16.9261\n",
            "03/25/2025 17:21:38 - INFO - __main__ -   Epoch 2, Batch 3400, Avg Loss: 16.9286\n",
            "03/25/2025 17:21:45 - INFO - __main__ -   Epoch 2, Batch 3450, Avg Loss: 16.9146\n",
            "03/25/2025 17:21:52 - INFO - __main__ -   Epoch 2, Batch 3500, Avg Loss: 16.9086\n",
            "03/25/2025 17:21:59 - INFO - __main__ -   Epoch 2, Batch 3550, Avg Loss: 16.9104\n",
            "03/25/2025 17:22:06 - INFO - __main__ -   Epoch 2, Batch 3600, Avg Loss: 16.8991\n",
            "03/25/2025 17:22:13 - INFO - __main__ -   Epoch 2, Batch 3650, Avg Loss: 16.9033\n",
            "03/25/2025 17:22:20 - INFO - __main__ -   Epoch 2, Batch 3700, Avg Loss: 16.9013\n",
            "03/25/2025 17:22:27 - INFO - __main__ -   Epoch 2, Batch 3750, Avg Loss: 16.8860\n",
            "03/25/2025 17:22:34 - INFO - __main__ -   Epoch 2, Batch 3800, Avg Loss: 16.8793\n",
            "03/25/2025 17:22:41 - INFO - __main__ -   Epoch 2, Batch 3850, Avg Loss: 16.8867\n",
            "03/25/2025 17:22:48 - INFO - __main__ -   Epoch 2, Batch 3900, Avg Loss: 16.8776\n",
            "03/25/2025 17:22:55 - INFO - __main__ -   Epoch 2, Batch 3950, Avg Loss: 16.8632\n",
            "03/25/2025 17:23:02 - INFO - __main__ -   Epoch 2, Batch 4000, Avg Loss: 16.8465\n",
            "03/25/2025 17:23:09 - INFO - __main__ -   Epoch 2, Batch 4050, Avg Loss: 16.8399\n",
            "03/25/2025 17:23:16 - INFO - __main__ -   Epoch 2, Batch 4100, Avg Loss: 16.8232\n",
            "03/25/2025 17:23:23 - INFO - __main__ -   Epoch 2, Batch 4150, Avg Loss: 16.8070\n",
            "03/25/2025 17:23:30 - INFO - __main__ -   Epoch 2, Batch 4200, Avg Loss: 16.8364\n",
            "03/25/2025 17:23:37 - INFO - __main__ -   Epoch 2, Batch 4250, Avg Loss: 16.8277\n",
            "03/25/2025 17:23:44 - INFO - __main__ -   Epoch 2, Batch 4300, Avg Loss: 16.8261\n",
            "03/25/2025 17:23:51 - INFO - __main__ -   Epoch 2, Batch 4350, Avg Loss: 16.8084\n",
            "03/25/2025 17:23:58 - INFO - __main__ -   Epoch 2, Batch 4400, Avg Loss: 16.7994\n",
            "03/25/2025 17:24:05 - INFO - __main__ -   Epoch 2, Batch 4450, Avg Loss: 16.8013\n",
            "03/25/2025 17:24:12 - INFO - __main__ -   Epoch 2, Batch 4500, Avg Loss: 16.7965\n",
            "03/25/2025 17:24:19 - INFO - __main__ -   Epoch 2, Batch 4550, Avg Loss: 16.7977\n",
            "03/25/2025 17:24:26 - INFO - __main__ -   Epoch 2, Batch 4600, Avg Loss: 16.7863\n",
            "03/25/2025 17:24:33 - INFO - __main__ -   Epoch 2, Batch 4650, Avg Loss: 16.7759\n",
            "03/25/2025 17:24:40 - INFO - __main__ -   Epoch 2, Batch 4700, Avg Loss: 16.7621\n",
            "03/25/2025 17:24:47 - INFO - __main__ -   Epoch 2, Batch 4750, Avg Loss: 16.7678\n",
            "03/25/2025 17:24:54 - INFO - __main__ -   Epoch 2, Batch 4800, Avg Loss: 16.7860\n",
            "03/25/2025 17:25:01 - INFO - __main__ -   Epoch 2, Batch 4850, Avg Loss: 16.7811\n",
            "03/25/2025 17:25:08 - INFO - __main__ -   Epoch 2, Batch 4900, Avg Loss: 16.7765\n",
            "03/25/2025 17:25:15 - INFO - __main__ -   Epoch 2, Batch 4950, Avg Loss: 16.7689\n",
            "03/25/2025 17:25:22 - INFO - __main__ -   Epoch 2, Batch 5000, Avg Loss: 16.7638\n",
            "03/25/2025 17:25:29 - INFO - __main__ -   Epoch 2, Batch 5050, Avg Loss: 16.7498\n",
            "03/25/2025 17:25:36 - INFO - __main__ -   Epoch 2, Batch 5100, Avg Loss: 16.7489\n",
            "03/25/2025 17:25:43 - INFO - __main__ -   Epoch 2, Batch 5150, Avg Loss: 16.7460\n",
            "03/25/2025 17:25:50 - INFO - __main__ -   Epoch 2, Batch 5200, Avg Loss: 16.7297\n",
            "03/25/2025 17:25:57 - INFO - __main__ -   Epoch 2, Batch 5250, Avg Loss: 16.7102\n",
            "03/25/2025 17:26:04 - INFO - __main__ -   Epoch 2, Batch 5300, Avg Loss: 16.7053\n",
            "03/25/2025 17:26:12 - INFO - __main__ -   Epoch 2, Batch 5350, Avg Loss: 16.6842\n",
            "03/25/2025 17:26:19 - INFO - __main__ -   Epoch 2, Batch 5400, Avg Loss: 16.6722\n",
            "03/25/2025 17:26:26 - INFO - __main__ -   Epoch 2, Batch 5450, Avg Loss: 16.6722\n",
            "03/25/2025 17:26:33 - INFO - __main__ -   Epoch 2, Batch 5500, Avg Loss: 16.6603\n",
            "03/25/2025 17:26:40 - INFO - __main__ -   Epoch 2, Batch 5550, Avg Loss: 16.6551\n",
            "03/25/2025 17:26:47 - INFO - __main__ -   Epoch 2, Batch 5600, Avg Loss: 16.6420\n",
            "03/25/2025 17:26:54 - INFO - __main__ -   Epoch 2, Batch 5650, Avg Loss: 16.6517\n",
            "03/25/2025 17:27:01 - INFO - __main__ -   Epoch 2, Batch 5700, Avg Loss: 16.6423\n",
            "03/25/2025 17:27:08 - INFO - __main__ -   Epoch 2, Batch 5750, Avg Loss: 16.6200\n",
            "03/25/2025 17:27:15 - INFO - __main__ -   Epoch 2, Batch 5800, Avg Loss: 16.6034\n",
            "03/25/2025 17:27:22 - INFO - __main__ -   Epoch 2, Batch 5850, Avg Loss: 16.5894\n",
            "03/25/2025 17:27:29 - INFO - __main__ -   Epoch 2, Batch 5900, Avg Loss: 16.5760\n",
            "03/25/2025 17:27:36 - INFO - __main__ -   Epoch 2, Batch 5950, Avg Loss: 16.5708\n",
            "03/25/2025 17:27:43 - INFO - __main__ -   Epoch 2, Batch 6000, Avg Loss: 16.5796\n",
            "03/25/2025 17:27:50 - INFO - __main__ -   Epoch 2, Batch 6050, Avg Loss: 16.5669\n",
            "03/25/2025 17:27:57 - INFO - __main__ -   Epoch 2, Batch 6100, Avg Loss: 16.5575\n",
            "03/25/2025 17:28:04 - INFO - __main__ -   Epoch 2, Batch 6150, Avg Loss: 16.5434\n",
            "03/25/2025 17:28:11 - INFO - __main__ -   Epoch 2, Batch 6200, Avg Loss: 16.5339\n",
            "03/25/2025 17:28:18 - INFO - __main__ -   Epoch 2, Batch 6250, Avg Loss: 16.5312\n",
            "03/25/2025 17:28:25 - INFO - __main__ -   Epoch 2, Batch 6300, Avg Loss: 16.5176\n",
            "03/25/2025 17:28:32 - INFO - __main__ -   Epoch 2, Batch 6350, Avg Loss: 16.5127\n",
            "03/25/2025 17:28:39 - INFO - __main__ -   Epoch 2, Batch 6400, Avg Loss: 16.5049\n",
            "03/25/2025 17:28:46 - INFO - __main__ -   Epoch 2, Batch 6450, Avg Loss: 16.4931\n",
            "03/25/2025 17:28:53 - INFO - __main__ -   Epoch 2, Batch 6500, Avg Loss: 16.4794\n",
            "03/25/2025 17:29:00 - INFO - __main__ -   Epoch 2, Batch 6550, Avg Loss: 16.4921\n",
            "03/25/2025 17:29:07 - INFO - __main__ -   Epoch 2, Batch 6600, Avg Loss: 16.4803\n",
            "03/25/2025 17:29:14 - INFO - __main__ -   Epoch 2, Batch 6650, Avg Loss: 16.4746\n",
            "03/25/2025 17:29:21 - INFO - __main__ -   Epoch 2, Batch 6700, Avg Loss: 16.4711\n",
            "03/25/2025 17:29:28 - INFO - __main__ -   Epoch 2, Batch 6750, Avg Loss: 16.4688\n",
            "03/25/2025 17:29:35 - INFO - __main__ -   Epoch 2, Batch 6800, Avg Loss: 16.4628\n",
            "03/25/2025 17:29:42 - INFO - __main__ -   Epoch 2, Batch 6850, Avg Loss: 16.4482\n",
            "03/25/2025 17:29:49 - INFO - __main__ -   Epoch 2, Batch 6900, Avg Loss: 16.4451\n",
            "03/25/2025 17:29:56 - INFO - __main__ -   Epoch 2, Batch 6950, Avg Loss: 16.4319\n",
            "03/25/2025 17:30:03 - INFO - __main__ -   Epoch 2, Batch 7000, Avg Loss: 16.4280\n",
            "03/25/2025 17:30:10 - INFO - __main__ -   Epoch 2, Batch 7050, Avg Loss: 16.4261\n",
            "03/25/2025 17:30:17 - INFO - __main__ -   Epoch 2, Batch 7100, Avg Loss: 16.4138\n",
            "03/25/2025 17:30:24 - INFO - __main__ -   Epoch 2, Batch 7150, Avg Loss: 16.4080\n",
            "03/25/2025 17:30:31 - INFO - __main__ -   Epoch 2, Batch 7200, Avg Loss: 16.4031\n",
            "03/25/2025 17:30:38 - INFO - __main__ -   Epoch 2, Batch 7250, Avg Loss: 16.4079\n",
            "03/25/2025 17:30:45 - INFO - __main__ -   Epoch 2, Batch 7300, Avg Loss: 16.3996\n",
            "03/25/2025 17:30:52 - INFO - __main__ -   Epoch 2, Batch 7350, Avg Loss: 16.3903\n",
            "03/25/2025 17:30:59 - INFO - __main__ -   Epoch 2, Batch 7400, Avg Loss: 16.3887\n",
            "03/25/2025 17:31:06 - INFO - __main__ -   Epoch 2, Batch 7450, Avg Loss: 16.3942\n",
            "03/25/2025 17:31:13 - INFO - __main__ -   Epoch 2, Batch 7500, Avg Loss: 16.3839\n",
            "03/25/2025 17:31:20 - INFO - __main__ -   Epoch 2, Batch 7550, Avg Loss: 16.3699\n",
            "03/25/2025 17:31:27 - INFO - __main__ -   Epoch 2, Batch 7600, Avg Loss: 16.3677\n",
            "03/25/2025 17:31:34 - INFO - __main__ -   Epoch 2, Batch 7650, Avg Loss: 16.3556\n",
            "03/25/2025 17:31:41 - INFO - __main__ -   Epoch 2, Batch 7700, Avg Loss: 16.3432\n",
            "03/25/2025 17:31:48 - INFO - __main__ -   Epoch 2, Batch 7750, Avg Loss: 16.3591\n",
            "03/25/2025 17:31:55 - INFO - __main__ -   Epoch 2, Batch 7800, Avg Loss: 16.3567\n",
            "03/25/2025 17:32:02 - INFO - __main__ -   Epoch 2, Batch 7850, Avg Loss: 16.3490\n",
            "03/25/2025 17:32:05 - INFO - __main__ -   Epoch 2 completed, Avg Loss: 16.3445\n",
            "03/25/2025 17:32:20 - INFO - __main__ -   Epoch 3, Batch 50, Avg Loss: 14.8943\n",
            "03/25/2025 17:32:27 - INFO - __main__ -   Epoch 3, Batch 100, Avg Loss: 14.5384\n",
            "03/25/2025 17:32:34 - INFO - __main__ -   Epoch 3, Batch 150, Avg Loss: 14.6001\n",
            "03/25/2025 17:32:41 - INFO - __main__ -   Epoch 3, Batch 200, Avg Loss: 14.5261\n",
            "03/25/2025 17:32:48 - INFO - __main__ -   Epoch 3, Batch 250, Avg Loss: 14.6506\n",
            "03/25/2025 17:32:55 - INFO - __main__ -   Epoch 3, Batch 300, Avg Loss: 14.5871\n",
            "03/25/2025 17:33:02 - INFO - __main__ -   Epoch 3, Batch 350, Avg Loss: 14.4323\n",
            "03/25/2025 17:33:09 - INFO - __main__ -   Epoch 3, Batch 400, Avg Loss: 14.4162\n",
            "03/25/2025 17:33:16 - INFO - __main__ -   Epoch 3, Batch 450, Avg Loss: 14.4895\n",
            "03/25/2025 17:33:23 - INFO - __main__ -   Epoch 3, Batch 500, Avg Loss: 14.5034\n",
            "03/25/2025 17:33:30 - INFO - __main__ -   Epoch 3, Batch 550, Avg Loss: 14.5323\n",
            "03/25/2025 17:33:37 - INFO - __main__ -   Epoch 3, Batch 600, Avg Loss: 14.5759\n",
            "03/25/2025 17:33:44 - INFO - __main__ -   Epoch 3, Batch 650, Avg Loss: 14.7387\n",
            "03/25/2025 17:33:51 - INFO - __main__ -   Epoch 3, Batch 700, Avg Loss: 14.6944\n",
            "03/25/2025 17:33:58 - INFO - __main__ -   Epoch 3, Batch 750, Avg Loss: 14.6900\n",
            "03/25/2025 17:34:05 - INFO - __main__ -   Epoch 3, Batch 800, Avg Loss: 14.6831\n",
            "03/25/2025 17:34:12 - INFO - __main__ -   Epoch 3, Batch 850, Avg Loss: 14.6778\n",
            "03/25/2025 17:34:19 - INFO - __main__ -   Epoch 3, Batch 900, Avg Loss: 14.6807\n",
            "03/25/2025 17:34:26 - INFO - __main__ -   Epoch 3, Batch 950, Avg Loss: 14.6759\n",
            "03/25/2025 17:34:33 - INFO - __main__ -   Epoch 3, Batch 1000, Avg Loss: 14.7092\n",
            "03/25/2025 17:34:40 - INFO - __main__ -   Epoch 3, Batch 1050, Avg Loss: 14.6866\n",
            "03/25/2025 17:34:47 - INFO - __main__ -   Epoch 3, Batch 1100, Avg Loss: 14.6423\n",
            "03/25/2025 17:34:54 - INFO - __main__ -   Epoch 3, Batch 1150, Avg Loss: 14.6225\n",
            "03/25/2025 17:35:01 - INFO - __main__ -   Epoch 3, Batch 1200, Avg Loss: 14.6467\n",
            "03/25/2025 17:35:08 - INFO - __main__ -   Epoch 3, Batch 1250, Avg Loss: 14.6308\n",
            "03/25/2025 17:35:15 - INFO - __main__ -   Epoch 3, Batch 1300, Avg Loss: 14.6334\n",
            "03/25/2025 17:35:22 - INFO - __main__ -   Epoch 3, Batch 1350, Avg Loss: 14.5974\n",
            "03/25/2025 17:35:29 - INFO - __main__ -   Epoch 3, Batch 1400, Avg Loss: 14.5950\n",
            "03/25/2025 17:35:36 - INFO - __main__ -   Epoch 3, Batch 1450, Avg Loss: 14.5808\n",
            "03/25/2025 17:35:43 - INFO - __main__ -   Epoch 3, Batch 1500, Avg Loss: 14.5495\n",
            "03/25/2025 17:35:50 - INFO - __main__ -   Epoch 3, Batch 1550, Avg Loss: 14.5089\n",
            "03/25/2025 17:35:57 - INFO - __main__ -   Epoch 3, Batch 1600, Avg Loss: 14.4945\n",
            "03/25/2025 17:36:04 - INFO - __main__ -   Epoch 3, Batch 1650, Avg Loss: 14.4941\n",
            "03/25/2025 17:36:11 - INFO - __main__ -   Epoch 3, Batch 1700, Avg Loss: 14.4635\n",
            "03/25/2025 17:36:18 - INFO - __main__ -   Epoch 3, Batch 1750, Avg Loss: 14.4399\n",
            "03/25/2025 17:36:25 - INFO - __main__ -   Epoch 3, Batch 1800, Avg Loss: 14.4010\n",
            "03/25/2025 17:36:32 - INFO - __main__ -   Epoch 3, Batch 1850, Avg Loss: 14.3751\n",
            "03/25/2025 17:36:39 - INFO - __main__ -   Epoch 3, Batch 1900, Avg Loss: 14.3849\n",
            "03/25/2025 17:36:46 - INFO - __main__ -   Epoch 3, Batch 1950, Avg Loss: 14.3811\n",
            "03/25/2025 17:36:53 - INFO - __main__ -   Epoch 3, Batch 2000, Avg Loss: 14.4039\n",
            "03/25/2025 17:37:00 - INFO - __main__ -   Epoch 3, Batch 2050, Avg Loss: 14.4077\n",
            "03/25/2025 17:37:07 - INFO - __main__ -   Epoch 3, Batch 2100, Avg Loss: 14.3976\n",
            "03/25/2025 17:37:14 - INFO - __main__ -   Epoch 3, Batch 2150, Avg Loss: 14.3824\n",
            "03/25/2025 17:37:21 - INFO - __main__ -   Epoch 3, Batch 2200, Avg Loss: 14.3619\n",
            "03/25/2025 17:37:28 - INFO - __main__ -   Epoch 3, Batch 2250, Avg Loss: 14.3404\n",
            "03/25/2025 17:37:35 - INFO - __main__ -   Epoch 3, Batch 2300, Avg Loss: 14.3297\n",
            "03/25/2025 17:37:42 - INFO - __main__ -   Epoch 3, Batch 2350, Avg Loss: 14.3093\n",
            "03/25/2025 17:37:49 - INFO - __main__ -   Epoch 3, Batch 2400, Avg Loss: 14.2961\n",
            "03/25/2025 17:37:56 - INFO - __main__ -   Epoch 3, Batch 2450, Avg Loss: 14.2788\n",
            "03/25/2025 17:38:03 - INFO - __main__ -   Epoch 3, Batch 2500, Avg Loss: 14.2590\n",
            "03/25/2025 17:38:10 - INFO - __main__ -   Epoch 3, Batch 2550, Avg Loss: 14.2562\n",
            "03/25/2025 17:38:17 - INFO - __main__ -   Epoch 3, Batch 2600, Avg Loss: 14.4559\n",
            "03/25/2025 17:38:24 - INFO - __main__ -   Epoch 3, Batch 2650, Avg Loss: 14.5283\n",
            "03/25/2025 17:38:31 - INFO - __main__ -   Epoch 3, Batch 2700, Avg Loss: 14.5086\n",
            "03/25/2025 17:38:38 - INFO - __main__ -   Epoch 3, Batch 2750, Avg Loss: 14.4830\n",
            "03/25/2025 17:38:45 - INFO - __main__ -   Epoch 3, Batch 2800, Avg Loss: 14.4626\n",
            "03/25/2025 17:38:52 - INFO - __main__ -   Epoch 3, Batch 2850, Avg Loss: 14.4381\n",
            "03/25/2025 17:38:59 - INFO - __main__ -   Epoch 3, Batch 2900, Avg Loss: 14.4101\n",
            "03/25/2025 17:39:06 - INFO - __main__ -   Epoch 3, Batch 2950, Avg Loss: 14.3807\n",
            "03/25/2025 17:39:13 - INFO - __main__ -   Epoch 3, Batch 3000, Avg Loss: 14.3575\n",
            "03/25/2025 17:39:20 - INFO - __main__ -   Epoch 3, Batch 3050, Avg Loss: 14.3371\n",
            "03/25/2025 17:39:27 - INFO - __main__ -   Epoch 3, Batch 3100, Avg Loss: 14.3152\n",
            "03/25/2025 17:39:34 - INFO - __main__ -   Epoch 3, Batch 3150, Avg Loss: 14.3057\n",
            "03/25/2025 17:39:41 - INFO - __main__ -   Epoch 3, Batch 3200, Avg Loss: 14.2832\n",
            "03/25/2025 17:39:48 - INFO - __main__ -   Epoch 3, Batch 3250, Avg Loss: 14.2689\n",
            "03/25/2025 17:39:55 - INFO - __main__ -   Epoch 3, Batch 3300, Avg Loss: 14.3146\n",
            "03/25/2025 17:40:02 - INFO - __main__ -   Epoch 3, Batch 3350, Avg Loss: 14.3127\n",
            "03/25/2025 17:40:09 - INFO - __main__ -   Epoch 3, Batch 3400, Avg Loss: 14.3275\n",
            "03/25/2025 17:40:16 - INFO - __main__ -   Epoch 3, Batch 3450, Avg Loss: 14.3206\n",
            "03/25/2025 17:40:23 - INFO - __main__ -   Epoch 3, Batch 3500, Avg Loss: 14.3233\n",
            "03/25/2025 17:40:30 - INFO - __main__ -   Epoch 3, Batch 3550, Avg Loss: 14.3315\n",
            "03/25/2025 17:40:37 - INFO - __main__ -   Epoch 3, Batch 3600, Avg Loss: 14.3280\n",
            "03/25/2025 17:40:44 - INFO - __main__ -   Epoch 3, Batch 3650, Avg Loss: 14.3368\n",
            "03/25/2025 17:40:51 - INFO - __main__ -   Epoch 3, Batch 3700, Avg Loss: 14.3443\n",
            "03/25/2025 17:40:58 - INFO - __main__ -   Epoch 3, Batch 3750, Avg Loss: 14.3331\n",
            "03/25/2025 17:41:05 - INFO - __main__ -   Epoch 3, Batch 3800, Avg Loss: 14.3319\n",
            "03/25/2025 17:41:12 - INFO - __main__ -   Epoch 3, Batch 3850, Avg Loss: 14.3390\n",
            "03/25/2025 17:41:19 - INFO - __main__ -   Epoch 3, Batch 3900, Avg Loss: 14.3370\n",
            "03/25/2025 17:41:26 - INFO - __main__ -   Epoch 3, Batch 3950, Avg Loss: 14.3344\n",
            "03/25/2025 17:41:33 - INFO - __main__ -   Epoch 3, Batch 4000, Avg Loss: 14.3229\n",
            "03/25/2025 17:41:40 - INFO - __main__ -   Epoch 3, Batch 4050, Avg Loss: 14.3202\n",
            "03/25/2025 17:41:47 - INFO - __main__ -   Epoch 3, Batch 4100, Avg Loss: 14.3100\n",
            "03/25/2025 17:41:54 - INFO - __main__ -   Epoch 3, Batch 4150, Avg Loss: 14.3019\n",
            "03/25/2025 17:42:01 - INFO - __main__ -   Epoch 3, Batch 4200, Avg Loss: 14.3262\n",
            "03/25/2025 17:42:08 - INFO - __main__ -   Epoch 3, Batch 4250, Avg Loss: 14.3251\n",
            "03/25/2025 17:42:15 - INFO - __main__ -   Epoch 3, Batch 4300, Avg Loss: 14.3283\n",
            "03/25/2025 17:42:22 - INFO - __main__ -   Epoch 3, Batch 4350, Avg Loss: 14.3177\n",
            "03/25/2025 17:42:29 - INFO - __main__ -   Epoch 3, Batch 4400, Avg Loss: 14.3147\n",
            "03/25/2025 17:42:36 - INFO - __main__ -   Epoch 3, Batch 4450, Avg Loss: 14.3186\n",
            "03/25/2025 17:42:43 - INFO - __main__ -   Epoch 3, Batch 4500, Avg Loss: 14.3143\n",
            "03/25/2025 17:42:50 - INFO - __main__ -   Epoch 3, Batch 4550, Avg Loss: 14.3160\n",
            "03/25/2025 17:42:57 - INFO - __main__ -   Epoch 3, Batch 4600, Avg Loss: 14.3078\n",
            "03/25/2025 17:43:04 - INFO - __main__ -   Epoch 3, Batch 4650, Avg Loss: 14.2968\n",
            "03/25/2025 17:43:11 - INFO - __main__ -   Epoch 3, Batch 4700, Avg Loss: 14.2894\n",
            "03/25/2025 17:43:18 - INFO - __main__ -   Epoch 3, Batch 4750, Avg Loss: 14.2899\n",
            "03/25/2025 17:43:25 - INFO - __main__ -   Epoch 3, Batch 4800, Avg Loss: 14.3139\n",
            "03/25/2025 17:43:32 - INFO - __main__ -   Epoch 3, Batch 4850, Avg Loss: 14.3112\n",
            "03/25/2025 17:43:39 - INFO - __main__ -   Epoch 3, Batch 4900, Avg Loss: 14.3124\n",
            "03/25/2025 17:43:46 - INFO - __main__ -   Epoch 3, Batch 4950, Avg Loss: 14.3060\n",
            "03/25/2025 17:43:53 - INFO - __main__ -   Epoch 3, Batch 5000, Avg Loss: 14.3037\n",
            "03/25/2025 17:44:00 - INFO - __main__ -   Epoch 3, Batch 5050, Avg Loss: 14.2975\n",
            "03/25/2025 17:44:07 - INFO - __main__ -   Epoch 3, Batch 5100, Avg Loss: 14.3031\n",
            "03/25/2025 17:44:14 - INFO - __main__ -   Epoch 3, Batch 5150, Avg Loss: 14.3078\n",
            "03/25/2025 17:44:21 - INFO - __main__ -   Epoch 3, Batch 5200, Avg Loss: 14.2987\n",
            "03/25/2025 17:44:28 - INFO - __main__ -   Epoch 3, Batch 5250, Avg Loss: 14.2844\n",
            "03/25/2025 17:44:35 - INFO - __main__ -   Epoch 3, Batch 5300, Avg Loss: 14.2818\n",
            "03/25/2025 17:44:42 - INFO - __main__ -   Epoch 3, Batch 5350, Avg Loss: 14.2690\n",
            "03/25/2025 17:44:49 - INFO - __main__ -   Epoch 3, Batch 5400, Avg Loss: 14.2654\n",
            "03/25/2025 17:44:56 - INFO - __main__ -   Epoch 3, Batch 5450, Avg Loss: 14.2708\n",
            "03/25/2025 17:45:03 - INFO - __main__ -   Epoch 3, Batch 5500, Avg Loss: 14.2644\n",
            "03/25/2025 17:45:10 - INFO - __main__ -   Epoch 3, Batch 5550, Avg Loss: 14.2640\n",
            "03/25/2025 17:45:17 - INFO - __main__ -   Epoch 3, Batch 5600, Avg Loss: 14.2572\n",
            "03/25/2025 17:45:24 - INFO - __main__ -   Epoch 3, Batch 5650, Avg Loss: 14.2676\n",
            "03/25/2025 17:45:31 - INFO - __main__ -   Epoch 3, Batch 5700, Avg Loss: 14.2619\n",
            "03/25/2025 17:45:38 - INFO - __main__ -   Epoch 3, Batch 5750, Avg Loss: 14.2460\n",
            "03/25/2025 17:45:45 - INFO - __main__ -   Epoch 3, Batch 5800, Avg Loss: 14.2343\n",
            "03/25/2025 17:45:52 - INFO - __main__ -   Epoch 3, Batch 5850, Avg Loss: 14.2251\n",
            "03/25/2025 17:45:59 - INFO - __main__ -   Epoch 3, Batch 5900, Avg Loss: 14.2168\n",
            "03/25/2025 17:46:06 - INFO - __main__ -   Epoch 3, Batch 5950, Avg Loss: 14.2142\n",
            "03/25/2025 17:46:13 - INFO - __main__ -   Epoch 3, Batch 6000, Avg Loss: 14.2220\n",
            "03/25/2025 17:46:20 - INFO - __main__ -   Epoch 3, Batch 6050, Avg Loss: 14.2140\n",
            "03/25/2025 17:46:27 - INFO - __main__ -   Epoch 3, Batch 6100, Avg Loss: 14.2084\n",
            "03/25/2025 17:46:34 - INFO - __main__ -   Epoch 3, Batch 6150, Avg Loss: 14.1997\n",
            "03/25/2025 17:46:41 - INFO - __main__ -   Epoch 3, Batch 6200, Avg Loss: 14.1952\n",
            "03/25/2025 17:46:48 - INFO - __main__ -   Epoch 3, Batch 6250, Avg Loss: 14.1937\n",
            "03/25/2025 17:46:55 - INFO - __main__ -   Epoch 3, Batch 6300, Avg Loss: 14.1813\n",
            "03/25/2025 17:47:02 - INFO - __main__ -   Epoch 3, Batch 6350, Avg Loss: 14.1827\n",
            "03/25/2025 17:47:09 - INFO - __main__ -   Epoch 3, Batch 6400, Avg Loss: 14.1793\n",
            "03/25/2025 17:47:16 - INFO - __main__ -   Epoch 3, Batch 6450, Avg Loss: 14.1750\n",
            "03/25/2025 17:47:23 - INFO - __main__ -   Epoch 3, Batch 6500, Avg Loss: 14.1673\n",
            "03/25/2025 17:47:30 - INFO - __main__ -   Epoch 3, Batch 6550, Avg Loss: 14.1863\n",
            "03/25/2025 17:47:37 - INFO - __main__ -   Epoch 3, Batch 6600, Avg Loss: 14.1812\n",
            "03/25/2025 17:47:44 - INFO - __main__ -   Epoch 3, Batch 6650, Avg Loss: 14.1808\n",
            "03/25/2025 17:47:51 - INFO - __main__ -   Epoch 3, Batch 6700, Avg Loss: 14.1849\n",
            "03/25/2025 17:47:58 - INFO - __main__ -   Epoch 3, Batch 6750, Avg Loss: 14.1854\n",
            "03/25/2025 17:48:05 - INFO - __main__ -   Epoch 3, Batch 6800, Avg Loss: 14.1826\n",
            "03/25/2025 17:48:12 - INFO - __main__ -   Epoch 3, Batch 6850, Avg Loss: 14.1774\n",
            "03/25/2025 17:48:19 - INFO - __main__ -   Epoch 3, Batch 6900, Avg Loss: 14.1786\n",
            "03/25/2025 17:48:26 - INFO - __main__ -   Epoch 3, Batch 6950, Avg Loss: 14.1708\n",
            "03/25/2025 17:48:33 - INFO - __main__ -   Epoch 3, Batch 7000, Avg Loss: 14.1732\n",
            "03/25/2025 17:48:40 - INFO - __main__ -   Epoch 3, Batch 7050, Avg Loss: 14.1763\n",
            "03/25/2025 17:48:47 - INFO - __main__ -   Epoch 3, Batch 7100, Avg Loss: 14.1691\n",
            "03/25/2025 17:48:54 - INFO - __main__ -   Epoch 3, Batch 7150, Avg Loss: 14.1679\n",
            "03/25/2025 17:49:01 - INFO - __main__ -   Epoch 3, Batch 7200, Avg Loss: 14.1699\n",
            "03/25/2025 17:49:08 - INFO - __main__ -   Epoch 3, Batch 7250, Avg Loss: 14.1787\n",
            "03/25/2025 17:49:15 - INFO - __main__ -   Epoch 3, Batch 7300, Avg Loss: 14.1770\n",
            "03/25/2025 17:49:22 - INFO - __main__ -   Epoch 3, Batch 7350, Avg Loss: 14.1722\n",
            "03/25/2025 17:49:30 - INFO - __main__ -   Epoch 3, Batch 7400, Avg Loss: 14.1740\n",
            "03/25/2025 17:49:37 - INFO - __main__ -   Epoch 3, Batch 7450, Avg Loss: 14.1837\n",
            "03/25/2025 17:49:44 - INFO - __main__ -   Epoch 3, Batch 7500, Avg Loss: 14.1787\n",
            "03/25/2025 17:49:51 - INFO - __main__ -   Epoch 3, Batch 7550, Avg Loss: 14.1690\n",
            "03/25/2025 17:49:58 - INFO - __main__ -   Epoch 3, Batch 7600, Avg Loss: 14.1690\n",
            "03/25/2025 17:50:05 - INFO - __main__ -   Epoch 3, Batch 7650, Avg Loss: 14.1608\n",
            "03/25/2025 17:50:12 - INFO - __main__ -   Epoch 3, Batch 7700, Avg Loss: 14.1530\n",
            "03/25/2025 17:50:19 - INFO - __main__ -   Epoch 3, Batch 7750, Avg Loss: 14.1721\n",
            "03/25/2025 17:50:26 - INFO - __main__ -   Epoch 3, Batch 7800, Avg Loss: 14.1736\n",
            "03/25/2025 17:50:33 - INFO - __main__ -   Epoch 3, Batch 7850, Avg Loss: 14.1701\n",
            "03/25/2025 17:50:35 - INFO - __main__ -   Epoch 3 completed, Avg Loss: 14.1686\n",
            "03/25/2025 17:50:48 - INFO - __main__ -   Epoch 4, Batch 50, Avg Loss: 13.4671\n",
            "03/25/2025 17:50:55 - INFO - __main__ -   Epoch 4, Batch 100, Avg Loss: 13.2559\n",
            "03/25/2025 17:51:02 - INFO - __main__ -   Epoch 4, Batch 150, Avg Loss: 13.2841\n",
            "03/25/2025 17:51:09 - INFO - __main__ -   Epoch 4, Batch 200, Avg Loss: 13.1824\n",
            "03/25/2025 17:51:16 - INFO - __main__ -   Epoch 4, Batch 250, Avg Loss: 13.2662\n",
            "03/25/2025 17:51:23 - INFO - __main__ -   Epoch 4, Batch 300, Avg Loss: 13.2060\n",
            "03/25/2025 17:51:30 - INFO - __main__ -   Epoch 4, Batch 350, Avg Loss: 13.0959\n",
            "03/25/2025 17:51:37 - INFO - __main__ -   Epoch 4, Batch 400, Avg Loss: 13.0755\n",
            "03/25/2025 17:51:44 - INFO - __main__ -   Epoch 4, Batch 450, Avg Loss: 13.1445\n",
            "03/25/2025 17:51:51 - INFO - __main__ -   Epoch 4, Batch 500, Avg Loss: 13.1487\n",
            "03/25/2025 17:51:58 - INFO - __main__ -   Epoch 4, Batch 550, Avg Loss: 13.1762\n",
            "03/25/2025 17:52:05 - INFO - __main__ -   Epoch 4, Batch 600, Avg Loss: 13.2248\n",
            "03/25/2025 17:52:12 - INFO - __main__ -   Epoch 4, Batch 650, Avg Loss: 13.4088\n",
            "03/25/2025 17:52:19 - INFO - __main__ -   Epoch 4, Batch 700, Avg Loss: 13.3640\n",
            "03/25/2025 17:52:26 - INFO - __main__ -   Epoch 4, Batch 750, Avg Loss: 13.3703\n",
            "03/25/2025 17:52:33 - INFO - __main__ -   Epoch 4, Batch 800, Avg Loss: 13.3626\n",
            "03/25/2025 17:52:40 - INFO - __main__ -   Epoch 4, Batch 850, Avg Loss: 13.3709\n",
            "03/25/2025 17:52:47 - INFO - __main__ -   Epoch 4, Batch 900, Avg Loss: 13.3771\n",
            "03/25/2025 17:52:54 - INFO - __main__ -   Epoch 4, Batch 950, Avg Loss: 13.3791\n",
            "03/25/2025 17:53:01 - INFO - __main__ -   Epoch 4, Batch 1000, Avg Loss: 13.4180\n",
            "03/25/2025 17:53:08 - INFO - __main__ -   Epoch 4, Batch 1050, Avg Loss: 13.3986\n",
            "03/25/2025 17:53:15 - INFO - __main__ -   Epoch 4, Batch 1100, Avg Loss: 13.3603\n",
            "03/25/2025 17:53:22 - INFO - __main__ -   Epoch 4, Batch 1150, Avg Loss: 13.3479\n",
            "03/25/2025 17:53:29 - INFO - __main__ -   Epoch 4, Batch 1200, Avg Loss: 13.3762\n",
            "03/25/2025 17:53:36 - INFO - __main__ -   Epoch 4, Batch 1250, Avg Loss: 13.3672\n",
            "03/25/2025 17:53:43 - INFO - __main__ -   Epoch 4, Batch 1300, Avg Loss: 13.3759\n",
            "03/25/2025 17:53:50 - INFO - __main__ -   Epoch 4, Batch 1350, Avg Loss: 13.3513\n",
            "03/25/2025 17:53:57 - INFO - __main__ -   Epoch 4, Batch 1400, Avg Loss: 13.3512\n",
            "03/25/2025 17:54:04 - INFO - __main__ -   Epoch 4, Batch 1450, Avg Loss: 13.3415\n",
            "03/25/2025 17:54:11 - INFO - __main__ -   Epoch 4, Batch 1500, Avg Loss: 13.3171\n",
            "03/25/2025 17:54:18 - INFO - __main__ -   Epoch 4, Batch 1550, Avg Loss: 13.2816\n",
            "03/25/2025 17:54:25 - INFO - __main__ -   Epoch 4, Batch 1600, Avg Loss: 13.2723\n",
            "03/25/2025 17:54:32 - INFO - __main__ -   Epoch 4, Batch 1650, Avg Loss: 13.2757\n",
            "03/25/2025 17:54:39 - INFO - __main__ -   Epoch 4, Batch 1700, Avg Loss: 13.2458\n",
            "03/25/2025 17:54:46 - INFO - __main__ -   Epoch 4, Batch 1750, Avg Loss: 13.2215\n",
            "03/25/2025 17:54:53 - INFO - __main__ -   Epoch 4, Batch 1800, Avg Loss: 13.1897\n",
            "03/25/2025 17:55:00 - INFO - __main__ -   Epoch 4, Batch 1850, Avg Loss: 13.1649\n",
            "03/25/2025 17:55:07 - INFO - __main__ -   Epoch 4, Batch 1900, Avg Loss: 13.1787\n",
            "03/25/2025 17:55:14 - INFO - __main__ -   Epoch 4, Batch 1950, Avg Loss: 13.1761\n",
            "03/25/2025 17:55:21 - INFO - __main__ -   Epoch 4, Batch 2000, Avg Loss: 13.1926\n",
            "03/25/2025 17:55:28 - INFO - __main__ -   Epoch 4, Batch 2050, Avg Loss: 13.1940\n",
            "03/25/2025 17:55:35 - INFO - __main__ -   Epoch 4, Batch 2100, Avg Loss: 13.1858\n",
            "03/25/2025 17:55:42 - INFO - __main__ -   Epoch 4, Batch 2150, Avg Loss: 13.1690\n",
            "03/25/2025 17:55:49 - INFO - __main__ -   Epoch 4, Batch 2200, Avg Loss: 13.1501\n",
            "03/25/2025 17:55:56 - INFO - __main__ -   Epoch 4, Batch 2250, Avg Loss: 13.1300\n",
            "03/25/2025 17:56:04 - INFO - __main__ -   Epoch 4, Batch 2300, Avg Loss: 13.1199\n",
            "03/25/2025 17:56:11 - INFO - __main__ -   Epoch 4, Batch 2350, Avg Loss: 13.1050\n",
            "03/25/2025 17:56:18 - INFO - __main__ -   Epoch 4, Batch 2400, Avg Loss: 13.0898\n",
            "03/25/2025 17:56:25 - INFO - __main__ -   Epoch 4, Batch 2450, Avg Loss: 13.0743\n",
            "03/25/2025 17:56:32 - INFO - __main__ -   Epoch 4, Batch 2500, Avg Loss: 13.0559\n",
            "03/25/2025 17:56:39 - INFO - __main__ -   Epoch 4, Batch 2550, Avg Loss: 13.0575\n",
            "03/25/2025 17:56:46 - INFO - __main__ -   Epoch 4, Batch 2600, Avg Loss: 13.1998\n",
            "03/25/2025 17:56:53 - INFO - __main__ -   Epoch 4, Batch 2650, Avg Loss: 13.2513\n",
            "03/25/2025 17:57:00 - INFO - __main__ -   Epoch 4, Batch 2700, Avg Loss: 13.2344\n",
            "03/25/2025 17:57:07 - INFO - __main__ -   Epoch 4, Batch 2750, Avg Loss: 13.2109\n",
            "03/25/2025 17:57:14 - INFO - __main__ -   Epoch 4, Batch 2800, Avg Loss: 13.1921\n",
            "03/25/2025 17:57:21 - INFO - __main__ -   Epoch 4, Batch 2850, Avg Loss: 13.1728\n",
            "03/25/2025 17:57:28 - INFO - __main__ -   Epoch 4, Batch 2900, Avg Loss: 13.1508\n",
            "03/25/2025 17:57:35 - INFO - __main__ -   Epoch 4, Batch 2950, Avg Loss: 13.1254\n",
            "03/25/2025 17:57:42 - INFO - __main__ -   Epoch 4, Batch 3000, Avg Loss: 13.1063\n",
            "03/25/2025 17:57:49 - INFO - __main__ -   Epoch 4, Batch 3050, Avg Loss: 13.0911\n",
            "03/25/2025 17:57:56 - INFO - __main__ -   Epoch 4, Batch 3100, Avg Loss: 13.0724\n",
            "03/25/2025 17:58:03 - INFO - __main__ -   Epoch 4, Batch 3150, Avg Loss: 13.0611\n",
            "03/25/2025 17:58:10 - INFO - __main__ -   Epoch 4, Batch 3200, Avg Loss: 13.0462\n",
            "03/25/2025 17:58:17 - INFO - __main__ -   Epoch 4, Batch 3250, Avg Loss: 13.0347\n",
            "03/25/2025 17:58:24 - INFO - __main__ -   Epoch 4, Batch 3300, Avg Loss: 13.0789\n",
            "03/25/2025 17:58:31 - INFO - __main__ -   Epoch 4, Batch 3350, Avg Loss: 13.0819\n",
            "03/25/2025 17:58:38 - INFO - __main__ -   Epoch 4, Batch 3400, Avg Loss: 13.0998\n",
            "03/25/2025 17:58:45 - INFO - __main__ -   Epoch 4, Batch 3450, Avg Loss: 13.0939\n",
            "03/25/2025 17:58:52 - INFO - __main__ -   Epoch 4, Batch 3500, Avg Loss: 13.0982\n",
            "03/25/2025 17:58:59 - INFO - __main__ -   Epoch 4, Batch 3550, Avg Loss: 13.1066\n",
            "03/25/2025 17:59:06 - INFO - __main__ -   Epoch 4, Batch 3600, Avg Loss: 13.1074\n",
            "03/25/2025 17:59:13 - INFO - __main__ -   Epoch 4, Batch 3650, Avg Loss: 13.1167\n",
            "03/25/2025 17:59:20 - INFO - __main__ -   Epoch 4, Batch 3700, Avg Loss: 13.1248\n",
            "03/25/2025 17:59:27 - INFO - __main__ -   Epoch 4, Batch 3750, Avg Loss: 13.1178\n",
            "03/25/2025 17:59:34 - INFO - __main__ -   Epoch 4, Batch 3800, Avg Loss: 13.1208\n",
            "03/25/2025 17:59:41 - INFO - __main__ -   Epoch 4, Batch 3850, Avg Loss: 13.1277\n",
            "03/25/2025 17:59:48 - INFO - __main__ -   Epoch 4, Batch 3900, Avg Loss: 13.1288\n",
            "03/25/2025 17:59:55 - INFO - __main__ -   Epoch 4, Batch 3950, Avg Loss: 13.1308\n",
            "03/25/2025 18:00:02 - INFO - __main__ -   Epoch 4, Batch 4000, Avg Loss: 13.1228\n",
            "03/25/2025 18:00:09 - INFO - __main__ -   Epoch 4, Batch 4050, Avg Loss: 13.1226\n",
            "03/25/2025 18:00:16 - INFO - __main__ -   Epoch 4, Batch 4100, Avg Loss: 13.1189\n",
            "03/25/2025 18:00:23 - INFO - __main__ -   Epoch 4, Batch 4150, Avg Loss: 13.1151\n",
            "03/25/2025 18:00:30 - INFO - __main__ -   Epoch 4, Batch 4200, Avg Loss: 13.1325\n",
            "03/25/2025 18:00:37 - INFO - __main__ -   Epoch 4, Batch 4250, Avg Loss: 13.1374\n",
            "03/25/2025 18:00:44 - INFO - __main__ -   Epoch 4, Batch 4300, Avg Loss: 13.1454\n",
            "03/25/2025 18:00:51 - INFO - __main__ -   Epoch 4, Batch 4350, Avg Loss: 13.1375\n",
            "03/25/2025 18:00:58 - INFO - __main__ -   Epoch 4, Batch 4400, Avg Loss: 13.1356\n",
            "03/25/2025 18:01:05 - INFO - __main__ -   Epoch 4, Batch 4450, Avg Loss: 13.1430\n",
            "03/25/2025 18:01:12 - INFO - __main__ -   Epoch 4, Batch 4500, Avg Loss: 13.1418\n",
            "03/25/2025 18:01:19 - INFO - __main__ -   Epoch 4, Batch 4550, Avg Loss: 13.1461\n",
            "03/25/2025 18:01:26 - INFO - __main__ -   Epoch 4, Batch 4600, Avg Loss: 13.1397\n",
            "03/25/2025 18:01:33 - INFO - __main__ -   Epoch 4, Batch 4650, Avg Loss: 13.1286\n",
            "03/25/2025 18:01:40 - INFO - __main__ -   Epoch 4, Batch 4700, Avg Loss: 13.1253\n",
            "03/25/2025 18:01:47 - INFO - __main__ -   Epoch 4, Batch 4750, Avg Loss: 13.1263\n",
            "03/25/2025 18:01:54 - INFO - __main__ -   Epoch 4, Batch 4800, Avg Loss: 13.1495\n",
            "03/25/2025 18:02:01 - INFO - __main__ -   Epoch 4, Batch 4850, Avg Loss: 13.1491\n",
            "03/25/2025 18:02:08 - INFO - __main__ -   Epoch 4, Batch 4900, Avg Loss: 13.1513\n",
            "03/25/2025 18:02:15 - INFO - __main__ -   Epoch 4, Batch 4950, Avg Loss: 13.1484\n",
            "03/25/2025 18:02:22 - INFO - __main__ -   Epoch 4, Batch 5000, Avg Loss: 13.1492\n",
            "03/25/2025 18:02:29 - INFO - __main__ -   Epoch 4, Batch 5050, Avg Loss: 13.1459\n",
            "03/25/2025 18:02:36 - INFO - __main__ -   Epoch 4, Batch 5100, Avg Loss: 13.1538\n",
            "03/25/2025 18:02:43 - INFO - __main__ -   Epoch 4, Batch 5150, Avg Loss: 13.1625\n",
            "03/25/2025 18:02:50 - INFO - __main__ -   Epoch 4, Batch 5200, Avg Loss: 13.1560\n",
            "03/25/2025 18:02:57 - INFO - __main__ -   Epoch 4, Batch 5250, Avg Loss: 13.1440\n",
            "03/25/2025 18:03:04 - INFO - __main__ -   Epoch 4, Batch 5300, Avg Loss: 13.1441\n",
            "03/25/2025 18:03:11 - INFO - __main__ -   Epoch 4, Batch 5350, Avg Loss: 13.1337\n",
            "03/25/2025 18:03:18 - INFO - __main__ -   Epoch 4, Batch 5400, Avg Loss: 13.1308\n",
            "03/25/2025 18:03:25 - INFO - __main__ -   Epoch 4, Batch 5450, Avg Loss: 13.1345\n",
            "03/25/2025 18:03:32 - INFO - __main__ -   Epoch 4, Batch 5500, Avg Loss: 13.1294\n",
            "03/25/2025 18:03:39 - INFO - __main__ -   Epoch 4, Batch 5550, Avg Loss: 13.1302\n",
            "03/25/2025 18:03:46 - INFO - __main__ -   Epoch 4, Batch 5600, Avg Loss: 13.1246\n",
            "03/25/2025 18:03:53 - INFO - __main__ -   Epoch 4, Batch 5650, Avg Loss: 13.1369\n",
            "03/25/2025 18:04:00 - INFO - __main__ -   Epoch 4, Batch 5700, Avg Loss: 13.1317\n",
            "03/25/2025 18:04:07 - INFO - __main__ -   Epoch 4, Batch 5750, Avg Loss: 13.1186\n",
            "03/25/2025 18:04:14 - INFO - __main__ -   Epoch 4, Batch 5800, Avg Loss: 13.1103\n",
            "03/25/2025 18:04:21 - INFO - __main__ -   Epoch 4, Batch 5850, Avg Loss: 13.1049\n",
            "03/25/2025 18:04:28 - INFO - __main__ -   Epoch 4, Batch 5900, Avg Loss: 13.0978\n",
            "03/25/2025 18:04:35 - INFO - __main__ -   Epoch 4, Batch 5950, Avg Loss: 13.0974\n",
            "03/25/2025 18:04:42 - INFO - __main__ -   Epoch 4, Batch 6000, Avg Loss: 13.1059\n",
            "03/25/2025 18:04:49 - INFO - __main__ -   Epoch 4, Batch 6050, Avg Loss: 13.1000\n",
            "03/25/2025 18:04:56 - INFO - __main__ -   Epoch 4, Batch 6100, Avg Loss: 13.0966\n",
            "03/25/2025 18:05:03 - INFO - __main__ -   Epoch 4, Batch 6150, Avg Loss: 13.0904\n",
            "03/25/2025 18:05:10 - INFO - __main__ -   Epoch 4, Batch 6200, Avg Loss: 13.0883\n",
            "03/25/2025 18:05:17 - INFO - __main__ -   Epoch 4, Batch 6250, Avg Loss: 13.0901\n",
            "03/25/2025 18:05:24 - INFO - __main__ -   Epoch 4, Batch 6300, Avg Loss: 13.0795\n",
            "03/25/2025 18:05:31 - INFO - __main__ -   Epoch 4, Batch 6350, Avg Loss: 13.0826\n",
            "03/25/2025 18:05:38 - INFO - __main__ -   Epoch 4, Batch 6400, Avg Loss: 13.0812\n",
            "03/25/2025 18:05:45 - INFO - __main__ -   Epoch 4, Batch 6450, Avg Loss: 13.0790\n",
            "03/25/2025 18:05:52 - INFO - __main__ -   Epoch 4, Batch 6500, Avg Loss: 13.0728\n",
            "03/25/2025 18:05:59 - INFO - __main__ -   Epoch 4, Batch 6550, Avg Loss: 13.0910\n",
            "03/25/2025 18:06:06 - INFO - __main__ -   Epoch 4, Batch 6600, Avg Loss: 13.0891\n",
            "03/25/2025 18:06:13 - INFO - __main__ -   Epoch 4, Batch 6650, Avg Loss: 13.0896\n",
            "03/25/2025 18:06:20 - INFO - __main__ -   Epoch 4, Batch 6700, Avg Loss: 13.0949\n",
            "03/25/2025 18:06:27 - INFO - __main__ -   Epoch 4, Batch 6750, Avg Loss: 13.0969\n",
            "03/25/2025 18:06:34 - INFO - __main__ -   Epoch 4, Batch 6800, Avg Loss: 13.0957\n",
            "03/25/2025 18:06:41 - INFO - __main__ -   Epoch 4, Batch 6850, Avg Loss: 13.0944\n",
            "03/25/2025 18:06:48 - INFO - __main__ -   Epoch 4, Batch 6900, Avg Loss: 13.0965\n",
            "03/25/2025 18:06:55 - INFO - __main__ -   Epoch 4, Batch 6950, Avg Loss: 13.0906\n",
            "03/25/2025 18:07:02 - INFO - __main__ -   Epoch 4, Batch 7000, Avg Loss: 13.0952\n",
            "03/25/2025 18:07:09 - INFO - __main__ -   Epoch 4, Batch 7050, Avg Loss: 13.0998\n",
            "03/25/2025 18:07:16 - INFO - __main__ -   Epoch 4, Batch 7100, Avg Loss: 13.0946\n",
            "03/25/2025 18:07:23 - INFO - __main__ -   Epoch 4, Batch 7150, Avg Loss: 13.0950\n",
            "03/25/2025 18:07:30 - INFO - __main__ -   Epoch 4, Batch 7200, Avg Loss: 13.0984\n",
            "03/25/2025 18:07:37 - INFO - __main__ -   Epoch 4, Batch 7250, Avg Loss: 13.1091\n",
            "03/25/2025 18:07:44 - INFO - __main__ -   Epoch 4, Batch 7300, Avg Loss: 13.1090\n",
            "03/25/2025 18:07:51 - INFO - __main__ -   Epoch 4, Batch 7350, Avg Loss: 13.1051\n",
            "03/25/2025 18:07:58 - INFO - __main__ -   Epoch 4, Batch 7400, Avg Loss: 13.1071\n",
            "03/25/2025 18:08:05 - INFO - __main__ -   Epoch 4, Batch 7450, Avg Loss: 13.1173\n",
            "03/25/2025 18:08:12 - INFO - __main__ -   Epoch 4, Batch 7500, Avg Loss: 13.1137\n",
            "03/25/2025 18:08:19 - INFO - __main__ -   Epoch 4, Batch 7550, Avg Loss: 13.1060\n",
            "03/25/2025 18:08:26 - INFO - __main__ -   Epoch 4, Batch 7600, Avg Loss: 13.1093\n",
            "03/25/2025 18:08:33 - INFO - __main__ -   Epoch 4, Batch 7650, Avg Loss: 13.1029\n",
            "03/25/2025 18:08:40 - INFO - __main__ -   Epoch 4, Batch 7700, Avg Loss: 13.0967\n",
            "03/25/2025 18:08:47 - INFO - __main__ -   Epoch 4, Batch 7750, Avg Loss: 13.1170\n",
            "03/25/2025 18:08:54 - INFO - __main__ -   Epoch 4, Batch 7800, Avg Loss: 13.1197\n",
            "03/25/2025 18:09:01 - INFO - __main__ -   Epoch 4, Batch 7850, Avg Loss: 13.1172\n",
            "03/25/2025 18:09:04 - INFO - __main__ -   Epoch 4 completed, Avg Loss: 13.1174\n",
            "03/25/2025 18:09:20 - INFO - __main__ -   Epoch 5, Batch 50, Avg Loss: 12.6927\n",
            "03/25/2025 18:09:27 - INFO - __main__ -   Epoch 5, Batch 100, Avg Loss: 12.5255\n",
            "03/25/2025 18:09:34 - INFO - __main__ -   Epoch 5, Batch 150, Avg Loss: 12.5654\n",
            "03/25/2025 18:09:41 - INFO - __main__ -   Epoch 5, Batch 200, Avg Loss: 12.4810\n",
            "03/25/2025 18:09:48 - INFO - __main__ -   Epoch 5, Batch 250, Avg Loss: 12.5423\n",
            "03/25/2025 18:09:55 - INFO - __main__ -   Epoch 5, Batch 300, Avg Loss: 12.4802\n",
            "03/25/2025 18:10:02 - INFO - __main__ -   Epoch 5, Batch 350, Avg Loss: 12.3643\n",
            "03/25/2025 18:10:09 - INFO - __main__ -   Epoch 5, Batch 400, Avg Loss: 12.3518\n",
            "03/25/2025 18:10:16 - INFO - __main__ -   Epoch 5, Batch 450, Avg Loss: 12.4457\n",
            "03/25/2025 18:10:23 - INFO - __main__ -   Epoch 5, Batch 500, Avg Loss: 12.4596\n",
            "03/25/2025 18:10:30 - INFO - __main__ -   Epoch 5, Batch 550, Avg Loss: 12.4914\n",
            "03/25/2025 18:10:37 - INFO - __main__ -   Epoch 5, Batch 600, Avg Loss: 12.5703\n",
            "03/25/2025 18:10:44 - INFO - __main__ -   Epoch 5, Batch 650, Avg Loss: 12.7606\n",
            "03/25/2025 18:10:51 - INFO - __main__ -   Epoch 5, Batch 700, Avg Loss: 12.7141\n",
            "03/25/2025 18:10:58 - INFO - __main__ -   Epoch 5, Batch 750, Avg Loss: 12.7283\n",
            "03/25/2025 18:11:05 - INFO - __main__ -   Epoch 5, Batch 800, Avg Loss: 12.7215\n",
            "03/25/2025 18:11:12 - INFO - __main__ -   Epoch 5, Batch 850, Avg Loss: 12.7300\n",
            "03/25/2025 18:11:19 - INFO - __main__ -   Epoch 5, Batch 900, Avg Loss: 12.7481\n",
            "03/25/2025 18:11:26 - INFO - __main__ -   Epoch 5, Batch 950, Avg Loss: 12.7510\n",
            "03/25/2025 18:11:33 - INFO - __main__ -   Epoch 5, Batch 1000, Avg Loss: 12.7870\n",
            "03/25/2025 18:11:40 - INFO - __main__ -   Epoch 5, Batch 1050, Avg Loss: 12.7647\n",
            "03/25/2025 18:11:47 - INFO - __main__ -   Epoch 5, Batch 1100, Avg Loss: 12.7327\n",
            "03/25/2025 18:11:54 - INFO - __main__ -   Epoch 5, Batch 1150, Avg Loss: 12.7208\n",
            "03/25/2025 18:12:01 - INFO - __main__ -   Epoch 5, Batch 1200, Avg Loss: 12.7502\n",
            "03/25/2025 18:12:08 - INFO - __main__ -   Epoch 5, Batch 1250, Avg Loss: 12.7398\n",
            "03/25/2025 18:12:15 - INFO - __main__ -   Epoch 5, Batch 1300, Avg Loss: 12.7477\n",
            "03/25/2025 18:12:22 - INFO - __main__ -   Epoch 5, Batch 1350, Avg Loss: 12.7281\n",
            "03/25/2025 18:12:29 - INFO - __main__ -   Epoch 5, Batch 1400, Avg Loss: 12.7230\n",
            "03/25/2025 18:12:36 - INFO - __main__ -   Epoch 5, Batch 1450, Avg Loss: 12.7171\n",
            "03/25/2025 18:12:43 - INFO - __main__ -   Epoch 5, Batch 1500, Avg Loss: 12.6994\n",
            "03/25/2025 18:12:50 - INFO - __main__ -   Epoch 5, Batch 1550, Avg Loss: 12.6679\n",
            "03/25/2025 18:12:57 - INFO - __main__ -   Epoch 5, Batch 1600, Avg Loss: 12.6578\n",
            "03/25/2025 18:13:04 - INFO - __main__ -   Epoch 5, Batch 1650, Avg Loss: 12.6618\n",
            "03/25/2025 18:13:11 - INFO - __main__ -   Epoch 5, Batch 1700, Avg Loss: 12.6330\n",
            "03/25/2025 18:13:18 - INFO - __main__ -   Epoch 5, Batch 1750, Avg Loss: 12.6159\n",
            "03/25/2025 18:13:25 - INFO - __main__ -   Epoch 5, Batch 1800, Avg Loss: 12.5889\n",
            "03/25/2025 18:13:32 - INFO - __main__ -   Epoch 5, Batch 1850, Avg Loss: 12.5665\n",
            "03/25/2025 18:13:39 - INFO - __main__ -   Epoch 5, Batch 1900, Avg Loss: 12.5777\n",
            "03/25/2025 18:13:46 - INFO - __main__ -   Epoch 5, Batch 1950, Avg Loss: 12.5787\n",
            "03/25/2025 18:13:53 - INFO - __main__ -   Epoch 5, Batch 2000, Avg Loss: 12.5959\n",
            "03/25/2025 18:14:00 - INFO - __main__ -   Epoch 5, Batch 2050, Avg Loss: 12.5996\n",
            "03/25/2025 18:14:07 - INFO - __main__ -   Epoch 5, Batch 2100, Avg Loss: 12.5882\n",
            "03/25/2025 18:14:14 - INFO - __main__ -   Epoch 5, Batch 2150, Avg Loss: 12.5691\n",
            "03/25/2025 18:14:21 - INFO - __main__ -   Epoch 5, Batch 2200, Avg Loss: 12.5494\n",
            "03/25/2025 18:14:28 - INFO - __main__ -   Epoch 5, Batch 2250, Avg Loss: 12.5312\n",
            "03/25/2025 18:14:35 - INFO - __main__ -   Epoch 5, Batch 2300, Avg Loss: 12.5225\n",
            "03/25/2025 18:14:42 - INFO - __main__ -   Epoch 5, Batch 2350, Avg Loss: 12.5107\n",
            "03/25/2025 18:14:49 - INFO - __main__ -   Epoch 5, Batch 2400, Avg Loss: 12.4981\n",
            "03/25/2025 18:14:56 - INFO - __main__ -   Epoch 5, Batch 2450, Avg Loss: 12.4868\n",
            "03/25/2025 18:15:03 - INFO - __main__ -   Epoch 5, Batch 2500, Avg Loss: 12.4722\n",
            "03/25/2025 18:15:10 - INFO - __main__ -   Epoch 5, Batch 2550, Avg Loss: 12.4734\n",
            "03/25/2025 18:15:17 - INFO - __main__ -   Epoch 5, Batch 2600, Avg Loss: 12.5962\n",
            "03/25/2025 18:15:24 - INFO - __main__ -   Epoch 5, Batch 2650, Avg Loss: 12.6400\n",
            "03/25/2025 18:15:31 - INFO - __main__ -   Epoch 5, Batch 2700, Avg Loss: 12.6261\n",
            "03/25/2025 18:15:38 - INFO - __main__ -   Epoch 5, Batch 2750, Avg Loss: 12.6076\n",
            "03/25/2025 18:15:45 - INFO - __main__ -   Epoch 5, Batch 2800, Avg Loss: 12.5885\n",
            "03/25/2025 18:15:52 - INFO - __main__ -   Epoch 5, Batch 2850, Avg Loss: 12.5730\n",
            "03/25/2025 18:15:59 - INFO - __main__ -   Epoch 5, Batch 2900, Avg Loss: 12.5523\n",
            "03/25/2025 18:16:06 - INFO - __main__ -   Epoch 5, Batch 2950, Avg Loss: 12.5300\n",
            "03/25/2025 18:16:13 - INFO - __main__ -   Epoch 5, Batch 3000, Avg Loss: 12.5129\n",
            "03/25/2025 18:16:20 - INFO - __main__ -   Epoch 5, Batch 3050, Avg Loss: 12.5009\n",
            "03/25/2025 18:16:27 - INFO - __main__ -   Epoch 5, Batch 3100, Avg Loss: 12.4838\n",
            "03/25/2025 18:16:34 - INFO - __main__ -   Epoch 5, Batch 3150, Avg Loss: 12.4739\n",
            "03/25/2025 18:16:41 - INFO - __main__ -   Epoch 5, Batch 3200, Avg Loss: 12.4615\n",
            "03/25/2025 18:16:48 - INFO - __main__ -   Epoch 5, Batch 3250, Avg Loss: 12.4535\n",
            "03/25/2025 18:16:55 - INFO - __main__ -   Epoch 5, Batch 3300, Avg Loss: 12.4953\n",
            "03/25/2025 18:17:02 - INFO - __main__ -   Epoch 5, Batch 3350, Avg Loss: 12.5000\n",
            "03/25/2025 18:17:09 - INFO - __main__ -   Epoch 5, Batch 3400, Avg Loss: 12.5201\n",
            "03/25/2025 18:17:16 - INFO - __main__ -   Epoch 5, Batch 3450, Avg Loss: 12.5139\n",
            "03/25/2025 18:17:23 - INFO - __main__ -   Epoch 5, Batch 3500, Avg Loss: 12.5208\n",
            "03/25/2025 18:17:30 - INFO - __main__ -   Epoch 5, Batch 3550, Avg Loss: 12.5307\n",
            "03/25/2025 18:17:37 - INFO - __main__ -   Epoch 5, Batch 3600, Avg Loss: 12.5317\n",
            "03/25/2025 18:17:44 - INFO - __main__ -   Epoch 5, Batch 3650, Avg Loss: 12.5414\n",
            "03/25/2025 18:17:51 - INFO - __main__ -   Epoch 5, Batch 3700, Avg Loss: 12.5542\n",
            "03/25/2025 18:17:58 - INFO - __main__ -   Epoch 5, Batch 3750, Avg Loss: 12.5477\n",
            "03/25/2025 18:18:05 - INFO - __main__ -   Epoch 5, Batch 3800, Avg Loss: 12.5509\n",
            "03/25/2025 18:18:12 - INFO - __main__ -   Epoch 5, Batch 3850, Avg Loss: 12.5580\n",
            "03/25/2025 18:18:19 - INFO - __main__ -   Epoch 5, Batch 3900, Avg Loss: 12.5602\n",
            "03/25/2025 18:18:26 - INFO - __main__ -   Epoch 5, Batch 3950, Avg Loss: 12.5636\n",
            "03/25/2025 18:18:33 - INFO - __main__ -   Epoch 5, Batch 4000, Avg Loss: 12.5600\n",
            "03/25/2025 18:18:40 - INFO - __main__ -   Epoch 5, Batch 4050, Avg Loss: 12.5583\n",
            "03/25/2025 18:18:47 - INFO - __main__ -   Epoch 5, Batch 4100, Avg Loss: 12.5571\n",
            "03/25/2025 18:18:54 - INFO - __main__ -   Epoch 5, Batch 4150, Avg Loss: 12.5539\n",
            "03/25/2025 18:19:01 - INFO - __main__ -   Epoch 5, Batch 4200, Avg Loss: 12.5747\n",
            "03/25/2025 18:19:08 - INFO - __main__ -   Epoch 5, Batch 4250, Avg Loss: 12.5802\n",
            "03/25/2025 18:19:15 - INFO - __main__ -   Epoch 5, Batch 4300, Avg Loss: 12.5891\n",
            "03/25/2025 18:19:22 - INFO - __main__ -   Epoch 5, Batch 4350, Avg Loss: 12.5814\n",
            "03/25/2025 18:19:29 - INFO - __main__ -   Epoch 5, Batch 4400, Avg Loss: 12.5818\n",
            "03/25/2025 18:19:36 - INFO - __main__ -   Epoch 5, Batch 4450, Avg Loss: 12.5889\n",
            "03/25/2025 18:19:43 - INFO - __main__ -   Epoch 5, Batch 4500, Avg Loss: 12.5903\n",
            "03/25/2025 18:19:50 - INFO - __main__ -   Epoch 5, Batch 4550, Avg Loss: 12.5948\n",
            "03/25/2025 18:19:57 - INFO - __main__ -   Epoch 5, Batch 4600, Avg Loss: 12.5896\n",
            "03/25/2025 18:20:04 - INFO - __main__ -   Epoch 5, Batch 4650, Avg Loss: 12.5794\n",
            "03/25/2025 18:20:11 - INFO - __main__ -   Epoch 5, Batch 4700, Avg Loss: 12.5771\n",
            "03/25/2025 18:20:18 - INFO - __main__ -   Epoch 5, Batch 4750, Avg Loss: 12.5777\n",
            "03/25/2025 18:20:25 - INFO - __main__ -   Epoch 5, Batch 4800, Avg Loss: 12.5985\n",
            "03/25/2025 18:20:32 - INFO - __main__ -   Epoch 5, Batch 4850, Avg Loss: 12.6000\n",
            "03/25/2025 18:20:39 - INFO - __main__ -   Epoch 5, Batch 4900, Avg Loss: 12.6034\n",
            "03/25/2025 18:20:46 - INFO - __main__ -   Epoch 5, Batch 4950, Avg Loss: 12.6012\n",
            "03/25/2025 18:20:53 - INFO - __main__ -   Epoch 5, Batch 5000, Avg Loss: 12.6029\n",
            "03/25/2025 18:21:00 - INFO - __main__ -   Epoch 5, Batch 5050, Avg Loss: 12.6003\n",
            "03/25/2025 18:21:07 - INFO - __main__ -   Epoch 5, Batch 5100, Avg Loss: 12.6087\n",
            "03/25/2025 18:21:14 - INFO - __main__ -   Epoch 5, Batch 5150, Avg Loss: 12.6181\n",
            "03/25/2025 18:21:21 - INFO - __main__ -   Epoch 5, Batch 5200, Avg Loss: 12.6136\n",
            "03/25/2025 18:21:28 - INFO - __main__ -   Epoch 5, Batch 5250, Avg Loss: 12.6029\n",
            "03/25/2025 18:21:35 - INFO - __main__ -   Epoch 5, Batch 5300, Avg Loss: 12.6014\n",
            "03/25/2025 18:21:42 - INFO - __main__ -   Epoch 5, Batch 5350, Avg Loss: 12.5934\n",
            "03/25/2025 18:21:49 - INFO - __main__ -   Epoch 5, Batch 5400, Avg Loss: 12.5908\n",
            "03/25/2025 18:21:56 - INFO - __main__ -   Epoch 5, Batch 5450, Avg Loss: 12.5952\n",
            "03/25/2025 18:22:03 - INFO - __main__ -   Epoch 5, Batch 5500, Avg Loss: 12.5900\n",
            "03/25/2025 18:22:10 - INFO - __main__ -   Epoch 5, Batch 5550, Avg Loss: 12.5906\n",
            "03/25/2025 18:22:17 - INFO - __main__ -   Epoch 5, Batch 5600, Avg Loss: 12.5859\n",
            "03/25/2025 18:22:24 - INFO - __main__ -   Epoch 5, Batch 5650, Avg Loss: 12.5968\n",
            "03/25/2025 18:22:31 - INFO - __main__ -   Epoch 5, Batch 5700, Avg Loss: 12.5924\n",
            "03/25/2025 18:22:38 - INFO - __main__ -   Epoch 5, Batch 5750, Avg Loss: 12.5803\n",
            "03/25/2025 18:22:45 - INFO - __main__ -   Epoch 5, Batch 5800, Avg Loss: 12.5732\n",
            "03/25/2025 18:22:52 - INFO - __main__ -   Epoch 5, Batch 5850, Avg Loss: 12.5673\n",
            "03/25/2025 18:22:59 - INFO - __main__ -   Epoch 5, Batch 5900, Avg Loss: 12.5613\n",
            "03/25/2025 18:23:06 - INFO - __main__ -   Epoch 5, Batch 5950, Avg Loss: 12.5612\n",
            "03/25/2025 18:23:13 - INFO - __main__ -   Epoch 5, Batch 6000, Avg Loss: 12.5704\n",
            "03/25/2025 18:23:20 - INFO - __main__ -   Epoch 5, Batch 6050, Avg Loss: 12.5655\n",
            "03/25/2025 18:23:27 - INFO - __main__ -   Epoch 5, Batch 6100, Avg Loss: 12.5635\n",
            "03/25/2025 18:23:34 - INFO - __main__ -   Epoch 5, Batch 6150, Avg Loss: 12.5587\n",
            "03/25/2025 18:23:41 - INFO - __main__ -   Epoch 5, Batch 6200, Avg Loss: 12.5573\n",
            "03/25/2025 18:23:48 - INFO - __main__ -   Epoch 5, Batch 6250, Avg Loss: 12.5589\n",
            "03/25/2025 18:23:55 - INFO - __main__ -   Epoch 5, Batch 6300, Avg Loss: 12.5481\n",
            "03/25/2025 18:24:02 - INFO - __main__ -   Epoch 5, Batch 6350, Avg Loss: 12.5512\n",
            "03/25/2025 18:24:09 - INFO - __main__ -   Epoch 5, Batch 6400, Avg Loss: 12.5509\n",
            "03/25/2025 18:24:16 - INFO - __main__ -   Epoch 5, Batch 6450, Avg Loss: 12.5505\n",
            "03/25/2025 18:24:23 - INFO - __main__ -   Epoch 5, Batch 6500, Avg Loss: 12.5465\n",
            "03/25/2025 18:24:30 - INFO - __main__ -   Epoch 5, Batch 6550, Avg Loss: 12.5647\n",
            "03/25/2025 18:24:37 - INFO - __main__ -   Epoch 5, Batch 6600, Avg Loss: 12.5634\n",
            "03/25/2025 18:24:44 - INFO - __main__ -   Epoch 5, Batch 6650, Avg Loss: 12.5639\n",
            "03/25/2025 18:24:51 - INFO - __main__ -   Epoch 5, Batch 6700, Avg Loss: 12.5685\n",
            "03/25/2025 18:24:58 - INFO - __main__ -   Epoch 5, Batch 6750, Avg Loss: 12.5709\n",
            "03/25/2025 18:25:05 - INFO - __main__ -   Epoch 5, Batch 6800, Avg Loss: 12.5708\n",
            "03/25/2025 18:25:12 - INFO - __main__ -   Epoch 5, Batch 6850, Avg Loss: 12.5714\n",
            "03/25/2025 18:25:19 - INFO - __main__ -   Epoch 5, Batch 6900, Avg Loss: 12.5743\n",
            "03/25/2025 18:25:26 - INFO - __main__ -   Epoch 5, Batch 6950, Avg Loss: 12.5690\n",
            "03/25/2025 18:25:33 - INFO - __main__ -   Epoch 5, Batch 7000, Avg Loss: 12.5746\n",
            "03/25/2025 18:25:40 - INFO - __main__ -   Epoch 5, Batch 7050, Avg Loss: 12.5808\n",
            "03/25/2025 18:25:47 - INFO - __main__ -   Epoch 5, Batch 7100, Avg Loss: 12.5766\n",
            "03/25/2025 18:25:54 - INFO - __main__ -   Epoch 5, Batch 7150, Avg Loss: 12.5770\n",
            "03/25/2025 18:26:01 - INFO - __main__ -   Epoch 5, Batch 7200, Avg Loss: 12.5809\n",
            "03/25/2025 18:26:08 - INFO - __main__ -   Epoch 5, Batch 7250, Avg Loss: 12.5917\n",
            "03/25/2025 18:26:15 - INFO - __main__ -   Epoch 5, Batch 7300, Avg Loss: 12.5932\n",
            "03/25/2025 18:26:22 - INFO - __main__ -   Epoch 5, Batch 7350, Avg Loss: 12.5906\n",
            "03/25/2025 18:26:29 - INFO - __main__ -   Epoch 5, Batch 7400, Avg Loss: 12.5931\n",
            "03/25/2025 18:26:36 - INFO - __main__ -   Epoch 5, Batch 7450, Avg Loss: 12.6052\n",
            "03/25/2025 18:26:43 - INFO - __main__ -   Epoch 5, Batch 7500, Avg Loss: 12.6018\n",
            "03/25/2025 18:26:50 - INFO - __main__ -   Epoch 5, Batch 7550, Avg Loss: 12.5953\n",
            "03/25/2025 18:26:57 - INFO - __main__ -   Epoch 5, Batch 7600, Avg Loss: 12.5995\n",
            "03/25/2025 18:27:04 - INFO - __main__ -   Epoch 5, Batch 7650, Avg Loss: 12.5935\n",
            "03/25/2025 18:27:11 - INFO - __main__ -   Epoch 5, Batch 7700, Avg Loss: 12.5882\n",
            "03/25/2025 18:27:18 - INFO - __main__ -   Epoch 5, Batch 7750, Avg Loss: 12.6091\n",
            "03/25/2025 18:27:25 - INFO - __main__ -   Epoch 5, Batch 7800, Avg Loss: 12.6126\n",
            "03/25/2025 18:27:32 - INFO - __main__ -   Epoch 5, Batch 7850, Avg Loss: 12.6117\n",
            "03/25/2025 18:27:35 - INFO - __main__ -   Epoch 5 completed, Avg Loss: 12.6120\n",
            "03/25/2025 18:27:47 - INFO - __main__ -   Epoch 6, Batch 50, Avg Loss: 12.4699\n",
            "03/25/2025 18:27:54 - INFO - __main__ -   Epoch 6, Batch 100, Avg Loss: 12.2032\n",
            "03/25/2025 18:28:01 - INFO - __main__ -   Epoch 6, Batch 150, Avg Loss: 12.2555\n",
            "03/25/2025 18:28:08 - INFO - __main__ -   Epoch 6, Batch 200, Avg Loss: 12.1807\n",
            "03/25/2025 18:28:15 - INFO - __main__ -   Epoch 6, Batch 250, Avg Loss: 12.2431\n",
            "03/25/2025 18:28:22 - INFO - __main__ -   Epoch 6, Batch 300, Avg Loss: 12.1789\n",
            "03/25/2025 18:28:29 - INFO - __main__ -   Epoch 6, Batch 350, Avg Loss: 12.0579\n",
            "03/25/2025 18:28:36 - INFO - __main__ -   Epoch 6, Batch 400, Avg Loss: 12.0459\n",
            "03/25/2025 18:28:43 - INFO - __main__ -   Epoch 6, Batch 450, Avg Loss: 12.1182\n",
            "03/25/2025 18:28:51 - INFO - __main__ -   Epoch 6, Batch 500, Avg Loss: 12.1197\n",
            "03/25/2025 18:28:58 - INFO - __main__ -   Epoch 6, Batch 550, Avg Loss: 12.1503\n",
            "03/25/2025 18:29:05 - INFO - __main__ -   Epoch 6, Batch 600, Avg Loss: 12.2208\n",
            "03/25/2025 18:29:12 - INFO - __main__ -   Epoch 6, Batch 650, Avg Loss: 12.4040\n",
            "03/25/2025 18:29:19 - INFO - __main__ -   Epoch 6, Batch 700, Avg Loss: 12.3511\n",
            "03/25/2025 18:29:26 - INFO - __main__ -   Epoch 6, Batch 750, Avg Loss: 12.3674\n",
            "03/25/2025 18:29:33 - INFO - __main__ -   Epoch 6, Batch 800, Avg Loss: 12.3507\n",
            "03/25/2025 18:29:40 - INFO - __main__ -   Epoch 6, Batch 850, Avg Loss: 12.3521\n",
            "03/25/2025 18:29:47 - INFO - __main__ -   Epoch 6, Batch 900, Avg Loss: 12.3729\n",
            "03/25/2025 18:29:54 - INFO - __main__ -   Epoch 6, Batch 950, Avg Loss: 12.3824\n",
            "03/25/2025 18:30:01 - INFO - __main__ -   Epoch 6, Batch 1000, Avg Loss: 12.4126\n",
            "03/25/2025 18:30:08 - INFO - __main__ -   Epoch 6, Batch 1050, Avg Loss: 12.3978\n",
            "03/25/2025 18:30:15 - INFO - __main__ -   Epoch 6, Batch 1100, Avg Loss: 12.3685\n",
            "03/25/2025 18:30:22 - INFO - __main__ -   Epoch 6, Batch 1150, Avg Loss: 12.3670\n",
            "03/25/2025 18:30:29 - INFO - __main__ -   Epoch 6, Batch 1200, Avg Loss: 12.4017\n",
            "03/25/2025 18:30:36 - INFO - __main__ -   Epoch 6, Batch 1250, Avg Loss: 12.3926\n",
            "03/25/2025 18:30:43 - INFO - __main__ -   Epoch 6, Batch 1300, Avg Loss: 12.4033\n",
            "03/25/2025 18:30:50 - INFO - __main__ -   Epoch 6, Batch 1350, Avg Loss: 12.3868\n",
            "03/25/2025 18:30:57 - INFO - __main__ -   Epoch 6, Batch 1400, Avg Loss: 12.3835\n",
            "03/25/2025 18:31:04 - INFO - __main__ -   Epoch 6, Batch 1450, Avg Loss: 12.3701\n",
            "03/25/2025 18:31:11 - INFO - __main__ -   Epoch 6, Batch 1500, Avg Loss: 12.3517\n",
            "03/25/2025 18:31:18 - INFO - __main__ -   Epoch 6, Batch 1550, Avg Loss: 12.3191\n",
            "03/25/2025 18:31:25 - INFO - __main__ -   Epoch 6, Batch 1600, Avg Loss: 12.3073\n",
            "03/25/2025 18:31:32 - INFO - __main__ -   Epoch 6, Batch 1650, Avg Loss: 12.3148\n",
            "03/25/2025 18:31:39 - INFO - __main__ -   Epoch 6, Batch 1700, Avg Loss: 12.2897\n",
            "03/25/2025 18:31:46 - INFO - __main__ -   Epoch 6, Batch 1750, Avg Loss: 12.2698\n",
            "03/25/2025 18:31:53 - INFO - __main__ -   Epoch 6, Batch 1800, Avg Loss: 12.2420\n",
            "03/25/2025 18:32:00 - INFO - __main__ -   Epoch 6, Batch 1850, Avg Loss: 12.2224\n",
            "03/25/2025 18:32:07 - INFO - __main__ -   Epoch 6, Batch 1900, Avg Loss: 12.2363\n",
            "03/25/2025 18:32:14 - INFO - __main__ -   Epoch 6, Batch 1950, Avg Loss: 12.2353\n",
            "03/25/2025 18:32:21 - INFO - __main__ -   Epoch 6, Batch 2000, Avg Loss: 12.2482\n",
            "03/25/2025 18:32:28 - INFO - __main__ -   Epoch 6, Batch 2050, Avg Loss: 12.2542\n",
            "03/25/2025 18:32:35 - INFO - __main__ -   Epoch 6, Batch 2100, Avg Loss: 12.2485\n",
            "03/25/2025 18:32:42 - INFO - __main__ -   Epoch 6, Batch 2150, Avg Loss: 12.2288\n",
            "03/25/2025 18:32:49 - INFO - __main__ -   Epoch 6, Batch 2200, Avg Loss: 12.2124\n",
            "03/25/2025 18:32:56 - INFO - __main__ -   Epoch 6, Batch 2250, Avg Loss: 12.1956\n",
            "03/25/2025 18:33:03 - INFO - __main__ -   Epoch 6, Batch 2300, Avg Loss: 12.1876\n",
            "03/25/2025 18:33:10 - INFO - __main__ -   Epoch 6, Batch 2350, Avg Loss: 12.1769\n",
            "03/25/2025 18:33:17 - INFO - __main__ -   Epoch 6, Batch 2400, Avg Loss: 12.1648\n",
            "03/25/2025 18:33:24 - INFO - __main__ -   Epoch 6, Batch 2450, Avg Loss: 12.1497\n",
            "03/25/2025 18:33:31 - INFO - __main__ -   Epoch 6, Batch 2500, Avg Loss: 12.1341\n",
            "03/25/2025 18:33:38 - INFO - __main__ -   Epoch 6, Batch 2550, Avg Loss: 12.1354\n",
            "03/25/2025 18:33:45 - INFO - __main__ -   Epoch 6, Batch 2600, Avg Loss: 12.2530\n",
            "03/25/2025 18:33:52 - INFO - __main__ -   Epoch 6, Batch 2650, Avg Loss: 12.2966\n",
            "03/25/2025 18:33:59 - INFO - __main__ -   Epoch 6, Batch 2700, Avg Loss: 12.2821\n",
            "03/25/2025 18:34:06 - INFO - __main__ -   Epoch 6, Batch 2750, Avg Loss: 12.2625\n",
            "03/25/2025 18:34:13 - INFO - __main__ -   Epoch 6, Batch 2800, Avg Loss: 12.2449\n",
            "03/25/2025 18:34:20 - INFO - __main__ -   Epoch 6, Batch 2850, Avg Loss: 12.2304\n",
            "03/25/2025 18:34:27 - INFO - __main__ -   Epoch 6, Batch 2900, Avg Loss: 12.2112\n",
            "03/25/2025 18:34:34 - INFO - __main__ -   Epoch 6, Batch 2950, Avg Loss: 12.1906\n",
            "03/25/2025 18:34:41 - INFO - __main__ -   Epoch 6, Batch 3000, Avg Loss: 12.1755\n",
            "03/25/2025 18:34:48 - INFO - __main__ -   Epoch 6, Batch 3050, Avg Loss: 12.1617\n",
            "03/25/2025 18:34:55 - INFO - __main__ -   Epoch 6, Batch 3100, Avg Loss: 12.1460\n",
            "03/25/2025 18:35:02 - INFO - __main__ -   Epoch 6, Batch 3150, Avg Loss: 12.1368\n",
            "03/25/2025 18:35:09 - INFO - __main__ -   Epoch 6, Batch 3200, Avg Loss: 12.1242\n",
            "03/25/2025 18:35:16 - INFO - __main__ -   Epoch 6, Batch 3250, Avg Loss: 12.1164\n",
            "03/25/2025 18:35:23 - INFO - __main__ -   Epoch 6, Batch 3300, Avg Loss: 12.1610\n",
            "03/25/2025 18:35:30 - INFO - __main__ -   Epoch 6, Batch 3350, Avg Loss: 12.1660\n",
            "03/25/2025 18:35:37 - INFO - __main__ -   Epoch 6, Batch 3400, Avg Loss: 12.1841\n",
            "03/25/2025 18:35:44 - INFO - __main__ -   Epoch 6, Batch 3450, Avg Loss: 12.1781\n",
            "03/25/2025 18:35:51 - INFO - __main__ -   Epoch 6, Batch 3500, Avg Loss: 12.1876\n",
            "03/25/2025 18:35:58 - INFO - __main__ -   Epoch 6, Batch 3550, Avg Loss: 12.1977\n",
            "03/25/2025 18:36:05 - INFO - __main__ -   Epoch 6, Batch 3600, Avg Loss: 12.2031\n",
            "03/25/2025 18:36:12 - INFO - __main__ -   Epoch 6, Batch 3650, Avg Loss: 12.2133\n",
            "03/25/2025 18:36:19 - INFO - __main__ -   Epoch 6, Batch 3700, Avg Loss: 12.2228\n",
            "03/25/2025 18:36:26 - INFO - __main__ -   Epoch 6, Batch 3750, Avg Loss: 12.2159\n",
            "03/25/2025 18:36:33 - INFO - __main__ -   Epoch 6, Batch 3800, Avg Loss: 12.2165\n",
            "03/25/2025 18:36:40 - INFO - __main__ -   Epoch 6, Batch 3850, Avg Loss: 12.2218\n",
            "03/25/2025 18:36:47 - INFO - __main__ -   Epoch 6, Batch 3900, Avg Loss: 12.2251\n",
            "03/25/2025 18:36:54 - INFO - __main__ -   Epoch 6, Batch 3950, Avg Loss: 12.2280\n",
            "03/25/2025 18:37:01 - INFO - __main__ -   Epoch 6, Batch 4000, Avg Loss: 12.2237\n",
            "03/25/2025 18:37:08 - INFO - __main__ -   Epoch 6, Batch 4050, Avg Loss: 12.2239\n",
            "03/25/2025 18:37:15 - INFO - __main__ -   Epoch 6, Batch 4100, Avg Loss: 12.2227\n",
            "03/25/2025 18:37:22 - INFO - __main__ -   Epoch 6, Batch 4150, Avg Loss: 12.2201\n",
            "03/25/2025 18:37:29 - INFO - __main__ -   Epoch 6, Batch 4200, Avg Loss: 12.2379\n",
            "03/25/2025 18:37:36 - INFO - __main__ -   Epoch 6, Batch 4250, Avg Loss: 12.2449\n",
            "03/25/2025 18:37:43 - INFO - __main__ -   Epoch 6, Batch 4300, Avg Loss: 12.2559\n",
            "03/25/2025 18:37:50 - INFO - __main__ -   Epoch 6, Batch 4350, Avg Loss: 12.2490\n",
            "03/25/2025 18:37:57 - INFO - __main__ -   Epoch 6, Batch 4400, Avg Loss: 12.2487\n",
            "03/25/2025 18:38:04 - INFO - __main__ -   Epoch 6, Batch 4450, Avg Loss: 12.2568\n",
            "03/25/2025 18:38:11 - INFO - __main__ -   Epoch 6, Batch 4500, Avg Loss: 12.2582\n",
            "03/25/2025 18:38:18 - INFO - __main__ -   Epoch 6, Batch 4550, Avg Loss: 12.2633\n",
            "03/25/2025 18:38:25 - INFO - __main__ -   Epoch 6, Batch 4600, Avg Loss: 12.2595\n",
            "03/25/2025 18:38:32 - INFO - __main__ -   Epoch 6, Batch 4650, Avg Loss: 12.2495\n",
            "03/25/2025 18:38:39 - INFO - __main__ -   Epoch 6, Batch 4700, Avg Loss: 12.2477\n",
            "03/25/2025 18:38:46 - INFO - __main__ -   Epoch 6, Batch 4750, Avg Loss: 12.2475\n",
            "03/25/2025 18:38:53 - INFO - __main__ -   Epoch 6, Batch 4800, Avg Loss: 12.2669\n",
            "03/25/2025 18:39:00 - INFO - __main__ -   Epoch 6, Batch 4850, Avg Loss: 12.2679\n",
            "03/25/2025 18:39:07 - INFO - __main__ -   Epoch 6, Batch 4900, Avg Loss: 12.2733\n",
            "03/25/2025 18:39:14 - INFO - __main__ -   Epoch 6, Batch 4950, Avg Loss: 12.2713\n",
            "03/25/2025 18:39:21 - INFO - __main__ -   Epoch 6, Batch 5000, Avg Loss: 12.2721\n",
            "03/25/2025 18:39:28 - INFO - __main__ -   Epoch 6, Batch 5050, Avg Loss: 12.2704\n",
            "03/25/2025 18:39:35 - INFO - __main__ -   Epoch 6, Batch 5100, Avg Loss: 12.2771\n",
            "03/25/2025 18:39:42 - INFO - __main__ -   Epoch 6, Batch 5150, Avg Loss: 12.2893\n",
            "03/25/2025 18:39:49 - INFO - __main__ -   Epoch 6, Batch 5200, Avg Loss: 12.2860\n",
            "03/25/2025 18:39:56 - INFO - __main__ -   Epoch 6, Batch 5250, Avg Loss: 12.2762\n",
            "03/25/2025 18:40:03 - INFO - __main__ -   Epoch 6, Batch 5300, Avg Loss: 12.2762\n",
            "03/25/2025 18:40:10 - INFO - __main__ -   Epoch 6, Batch 5350, Avg Loss: 12.2691\n",
            "03/25/2025 18:40:17 - INFO - __main__ -   Epoch 6, Batch 5400, Avg Loss: 12.2658\n",
            "03/25/2025 18:40:24 - INFO - __main__ -   Epoch 6, Batch 5450, Avg Loss: 12.2682\n",
            "03/25/2025 18:40:31 - INFO - __main__ -   Epoch 6, Batch 5500, Avg Loss: 12.2630\n",
            "03/25/2025 18:40:38 - INFO - __main__ -   Epoch 6, Batch 5550, Avg Loss: 12.2614\n",
            "03/25/2025 18:40:45 - INFO - __main__ -   Epoch 6, Batch 5600, Avg Loss: 12.2570\n",
            "03/25/2025 18:40:52 - INFO - __main__ -   Epoch 6, Batch 5650, Avg Loss: 12.2667\n",
            "03/25/2025 18:40:59 - INFO - __main__ -   Epoch 6, Batch 5700, Avg Loss: 12.2637\n",
            "03/25/2025 18:41:06 - INFO - __main__ -   Epoch 6, Batch 5750, Avg Loss: 12.2530\n",
            "03/25/2025 18:41:13 - INFO - __main__ -   Epoch 6, Batch 5800, Avg Loss: 12.2465\n",
            "03/25/2025 18:41:20 - INFO - __main__ -   Epoch 6, Batch 5850, Avg Loss: 12.2410\n",
            "03/25/2025 18:41:27 - INFO - __main__ -   Epoch 6, Batch 5900, Avg Loss: 12.2352\n",
            "03/25/2025 18:41:34 - INFO - __main__ -   Epoch 6, Batch 5950, Avg Loss: 12.2358\n",
            "03/25/2025 18:41:41 - INFO - __main__ -   Epoch 6, Batch 6000, Avg Loss: 12.2454\n",
            "03/25/2025 18:41:48 - INFO - __main__ -   Epoch 6, Batch 6050, Avg Loss: 12.2410\n",
            "03/25/2025 18:41:55 - INFO - __main__ -   Epoch 6, Batch 6100, Avg Loss: 12.2376\n",
            "03/25/2025 18:42:02 - INFO - __main__ -   Epoch 6, Batch 6150, Avg Loss: 12.2335\n",
            "03/25/2025 18:42:09 - INFO - __main__ -   Epoch 6, Batch 6200, Avg Loss: 12.2330\n",
            "03/25/2025 18:42:16 - INFO - __main__ -   Epoch 6, Batch 6250, Avg Loss: 12.2347\n",
            "03/25/2025 18:42:23 - INFO - __main__ -   Epoch 6, Batch 6300, Avg Loss: 12.2248\n",
            "03/25/2025 18:42:30 - INFO - __main__ -   Epoch 6, Batch 6350, Avg Loss: 12.2291\n",
            "03/25/2025 18:42:37 - INFO - __main__ -   Epoch 6, Batch 6400, Avg Loss: 12.2303\n",
            "03/25/2025 18:42:44 - INFO - __main__ -   Epoch 6, Batch 6450, Avg Loss: 12.2297\n",
            "03/25/2025 18:42:51 - INFO - __main__ -   Epoch 6, Batch 6500, Avg Loss: 12.2264\n",
            "03/25/2025 18:42:58 - INFO - __main__ -   Epoch 6, Batch 6550, Avg Loss: 12.2436\n",
            "03/25/2025 18:43:05 - INFO - __main__ -   Epoch 6, Batch 6600, Avg Loss: 12.2422\n",
            "03/25/2025 18:43:12 - INFO - __main__ -   Epoch 6, Batch 6650, Avg Loss: 12.2437\n",
            "03/25/2025 18:43:19 - INFO - __main__ -   Epoch 6, Batch 6700, Avg Loss: 12.2482\n",
            "03/25/2025 18:43:26 - INFO - __main__ -   Epoch 6, Batch 6750, Avg Loss: 12.2518\n",
            "03/25/2025 18:43:33 - INFO - __main__ -   Epoch 6, Batch 6800, Avg Loss: 12.2515\n",
            "03/25/2025 18:43:40 - INFO - __main__ -   Epoch 6, Batch 6850, Avg Loss: 12.2530\n",
            "03/25/2025 18:43:47 - INFO - __main__ -   Epoch 6, Batch 6900, Avg Loss: 12.2558\n",
            "03/25/2025 18:43:54 - INFO - __main__ -   Epoch 6, Batch 6950, Avg Loss: 12.2510\n",
            "03/25/2025 18:44:01 - INFO - __main__ -   Epoch 6, Batch 7000, Avg Loss: 12.2583\n",
            "03/25/2025 18:44:08 - INFO - __main__ -   Epoch 6, Batch 7050, Avg Loss: 12.2643\n",
            "03/25/2025 18:44:15 - INFO - __main__ -   Epoch 6, Batch 7100, Avg Loss: 12.2600\n",
            "03/25/2025 18:44:22 - INFO - __main__ -   Epoch 6, Batch 7150, Avg Loss: 12.2604\n",
            "03/25/2025 18:44:30 - INFO - __main__ -   Epoch 6, Batch 7200, Avg Loss: 12.2640\n",
            "03/25/2025 18:44:37 - INFO - __main__ -   Epoch 6, Batch 7250, Avg Loss: 12.2743\n",
            "03/25/2025 18:44:44 - INFO - __main__ -   Epoch 6, Batch 7300, Avg Loss: 12.2768\n",
            "03/25/2025 18:44:51 - INFO - __main__ -   Epoch 6, Batch 7350, Avg Loss: 12.2749\n",
            "03/25/2025 18:44:58 - INFO - __main__ -   Epoch 6, Batch 7400, Avg Loss: 12.2765\n",
            "03/25/2025 18:45:05 - INFO - __main__ -   Epoch 6, Batch 7450, Avg Loss: 12.2863\n",
            "03/25/2025 18:45:12 - INFO - __main__ -   Epoch 6, Batch 7500, Avg Loss: 12.2834\n",
            "03/25/2025 18:45:19 - INFO - __main__ -   Epoch 6, Batch 7550, Avg Loss: 12.2770\n",
            "03/25/2025 18:45:26 - INFO - __main__ -   Epoch 6, Batch 7600, Avg Loss: 12.2811\n",
            "03/25/2025 18:45:33 - INFO - __main__ -   Epoch 6, Batch 7650, Avg Loss: 12.2746\n",
            "03/25/2025 18:45:40 - INFO - __main__ -   Epoch 6, Batch 7700, Avg Loss: 12.2693\n",
            "03/25/2025 18:45:47 - INFO - __main__ -   Epoch 6, Batch 7750, Avg Loss: 12.2895\n",
            "03/25/2025 18:45:54 - INFO - __main__ -   Epoch 6, Batch 7800, Avg Loss: 12.2930\n",
            "03/25/2025 18:46:01 - INFO - __main__ -   Epoch 6, Batch 7850, Avg Loss: 12.2914\n",
            "03/25/2025 18:46:04 - INFO - __main__ -   Epoch 6 completed, Avg Loss: 12.2926\n",
            "03/25/2025 18:46:16 - INFO - __main__ -   Epoch 7, Batch 50, Avg Loss: 12.1202\n",
            "03/25/2025 18:46:23 - INFO - __main__ -   Epoch 7, Batch 100, Avg Loss: 11.9540\n",
            "03/25/2025 18:46:30 - INFO - __main__ -   Epoch 7, Batch 150, Avg Loss: 11.9464\n",
            "03/25/2025 18:46:37 - INFO - __main__ -   Epoch 7, Batch 200, Avg Loss: 11.8912\n",
            "03/25/2025 18:46:44 - INFO - __main__ -   Epoch 7, Batch 250, Avg Loss: 11.9647\n",
            "03/25/2025 18:46:51 - INFO - __main__ -   Epoch 7, Batch 300, Avg Loss: 11.9248\n",
            "03/25/2025 18:46:58 - INFO - __main__ -   Epoch 7, Batch 350, Avg Loss: 11.8170\n",
            "03/25/2025 18:47:05 - INFO - __main__ -   Epoch 7, Batch 400, Avg Loss: 11.8031\n",
            "03/25/2025 18:47:12 - INFO - __main__ -   Epoch 7, Batch 450, Avg Loss: 11.8551\n",
            "03/25/2025 18:47:19 - INFO - __main__ -   Epoch 7, Batch 500, Avg Loss: 11.8594\n",
            "03/25/2025 18:47:26 - INFO - __main__ -   Epoch 7, Batch 550, Avg Loss: 11.8884\n",
            "03/25/2025 18:47:33 - INFO - __main__ -   Epoch 7, Batch 600, Avg Loss: 11.9612\n",
            "03/25/2025 18:47:40 - INFO - __main__ -   Epoch 7, Batch 650, Avg Loss: 12.1344\n",
            "03/25/2025 18:47:47 - INFO - __main__ -   Epoch 7, Batch 700, Avg Loss: 12.0866\n",
            "03/25/2025 18:47:54 - INFO - __main__ -   Epoch 7, Batch 750, Avg Loss: 12.0989\n",
            "03/25/2025 18:48:01 - INFO - __main__ -   Epoch 7, Batch 800, Avg Loss: 12.0937\n",
            "03/25/2025 18:48:08 - INFO - __main__ -   Epoch 7, Batch 850, Avg Loss: 12.1024\n",
            "03/25/2025 18:48:15 - INFO - __main__ -   Epoch 7, Batch 900, Avg Loss: 12.1235\n",
            "03/25/2025 18:48:22 - INFO - __main__ -   Epoch 7, Batch 950, Avg Loss: 12.1290\n",
            "03/25/2025 18:48:29 - INFO - __main__ -   Epoch 7, Batch 1000, Avg Loss: 12.1661\n",
            "03/25/2025 18:48:36 - INFO - __main__ -   Epoch 7, Batch 1050, Avg Loss: 12.1522\n",
            "03/25/2025 18:48:43 - INFO - __main__ -   Epoch 7, Batch 1100, Avg Loss: 12.1246\n",
            "03/25/2025 18:48:50 - INFO - __main__ -   Epoch 7, Batch 1150, Avg Loss: 12.1176\n",
            "03/25/2025 18:48:57 - INFO - __main__ -   Epoch 7, Batch 1200, Avg Loss: 12.1466\n",
            "03/25/2025 18:49:04 - INFO - __main__ -   Epoch 7, Batch 1250, Avg Loss: 12.1403\n",
            "03/25/2025 18:49:11 - INFO - __main__ -   Epoch 7, Batch 1300, Avg Loss: 12.1488\n",
            "03/25/2025 18:49:18 - INFO - __main__ -   Epoch 7, Batch 1350, Avg Loss: 12.1341\n",
            "03/25/2025 18:49:25 - INFO - __main__ -   Epoch 7, Batch 1400, Avg Loss: 12.1295\n",
            "03/25/2025 18:49:32 - INFO - __main__ -   Epoch 7, Batch 1450, Avg Loss: 12.1250\n",
            "03/25/2025 18:49:39 - INFO - __main__ -   Epoch 7, Batch 1500, Avg Loss: 12.1078\n",
            "03/25/2025 18:49:46 - INFO - __main__ -   Epoch 7, Batch 1550, Avg Loss: 12.0768\n",
            "03/25/2025 18:49:53 - INFO - __main__ -   Epoch 7, Batch 1600, Avg Loss: 12.0609\n",
            "03/25/2025 18:50:00 - INFO - __main__ -   Epoch 7, Batch 1650, Avg Loss: 12.0701\n",
            "03/25/2025 18:50:07 - INFO - __main__ -   Epoch 7, Batch 1700, Avg Loss: 12.0455\n",
            "03/25/2025 18:50:14 - INFO - __main__ -   Epoch 7, Batch 1750, Avg Loss: 12.0263\n",
            "03/25/2025 18:50:21 - INFO - __main__ -   Epoch 7, Batch 1800, Avg Loss: 12.0021\n",
            "03/25/2025 18:50:28 - INFO - __main__ -   Epoch 7, Batch 1850, Avg Loss: 11.9805\n",
            "03/25/2025 18:50:35 - INFO - __main__ -   Epoch 7, Batch 1900, Avg Loss: 11.9952\n",
            "03/25/2025 18:50:42 - INFO - __main__ -   Epoch 7, Batch 1950, Avg Loss: 11.9941\n",
            "03/25/2025 18:50:49 - INFO - __main__ -   Epoch 7, Batch 2000, Avg Loss: 12.0079\n",
            "03/25/2025 18:50:56 - INFO - __main__ -   Epoch 7, Batch 2050, Avg Loss: 12.0199\n",
            "03/25/2025 18:51:03 - INFO - __main__ -   Epoch 7, Batch 2100, Avg Loss: 12.0166\n",
            "03/25/2025 18:51:10 - INFO - __main__ -   Epoch 7, Batch 2150, Avg Loss: 11.9978\n",
            "03/25/2025 18:51:17 - INFO - __main__ -   Epoch 7, Batch 2200, Avg Loss: 11.9822\n",
            "03/25/2025 18:51:24 - INFO - __main__ -   Epoch 7, Batch 2250, Avg Loss: 11.9669\n",
            "03/25/2025 18:51:31 - INFO - __main__ -   Epoch 7, Batch 2300, Avg Loss: 11.9596\n",
            "03/25/2025 18:51:38 - INFO - __main__ -   Epoch 7, Batch 2350, Avg Loss: 11.9506\n",
            "03/25/2025 18:51:45 - INFO - __main__ -   Epoch 7, Batch 2400, Avg Loss: 11.9381\n",
            "03/25/2025 18:51:52 - INFO - __main__ -   Epoch 7, Batch 2450, Avg Loss: 11.9249\n",
            "03/25/2025 18:51:59 - INFO - __main__ -   Epoch 7, Batch 2500, Avg Loss: 11.9110\n",
            "03/25/2025 18:52:06 - INFO - __main__ -   Epoch 7, Batch 2550, Avg Loss: 11.9128\n",
            "03/25/2025 18:52:13 - INFO - __main__ -   Epoch 7, Batch 2600, Avg Loss: 12.0162\n",
            "03/25/2025 18:52:20 - INFO - __main__ -   Epoch 7, Batch 2650, Avg Loss: 12.0576\n",
            "03/25/2025 18:52:27 - INFO - __main__ -   Epoch 7, Batch 2700, Avg Loss: 12.0421\n",
            "03/25/2025 18:52:34 - INFO - __main__ -   Epoch 7, Batch 2750, Avg Loss: 12.0242\n",
            "03/25/2025 18:52:41 - INFO - __main__ -   Epoch 7, Batch 2800, Avg Loss: 12.0095\n",
            "03/25/2025 18:52:48 - INFO - __main__ -   Epoch 7, Batch 2850, Avg Loss: 11.9965\n",
            "03/25/2025 18:52:55 - INFO - __main__ -   Epoch 7, Batch 2900, Avg Loss: 11.9784\n",
            "03/25/2025 18:53:02 - INFO - __main__ -   Epoch 7, Batch 2950, Avg Loss: 11.9587\n",
            "03/25/2025 18:53:09 - INFO - __main__ -   Epoch 7, Batch 3000, Avg Loss: 11.9432\n",
            "03/25/2025 18:53:16 - INFO - __main__ -   Epoch 7, Batch 3050, Avg Loss: 11.9299\n",
            "03/25/2025 18:53:23 - INFO - __main__ -   Epoch 7, Batch 3100, Avg Loss: 11.9133\n",
            "03/25/2025 18:53:30 - INFO - __main__ -   Epoch 7, Batch 3150, Avg Loss: 11.9038\n",
            "03/25/2025 18:53:37 - INFO - __main__ -   Epoch 7, Batch 3200, Avg Loss: 11.8946\n",
            "03/25/2025 18:53:44 - INFO - __main__ -   Epoch 7, Batch 3250, Avg Loss: 11.8877\n",
            "03/25/2025 18:53:51 - INFO - __main__ -   Epoch 7, Batch 3300, Avg Loss: 11.9320\n",
            "03/25/2025 18:53:58 - INFO - __main__ -   Epoch 7, Batch 3350, Avg Loss: 11.9361\n",
            "03/25/2025 18:54:05 - INFO - __main__ -   Epoch 7, Batch 3400, Avg Loss: 11.9571\n",
            "03/25/2025 18:54:12 - INFO - __main__ -   Epoch 7, Batch 3450, Avg Loss: 11.9529\n",
            "03/25/2025 18:54:19 - INFO - __main__ -   Epoch 7, Batch 3500, Avg Loss: 11.9620\n",
            "03/25/2025 18:54:26 - INFO - __main__ -   Epoch 7, Batch 3550, Avg Loss: 11.9689\n",
            "03/25/2025 18:54:33 - INFO - __main__ -   Epoch 7, Batch 3600, Avg Loss: 11.9747\n",
            "03/25/2025 18:54:40 - INFO - __main__ -   Epoch 7, Batch 3650, Avg Loss: 11.9876\n",
            "03/25/2025 18:54:47 - INFO - __main__ -   Epoch 7, Batch 3700, Avg Loss: 11.9977\n",
            "03/25/2025 18:54:54 - INFO - __main__ -   Epoch 7, Batch 3750, Avg Loss: 11.9919\n",
            "03/25/2025 18:55:01 - INFO - __main__ -   Epoch 7, Batch 3800, Avg Loss: 11.9934\n",
            "03/25/2025 18:55:08 - INFO - __main__ -   Epoch 7, Batch 3850, Avg Loss: 11.9984\n",
            "03/25/2025 18:55:15 - INFO - __main__ -   Epoch 7, Batch 3900, Avg Loss: 12.0022\n",
            "03/25/2025 18:55:22 - INFO - __main__ -   Epoch 7, Batch 3950, Avg Loss: 12.0061\n",
            "03/25/2025 18:55:29 - INFO - __main__ -   Epoch 7, Batch 4000, Avg Loss: 12.0019\n",
            "03/25/2025 18:55:36 - INFO - __main__ -   Epoch 7, Batch 4050, Avg Loss: 12.0035\n",
            "03/25/2025 18:55:43 - INFO - __main__ -   Epoch 7, Batch 4100, Avg Loss: 12.0028\n",
            "03/25/2025 18:55:50 - INFO - __main__ -   Epoch 7, Batch 4150, Avg Loss: 12.0008\n",
            "03/25/2025 18:55:57 - INFO - __main__ -   Epoch 7, Batch 4200, Avg Loss: 12.0175\n",
            "03/25/2025 18:56:04 - INFO - __main__ -   Epoch 7, Batch 4250, Avg Loss: 12.0251\n",
            "03/25/2025 18:56:11 - INFO - __main__ -   Epoch 7, Batch 4300, Avg Loss: 12.0346\n",
            "03/25/2025 18:56:18 - INFO - __main__ -   Epoch 7, Batch 4350, Avg Loss: 12.0293\n",
            "03/25/2025 18:56:25 - INFO - __main__ -   Epoch 7, Batch 4400, Avg Loss: 12.0281\n",
            "03/25/2025 18:56:32 - INFO - __main__ -   Epoch 7, Batch 4450, Avg Loss: 12.0361\n",
            "03/25/2025 18:56:39 - INFO - __main__ -   Epoch 7, Batch 4500, Avg Loss: 12.0378\n",
            "03/25/2025 18:56:46 - INFO - __main__ -   Epoch 7, Batch 4550, Avg Loss: 12.0440\n",
            "03/25/2025 18:56:53 - INFO - __main__ -   Epoch 7, Batch 4600, Avg Loss: 12.0400\n",
            "03/25/2025 18:57:00 - INFO - __main__ -   Epoch 7, Batch 4650, Avg Loss: 12.0310\n",
            "03/25/2025 18:57:07 - INFO - __main__ -   Epoch 7, Batch 4700, Avg Loss: 12.0310\n",
            "03/25/2025 18:57:14 - INFO - __main__ -   Epoch 7, Batch 4750, Avg Loss: 12.0293\n",
            "03/25/2025 18:57:21 - INFO - __main__ -   Epoch 7, Batch 4800, Avg Loss: 12.0473\n",
            "03/25/2025 18:57:28 - INFO - __main__ -   Epoch 7, Batch 4850, Avg Loss: 12.0481\n",
            "03/25/2025 18:57:35 - INFO - __main__ -   Epoch 7, Batch 4900, Avg Loss: 12.0524\n",
            "03/25/2025 18:57:42 - INFO - __main__ -   Epoch 7, Batch 4950, Avg Loss: 12.0501\n",
            "03/25/2025 18:57:49 - INFO - __main__ -   Epoch 7, Batch 5000, Avg Loss: 12.0532\n",
            "03/25/2025 18:57:56 - INFO - __main__ -   Epoch 7, Batch 5050, Avg Loss: 12.0522\n",
            "03/25/2025 18:58:03 - INFO - __main__ -   Epoch 7, Batch 5100, Avg Loss: 12.0596\n",
            "03/25/2025 18:58:10 - INFO - __main__ -   Epoch 7, Batch 5150, Avg Loss: 12.0729\n",
            "03/25/2025 18:58:17 - INFO - __main__ -   Epoch 7, Batch 5200, Avg Loss: 12.0696\n",
            "03/25/2025 18:58:24 - INFO - __main__ -   Epoch 7, Batch 5250, Avg Loss: 12.0598\n",
            "03/25/2025 18:58:31 - INFO - __main__ -   Epoch 7, Batch 5300, Avg Loss: 12.0578\n",
            "03/25/2025 18:58:38 - INFO - __main__ -   Epoch 7, Batch 5350, Avg Loss: 12.0495\n",
            "03/25/2025 18:58:45 - INFO - __main__ -   Epoch 7, Batch 5400, Avg Loss: 12.0466\n",
            "03/25/2025 18:58:52 - INFO - __main__ -   Epoch 7, Batch 5450, Avg Loss: 12.0477\n",
            "03/25/2025 18:58:59 - INFO - __main__ -   Epoch 7, Batch 5500, Avg Loss: 12.0429\n",
            "03/25/2025 18:59:06 - INFO - __main__ -   Epoch 7, Batch 5550, Avg Loss: 12.0422\n",
            "03/25/2025 18:59:13 - INFO - __main__ -   Epoch 7, Batch 5600, Avg Loss: 12.0370\n",
            "03/25/2025 18:59:20 - INFO - __main__ -   Epoch 7, Batch 5650, Avg Loss: 12.0465\n",
            "03/25/2025 18:59:27 - INFO - __main__ -   Epoch 7, Batch 5700, Avg Loss: 12.0438\n",
            "03/25/2025 18:59:34 - INFO - __main__ -   Epoch 7, Batch 5750, Avg Loss: 12.0342\n",
            "03/25/2025 18:59:41 - INFO - __main__ -   Epoch 7, Batch 5800, Avg Loss: 12.0280\n",
            "03/25/2025 18:59:48 - INFO - __main__ -   Epoch 7, Batch 5850, Avg Loss: 12.0230\n",
            "03/25/2025 18:59:55 - INFO - __main__ -   Epoch 7, Batch 5900, Avg Loss: 12.0184\n",
            "03/25/2025 19:00:02 - INFO - __main__ -   Epoch 7, Batch 5950, Avg Loss: 12.0189\n",
            "03/25/2025 19:00:09 - INFO - __main__ -   Epoch 7, Batch 6000, Avg Loss: 12.0287\n",
            "03/25/2025 19:00:16 - INFO - __main__ -   Epoch 7, Batch 6050, Avg Loss: 12.0246\n",
            "03/25/2025 19:00:23 - INFO - __main__ -   Epoch 7, Batch 6100, Avg Loss: 12.0229\n",
            "03/25/2025 19:00:30 - INFO - __main__ -   Epoch 7, Batch 6150, Avg Loss: 12.0199\n",
            "03/25/2025 19:00:37 - INFO - __main__ -   Epoch 7, Batch 6200, Avg Loss: 12.0199\n",
            "03/25/2025 19:00:44 - INFO - __main__ -   Epoch 7, Batch 6250, Avg Loss: 12.0244\n",
            "03/25/2025 19:00:51 - INFO - __main__ -   Epoch 7, Batch 6300, Avg Loss: 12.0155\n",
            "03/25/2025 19:00:58 - INFO - __main__ -   Epoch 7, Batch 6350, Avg Loss: 12.0181\n",
            "03/25/2025 19:01:05 - INFO - __main__ -   Epoch 7, Batch 6400, Avg Loss: 12.0208\n",
            "03/25/2025 19:01:12 - INFO - __main__ -   Epoch 7, Batch 6450, Avg Loss: 12.0206\n",
            "03/25/2025 19:01:19 - INFO - __main__ -   Epoch 7, Batch 6500, Avg Loss: 12.0176\n",
            "03/25/2025 19:01:26 - INFO - __main__ -   Epoch 7, Batch 6550, Avg Loss: 12.0340\n",
            "03/25/2025 19:01:33 - INFO - __main__ -   Epoch 7, Batch 6600, Avg Loss: 12.0333\n",
            "03/25/2025 19:01:40 - INFO - __main__ -   Epoch 7, Batch 6650, Avg Loss: 12.0346\n",
            "03/25/2025 19:01:47 - INFO - __main__ -   Epoch 7, Batch 6700, Avg Loss: 12.0386\n",
            "03/25/2025 19:01:54 - INFO - __main__ -   Epoch 7, Batch 6750, Avg Loss: 12.0416\n",
            "03/25/2025 19:02:01 - INFO - __main__ -   Epoch 7, Batch 6800, Avg Loss: 12.0418\n",
            "03/25/2025 19:02:08 - INFO - __main__ -   Epoch 7, Batch 6850, Avg Loss: 12.0431\n",
            "03/25/2025 19:02:15 - INFO - __main__ -   Epoch 7, Batch 6900, Avg Loss: 12.0451\n",
            "03/25/2025 19:02:22 - INFO - __main__ -   Epoch 7, Batch 6950, Avg Loss: 12.0408\n",
            "03/25/2025 19:02:29 - INFO - __main__ -   Epoch 7, Batch 7000, Avg Loss: 12.0476\n",
            "03/25/2025 19:02:36 - INFO - __main__ -   Epoch 7, Batch 7050, Avg Loss: 12.0536\n",
            "03/25/2025 19:02:43 - INFO - __main__ -   Epoch 7, Batch 7100, Avg Loss: 12.0501\n",
            "03/25/2025 19:02:50 - INFO - __main__ -   Epoch 7, Batch 7150, Avg Loss: 12.0499\n",
            "03/25/2025 19:02:57 - INFO - __main__ -   Epoch 7, Batch 7200, Avg Loss: 12.0540\n",
            "03/25/2025 19:03:04 - INFO - __main__ -   Epoch 7, Batch 7250, Avg Loss: 12.0650\n",
            "03/25/2025 19:03:11 - INFO - __main__ -   Epoch 7, Batch 7300, Avg Loss: 12.0679\n",
            "03/25/2025 19:03:18 - INFO - __main__ -   Epoch 7, Batch 7350, Avg Loss: 12.0661\n",
            "03/25/2025 19:03:25 - INFO - __main__ -   Epoch 7, Batch 7400, Avg Loss: 12.0681\n",
            "03/25/2025 19:03:32 - INFO - __main__ -   Epoch 7, Batch 7450, Avg Loss: 12.0786\n",
            "03/25/2025 19:03:39 - INFO - __main__ -   Epoch 7, Batch 7500, Avg Loss: 12.0764\n",
            "03/25/2025 19:03:46 - INFO - __main__ -   Epoch 7, Batch 7550, Avg Loss: 12.0713\n",
            "03/25/2025 19:03:53 - INFO - __main__ -   Epoch 7, Batch 7600, Avg Loss: 12.0752\n",
            "03/25/2025 19:04:00 - INFO - __main__ -   Epoch 7, Batch 7650, Avg Loss: 12.0706\n",
            "03/25/2025 19:04:07 - INFO - __main__ -   Epoch 7, Batch 7700, Avg Loss: 12.0667\n",
            "03/25/2025 19:04:14 - INFO - __main__ -   Epoch 7, Batch 7750, Avg Loss: 12.0879\n",
            "03/25/2025 19:04:21 - INFO - __main__ -   Epoch 7, Batch 7800, Avg Loss: 12.0916\n",
            "03/25/2025 19:04:28 - INFO - __main__ -   Epoch 7, Batch 7850, Avg Loss: 12.0901\n",
            "03/25/2025 19:04:31 - INFO - __main__ -   Epoch 7 completed, Avg Loss: 12.0911\n",
            "03/25/2025 19:04:44 - INFO - __main__ -   Epoch 8, Batch 50, Avg Loss: 12.1999\n",
            "03/25/2025 19:04:51 - INFO - __main__ -   Epoch 8, Batch 100, Avg Loss: 11.8868\n",
            "03/25/2025 19:04:58 - INFO - __main__ -   Epoch 8, Batch 150, Avg Loss: 11.8512\n",
            "03/25/2025 19:05:05 - INFO - __main__ -   Epoch 8, Batch 200, Avg Loss: 11.7908\n",
            "03/25/2025 19:05:12 - INFO - __main__ -   Epoch 8, Batch 250, Avg Loss: 11.8480\n",
            "03/25/2025 19:05:19 - INFO - __main__ -   Epoch 8, Batch 300, Avg Loss: 11.8089\n",
            "03/25/2025 19:05:26 - INFO - __main__ -   Epoch 8, Batch 350, Avg Loss: 11.7044\n",
            "03/25/2025 19:05:33 - INFO - __main__ -   Epoch 8, Batch 400, Avg Loss: 11.6994\n",
            "03/25/2025 19:05:40 - INFO - __main__ -   Epoch 8, Batch 450, Avg Loss: 11.7586\n",
            "03/25/2025 19:05:47 - INFO - __main__ -   Epoch 8, Batch 500, Avg Loss: 11.7656\n",
            "03/25/2025 19:05:54 - INFO - __main__ -   Epoch 8, Batch 550, Avg Loss: 11.7963\n",
            "03/25/2025 19:06:01 - INFO - __main__ -   Epoch 8, Batch 600, Avg Loss: 11.8765\n",
            "03/25/2025 19:06:08 - INFO - __main__ -   Epoch 8, Batch 650, Avg Loss: 12.0433\n",
            "03/25/2025 19:06:15 - INFO - __main__ -   Epoch 8, Batch 700, Avg Loss: 11.9950\n",
            "03/25/2025 19:06:22 - INFO - __main__ -   Epoch 8, Batch 750, Avg Loss: 12.0208\n",
            "03/25/2025 19:06:29 - INFO - __main__ -   Epoch 8, Batch 800, Avg Loss: 12.0117\n",
            "03/25/2025 19:06:36 - INFO - __main__ -   Epoch 8, Batch 850, Avg Loss: 12.0216\n",
            "03/25/2025 19:06:43 - INFO - __main__ -   Epoch 8, Batch 900, Avg Loss: 12.0366\n",
            "03/25/2025 19:06:50 - INFO - __main__ -   Epoch 8, Batch 950, Avg Loss: 12.0481\n",
            "03/25/2025 19:06:57 - INFO - __main__ -   Epoch 8, Batch 1000, Avg Loss: 12.0918\n",
            "03/25/2025 19:07:04 - INFO - __main__ -   Epoch 8, Batch 1050, Avg Loss: 12.0831\n",
            "03/25/2025 19:07:11 - INFO - __main__ -   Epoch 8, Batch 1100, Avg Loss: 12.0543\n",
            "03/25/2025 19:07:18 - INFO - __main__ -   Epoch 8, Batch 1150, Avg Loss: 12.0509\n",
            "03/25/2025 19:07:25 - INFO - __main__ -   Epoch 8, Batch 1200, Avg Loss: 12.0749\n",
            "03/25/2025 19:07:32 - INFO - __main__ -   Epoch 8, Batch 1250, Avg Loss: 12.0711\n",
            "03/25/2025 19:07:39 - INFO - __main__ -   Epoch 8, Batch 1300, Avg Loss: 12.0803\n",
            "03/25/2025 19:07:46 - INFO - __main__ -   Epoch 8, Batch 1350, Avg Loss: 12.0640\n",
            "03/25/2025 19:07:53 - INFO - __main__ -   Epoch 8, Batch 1400, Avg Loss: 12.0584\n",
            "03/25/2025 19:08:00 - INFO - __main__ -   Epoch 8, Batch 1450, Avg Loss: 12.0469\n",
            "03/25/2025 19:08:07 - INFO - __main__ -   Epoch 8, Batch 1500, Avg Loss: 12.0315\n",
            "03/25/2025 19:08:14 - INFO - __main__ -   Epoch 8, Batch 1550, Avg Loss: 12.0025\n",
            "03/25/2025 19:08:21 - INFO - __main__ -   Epoch 8, Batch 1600, Avg Loss: 11.9870\n",
            "03/25/2025 19:08:28 - INFO - __main__ -   Epoch 8, Batch 1650, Avg Loss: 11.9917\n",
            "03/25/2025 19:08:35 - INFO - __main__ -   Epoch 8, Batch 1700, Avg Loss: 11.9683\n",
            "03/25/2025 19:08:42 - INFO - __main__ -   Epoch 8, Batch 1750, Avg Loss: 11.9505\n",
            "03/25/2025 19:08:49 - INFO - __main__ -   Epoch 8, Batch 1800, Avg Loss: 11.9252\n",
            "03/25/2025 19:08:56 - INFO - __main__ -   Epoch 8, Batch 1850, Avg Loss: 11.9049\n",
            "03/25/2025 19:09:03 - INFO - __main__ -   Epoch 8, Batch 1900, Avg Loss: 11.9182\n",
            "03/25/2025 19:09:10 - INFO - __main__ -   Epoch 8, Batch 1950, Avg Loss: 11.9164\n",
            "03/25/2025 19:09:17 - INFO - __main__ -   Epoch 8, Batch 2000, Avg Loss: 11.9301\n",
            "03/25/2025 19:09:24 - INFO - __main__ -   Epoch 8, Batch 2050, Avg Loss: 11.9407\n",
            "03/25/2025 19:09:31 - INFO - __main__ -   Epoch 8, Batch 2100, Avg Loss: 11.9330\n",
            "03/25/2025 19:09:38 - INFO - __main__ -   Epoch 8, Batch 2150, Avg Loss: 11.9143\n",
            "03/25/2025 19:09:45 - INFO - __main__ -   Epoch 8, Batch 2200, Avg Loss: 11.8989\n",
            "03/25/2025 19:09:52 - INFO - __main__ -   Epoch 8, Batch 2250, Avg Loss: 11.8824\n",
            "03/25/2025 19:09:59 - INFO - __main__ -   Epoch 8, Batch 2300, Avg Loss: 11.8752\n",
            "03/25/2025 19:10:06 - INFO - __main__ -   Epoch 8, Batch 2350, Avg Loss: 11.8643\n",
            "03/25/2025 19:10:13 - INFO - __main__ -   Epoch 8, Batch 2400, Avg Loss: 11.8525\n",
            "03/25/2025 19:10:20 - INFO - __main__ -   Epoch 8, Batch 2450, Avg Loss: 11.8389\n",
            "03/25/2025 19:10:27 - INFO - __main__ -   Epoch 8, Batch 2500, Avg Loss: 11.8247\n",
            "03/25/2025 19:10:34 - INFO - __main__ -   Epoch 8, Batch 2550, Avg Loss: 11.8251\n",
            "03/25/2025 19:10:41 - INFO - __main__ -   Epoch 8, Batch 2600, Avg Loss: 11.9204\n",
            "03/25/2025 19:10:48 - INFO - __main__ -   Epoch 8, Batch 2650, Avg Loss: 11.9569\n",
            "03/25/2025 19:10:55 - INFO - __main__ -   Epoch 8, Batch 2700, Avg Loss: 11.9427\n",
            "03/25/2025 19:11:02 - INFO - __main__ -   Epoch 8, Batch 2750, Avg Loss: 11.9270\n",
            "03/25/2025 19:11:09 - INFO - __main__ -   Epoch 8, Batch 2800, Avg Loss: 11.9119\n",
            "03/25/2025 19:11:16 - INFO - __main__ -   Epoch 8, Batch 2850, Avg Loss: 11.8993\n",
            "03/25/2025 19:11:23 - INFO - __main__ -   Epoch 8, Batch 2900, Avg Loss: 11.8823\n",
            "03/25/2025 19:11:30 - INFO - __main__ -   Epoch 8, Batch 2950, Avg Loss: 11.8631\n",
            "03/25/2025 19:11:37 - INFO - __main__ -   Epoch 8, Batch 3000, Avg Loss: 11.8501\n",
            "03/25/2025 19:11:44 - INFO - __main__ -   Epoch 8, Batch 3050, Avg Loss: 11.8386\n",
            "03/25/2025 19:11:51 - INFO - __main__ -   Epoch 8, Batch 3100, Avg Loss: 11.8229\n",
            "03/25/2025 19:11:58 - INFO - __main__ -   Epoch 8, Batch 3150, Avg Loss: 11.8134\n",
            "03/25/2025 19:12:05 - INFO - __main__ -   Epoch 8, Batch 3200, Avg Loss: 11.8045\n",
            "03/25/2025 19:12:12 - INFO - __main__ -   Epoch 8, Batch 3250, Avg Loss: 11.7994\n",
            "03/25/2025 19:12:19 - INFO - __main__ -   Epoch 8, Batch 3300, Avg Loss: 11.8412\n",
            "03/25/2025 19:12:26 - INFO - __main__ -   Epoch 8, Batch 3350, Avg Loss: 11.8489\n",
            "03/25/2025 19:12:33 - INFO - __main__ -   Epoch 8, Batch 3400, Avg Loss: 11.8726\n",
            "03/25/2025 19:12:40 - INFO - __main__ -   Epoch 8, Batch 3450, Avg Loss: 11.8664\n",
            "03/25/2025 19:12:47 - INFO - __main__ -   Epoch 8, Batch 3500, Avg Loss: 11.8734\n",
            "03/25/2025 19:12:54 - INFO - __main__ -   Epoch 8, Batch 3550, Avg Loss: 11.8791\n",
            "03/25/2025 19:13:01 - INFO - __main__ -   Epoch 8, Batch 3600, Avg Loss: 11.8859\n",
            "03/25/2025 19:13:08 - INFO - __main__ -   Epoch 8, Batch 3650, Avg Loss: 11.8967\n",
            "03/25/2025 19:13:15 - INFO - __main__ -   Epoch 8, Batch 3700, Avg Loss: 11.9101\n",
            "03/25/2025 19:13:22 - INFO - __main__ -   Epoch 8, Batch 3750, Avg Loss: 11.9056\n",
            "03/25/2025 19:13:29 - INFO - __main__ -   Epoch 8, Batch 3800, Avg Loss: 11.9070\n",
            "03/25/2025 19:13:36 - INFO - __main__ -   Epoch 8, Batch 3850, Avg Loss: 11.9148\n",
            "03/25/2025 19:13:43 - INFO - __main__ -   Epoch 8, Batch 3900, Avg Loss: 11.9181\n",
            "03/25/2025 19:13:50 - INFO - __main__ -   Epoch 8, Batch 3950, Avg Loss: 11.9204\n",
            "03/25/2025 19:13:57 - INFO - __main__ -   Epoch 8, Batch 4000, Avg Loss: 11.9158\n",
            "03/25/2025 19:14:04 - INFO - __main__ -   Epoch 8, Batch 4050, Avg Loss: 11.9166\n",
            "03/25/2025 19:14:11 - INFO - __main__ -   Epoch 8, Batch 4100, Avg Loss: 11.9167\n",
            "03/25/2025 19:14:18 - INFO - __main__ -   Epoch 8, Batch 4150, Avg Loss: 11.9149\n",
            "03/25/2025 19:14:25 - INFO - __main__ -   Epoch 8, Batch 4200, Avg Loss: 11.9316\n",
            "03/25/2025 19:14:32 - INFO - __main__ -   Epoch 8, Batch 4250, Avg Loss: 11.9378\n",
            "03/25/2025 19:14:39 - INFO - __main__ -   Epoch 8, Batch 4300, Avg Loss: 11.9478\n",
            "03/25/2025 19:14:46 - INFO - __main__ -   Epoch 8, Batch 4350, Avg Loss: 11.9411\n",
            "03/25/2025 19:14:53 - INFO - __main__ -   Epoch 8, Batch 4400, Avg Loss: 11.9382\n",
            "03/25/2025 19:15:00 - INFO - __main__ -   Epoch 8, Batch 4450, Avg Loss: 11.9466\n",
            "03/25/2025 19:15:07 - INFO - __main__ -   Epoch 8, Batch 4500, Avg Loss: 11.9485\n",
            "03/25/2025 19:15:14 - INFO - __main__ -   Epoch 8, Batch 4550, Avg Loss: 11.9527\n",
            "03/25/2025 19:15:21 - INFO - __main__ -   Epoch 8, Batch 4600, Avg Loss: 11.9491\n",
            "03/25/2025 19:15:28 - INFO - __main__ -   Epoch 8, Batch 4650, Avg Loss: 11.9416\n",
            "03/25/2025 19:15:35 - INFO - __main__ -   Epoch 8, Batch 4700, Avg Loss: 11.9418\n",
            "03/25/2025 19:15:42 - INFO - __main__ -   Epoch 8, Batch 4750, Avg Loss: 11.9423\n",
            "03/25/2025 19:15:49 - INFO - __main__ -   Epoch 8, Batch 4800, Avg Loss: 11.9591\n",
            "03/25/2025 19:15:56 - INFO - __main__ -   Epoch 8, Batch 4850, Avg Loss: 11.9595\n",
            "03/25/2025 19:16:03 - INFO - __main__ -   Epoch 8, Batch 4900, Avg Loss: 11.9652\n",
            "03/25/2025 19:16:10 - INFO - __main__ -   Epoch 8, Batch 4950, Avg Loss: 11.9640\n",
            "03/25/2025 19:16:17 - INFO - __main__ -   Epoch 8, Batch 5000, Avg Loss: 11.9676\n",
            "03/25/2025 19:16:24 - INFO - __main__ -   Epoch 8, Batch 5050, Avg Loss: 11.9661\n",
            "03/25/2025 19:16:31 - INFO - __main__ -   Epoch 8, Batch 5100, Avg Loss: 11.9734\n",
            "03/25/2025 19:16:38 - INFO - __main__ -   Epoch 8, Batch 5150, Avg Loss: 11.9858\n",
            "03/25/2025 19:16:45 - INFO - __main__ -   Epoch 8, Batch 5200, Avg Loss: 11.9833\n",
            "03/25/2025 19:16:52 - INFO - __main__ -   Epoch 8, Batch 5250, Avg Loss: 11.9735\n",
            "03/25/2025 19:16:59 - INFO - __main__ -   Epoch 8, Batch 5300, Avg Loss: 11.9714\n",
            "03/25/2025 19:17:06 - INFO - __main__ -   Epoch 8, Batch 5350, Avg Loss: 11.9652\n",
            "03/25/2025 19:17:13 - INFO - __main__ -   Epoch 8, Batch 5400, Avg Loss: 11.9640\n",
            "03/25/2025 19:17:20 - INFO - __main__ -   Epoch 8, Batch 5450, Avg Loss: 11.9650\n",
            "03/25/2025 19:17:27 - INFO - __main__ -   Epoch 8, Batch 5500, Avg Loss: 11.9601\n",
            "03/25/2025 19:17:34 - INFO - __main__ -   Epoch 8, Batch 5550, Avg Loss: 11.9600\n",
            "03/25/2025 19:17:41 - INFO - __main__ -   Epoch 8, Batch 5600, Avg Loss: 11.9553\n",
            "03/25/2025 19:17:48 - INFO - __main__ -   Epoch 8, Batch 5650, Avg Loss: 11.9634\n",
            "03/25/2025 19:17:55 - INFO - __main__ -   Epoch 8, Batch 5700, Avg Loss: 11.9605\n",
            "03/25/2025 19:18:02 - INFO - __main__ -   Epoch 8, Batch 5750, Avg Loss: 11.9516\n",
            "03/25/2025 19:18:09 - INFO - __main__ -   Epoch 8, Batch 5800, Avg Loss: 11.9463\n",
            "03/25/2025 19:18:16 - INFO - __main__ -   Epoch 8, Batch 5850, Avg Loss: 11.9413\n",
            "03/25/2025 19:18:23 - INFO - __main__ -   Epoch 8, Batch 5900, Avg Loss: 11.9362\n",
            "03/25/2025 19:18:30 - INFO - __main__ -   Epoch 8, Batch 5950, Avg Loss: 11.9360\n",
            "03/25/2025 19:18:37 - INFO - __main__ -   Epoch 8, Batch 6000, Avg Loss: 11.9455\n",
            "03/25/2025 19:18:44 - INFO - __main__ -   Epoch 8, Batch 6050, Avg Loss: 11.9411\n",
            "03/25/2025 19:18:51 - INFO - __main__ -   Epoch 8, Batch 6100, Avg Loss: 11.9391\n",
            "03/25/2025 19:18:58 - INFO - __main__ -   Epoch 8, Batch 6150, Avg Loss: 11.9364\n",
            "03/25/2025 19:19:05 - INFO - __main__ -   Epoch 8, Batch 6200, Avg Loss: 11.9360\n",
            "03/25/2025 19:19:12 - INFO - __main__ -   Epoch 8, Batch 6250, Avg Loss: 11.9394\n",
            "03/25/2025 19:19:19 - INFO - __main__ -   Epoch 8, Batch 6300, Avg Loss: 11.9305\n",
            "03/25/2025 19:19:26 - INFO - __main__ -   Epoch 8, Batch 6350, Avg Loss: 11.9339\n",
            "03/25/2025 19:19:33 - INFO - __main__ -   Epoch 8, Batch 6400, Avg Loss: 11.9358\n",
            "03/25/2025 19:19:40 - INFO - __main__ -   Epoch 8, Batch 6450, Avg Loss: 11.9359\n",
            "03/25/2025 19:19:47 - INFO - __main__ -   Epoch 8, Batch 6500, Avg Loss: 11.9327\n",
            "03/25/2025 19:19:54 - INFO - __main__ -   Epoch 8, Batch 6550, Avg Loss: 11.9499\n",
            "03/25/2025 19:20:01 - INFO - __main__ -   Epoch 8, Batch 6600, Avg Loss: 11.9493\n",
            "03/25/2025 19:20:08 - INFO - __main__ -   Epoch 8, Batch 6650, Avg Loss: 11.9499\n",
            "03/25/2025 19:20:15 - INFO - __main__ -   Epoch 8, Batch 6700, Avg Loss: 11.9532\n",
            "03/25/2025 19:20:22 - INFO - __main__ -   Epoch 8, Batch 6750, Avg Loss: 11.9561\n",
            "03/25/2025 19:20:29 - INFO - __main__ -   Epoch 8, Batch 6800, Avg Loss: 11.9562\n",
            "03/25/2025 19:20:36 - INFO - __main__ -   Epoch 8, Batch 6850, Avg Loss: 11.9579\n",
            "03/25/2025 19:20:43 - INFO - __main__ -   Epoch 8, Batch 6900, Avg Loss: 11.9613\n",
            "03/25/2025 19:20:50 - INFO - __main__ -   Epoch 8, Batch 6950, Avg Loss: 11.9574\n",
            "03/25/2025 19:20:57 - INFO - __main__ -   Epoch 8, Batch 7000, Avg Loss: 11.9648\n",
            "03/25/2025 19:21:04 - INFO - __main__ -   Epoch 8, Batch 7050, Avg Loss: 11.9714\n",
            "03/25/2025 19:21:11 - INFO - __main__ -   Epoch 8, Batch 7100, Avg Loss: 11.9676\n",
            "03/25/2025 19:21:18 - INFO - __main__ -   Epoch 8, Batch 7150, Avg Loss: 11.9675\n",
            "03/25/2025 19:21:25 - INFO - __main__ -   Epoch 8, Batch 7200, Avg Loss: 11.9710\n",
            "03/25/2025 19:21:32 - INFO - __main__ -   Epoch 8, Batch 7250, Avg Loss: 11.9811\n",
            "03/25/2025 19:21:39 - INFO - __main__ -   Epoch 8, Batch 7300, Avg Loss: 11.9843\n",
            "03/25/2025 19:21:46 - INFO - __main__ -   Epoch 8, Batch 7350, Avg Loss: 11.9829\n",
            "03/25/2025 19:21:53 - INFO - __main__ -   Epoch 8, Batch 7400, Avg Loss: 11.9848\n",
            "03/25/2025 19:22:00 - INFO - __main__ -   Epoch 8, Batch 7450, Avg Loss: 11.9952\n",
            "03/25/2025 19:22:07 - INFO - __main__ -   Epoch 8, Batch 7500, Avg Loss: 11.9929\n",
            "03/25/2025 19:22:14 - INFO - __main__ -   Epoch 8, Batch 7550, Avg Loss: 11.9874\n",
            "03/25/2025 19:22:21 - INFO - __main__ -   Epoch 8, Batch 7600, Avg Loss: 11.9918\n",
            "03/25/2025 19:22:28 - INFO - __main__ -   Epoch 8, Batch 7650, Avg Loss: 11.9866\n",
            "03/25/2025 19:22:35 - INFO - __main__ -   Epoch 8, Batch 7700, Avg Loss: 11.9823\n",
            "03/25/2025 19:22:42 - INFO - __main__ -   Epoch 8, Batch 7750, Avg Loss: 12.0018\n",
            "03/25/2025 19:22:49 - INFO - __main__ -   Epoch 8, Batch 7800, Avg Loss: 12.0055\n",
            "03/25/2025 19:22:56 - INFO - __main__ -   Epoch 8, Batch 7850, Avg Loss: 12.0051\n",
            "03/25/2025 19:22:59 - INFO - __main__ -   Epoch 8 completed, Avg Loss: 12.0058\n",
            "03/25/2025 19:23:11 - INFO - __main__ -   Epoch 9, Batch 50, Avg Loss: 12.1280\n",
            "03/25/2025 19:23:18 - INFO - __main__ -   Epoch 9, Batch 100, Avg Loss: 11.9564\n",
            "03/25/2025 19:23:25 - INFO - __main__ -   Epoch 9, Batch 150, Avg Loss: 11.9197\n",
            "03/25/2025 19:23:32 - INFO - __main__ -   Epoch 9, Batch 200, Avg Loss: 11.8656\n",
            "03/25/2025 19:23:39 - INFO - __main__ -   Epoch 9, Batch 250, Avg Loss: 11.9120\n",
            "03/25/2025 19:23:46 - INFO - __main__ -   Epoch 9, Batch 300, Avg Loss: 11.8615\n",
            "03/25/2025 19:23:53 - INFO - __main__ -   Epoch 9, Batch 350, Avg Loss: 11.7463\n",
            "03/25/2025 19:24:00 - INFO - __main__ -   Epoch 9, Batch 400, Avg Loss: 11.7248\n",
            "03/25/2025 19:24:07 - INFO - __main__ -   Epoch 9, Batch 450, Avg Loss: 11.7681\n",
            "03/25/2025 19:24:14 - INFO - __main__ -   Epoch 9, Batch 500, Avg Loss: 11.7753\n",
            "03/25/2025 19:24:21 - INFO - __main__ -   Epoch 9, Batch 550, Avg Loss: 11.8064\n",
            "03/25/2025 19:24:28 - INFO - __main__ -   Epoch 9, Batch 600, Avg Loss: 11.8720\n",
            "03/25/2025 19:24:35 - INFO - __main__ -   Epoch 9, Batch 650, Avg Loss: 12.0126\n",
            "03/25/2025 19:24:42 - INFO - __main__ -   Epoch 9, Batch 700, Avg Loss: 11.9709\n",
            "03/25/2025 19:24:49 - INFO - __main__ -   Epoch 9, Batch 750, Avg Loss: 11.9953\n",
            "03/25/2025 19:24:56 - INFO - __main__ -   Epoch 9, Batch 800, Avg Loss: 11.9790\n",
            "03/25/2025 19:25:03 - INFO - __main__ -   Epoch 9, Batch 850, Avg Loss: 11.9822\n",
            "03/25/2025 19:25:10 - INFO - __main__ -   Epoch 9, Batch 900, Avg Loss: 11.9986\n",
            "03/25/2025 19:25:17 - INFO - __main__ -   Epoch 9, Batch 950, Avg Loss: 12.0137\n",
            "03/25/2025 19:25:24 - INFO - __main__ -   Epoch 9, Batch 1000, Avg Loss: 12.0468\n",
            "03/25/2025 19:25:31 - INFO - __main__ -   Epoch 9, Batch 1050, Avg Loss: 12.0342\n",
            "03/25/2025 19:25:38 - INFO - __main__ -   Epoch 9, Batch 1100, Avg Loss: 12.0100\n",
            "03/25/2025 19:25:45 - INFO - __main__ -   Epoch 9, Batch 1150, Avg Loss: 12.0015\n",
            "03/25/2025 19:25:52 - INFO - __main__ -   Epoch 9, Batch 1200, Avg Loss: 12.0286\n",
            "03/25/2025 19:25:59 - INFO - __main__ -   Epoch 9, Batch 1250, Avg Loss: 12.0226\n",
            "03/25/2025 19:26:06 - INFO - __main__ -   Epoch 9, Batch 1300, Avg Loss: 12.0341\n",
            "03/25/2025 19:26:13 - INFO - __main__ -   Epoch 9, Batch 1350, Avg Loss: 12.0212\n",
            "03/25/2025 19:26:20 - INFO - __main__ -   Epoch 9, Batch 1400, Avg Loss: 12.0172\n",
            "03/25/2025 19:26:27 - INFO - __main__ -   Epoch 9, Batch 1450, Avg Loss: 12.0065\n",
            "03/25/2025 19:26:34 - INFO - __main__ -   Epoch 9, Batch 1500, Avg Loss: 11.9865\n",
            "03/25/2025 19:26:41 - INFO - __main__ -   Epoch 9, Batch 1550, Avg Loss: 11.9567\n",
            "03/25/2025 19:26:48 - INFO - __main__ -   Epoch 9, Batch 1600, Avg Loss: 11.9443\n",
            "03/25/2025 19:26:55 - INFO - __main__ -   Epoch 9, Batch 1650, Avg Loss: 11.9507\n",
            "03/25/2025 19:27:02 - INFO - __main__ -   Epoch 9, Batch 1700, Avg Loss: 11.9304\n",
            "03/25/2025 19:27:09 - INFO - __main__ -   Epoch 9, Batch 1750, Avg Loss: 11.9149\n",
            "03/25/2025 19:27:16 - INFO - __main__ -   Epoch 9, Batch 1800, Avg Loss: 11.8886\n",
            "03/25/2025 19:27:23 - INFO - __main__ -   Epoch 9, Batch 1850, Avg Loss: 11.8685\n",
            "03/25/2025 19:27:30 - INFO - __main__ -   Epoch 9, Batch 1900, Avg Loss: 11.8831\n",
            "03/25/2025 19:27:37 - INFO - __main__ -   Epoch 9, Batch 1950, Avg Loss: 11.8834\n",
            "03/25/2025 19:27:44 - INFO - __main__ -   Epoch 9, Batch 2000, Avg Loss: 11.8941\n",
            "03/25/2025 19:27:51 - INFO - __main__ -   Epoch 9, Batch 2050, Avg Loss: 11.9031\n",
            "03/25/2025 19:27:58 - INFO - __main__ -   Epoch 9, Batch 2100, Avg Loss: 11.9004\n",
            "03/25/2025 19:28:05 - INFO - __main__ -   Epoch 9, Batch 2150, Avg Loss: 11.8805\n",
            "03/25/2025 19:28:12 - INFO - __main__ -   Epoch 9, Batch 2200, Avg Loss: 11.8661\n",
            "03/25/2025 19:28:19 - INFO - __main__ -   Epoch 9, Batch 2250, Avg Loss: 11.8506\n",
            "03/25/2025 19:28:26 - INFO - __main__ -   Epoch 9, Batch 2300, Avg Loss: 11.8457\n",
            "03/25/2025 19:28:33 - INFO - __main__ -   Epoch 9, Batch 2350, Avg Loss: 11.8391\n",
            "03/25/2025 19:28:40 - INFO - __main__ -   Epoch 9, Batch 2400, Avg Loss: 11.8248\n",
            "03/25/2025 19:28:47 - INFO - __main__ -   Epoch 9, Batch 2450, Avg Loss: 11.8161\n",
            "03/25/2025 19:28:54 - INFO - __main__ -   Epoch 9, Batch 2500, Avg Loss: 11.8035\n",
            "03/25/2025 19:29:01 - INFO - __main__ -   Epoch 9, Batch 2550, Avg Loss: 11.8039\n",
            "03/25/2025 19:29:08 - INFO - __main__ -   Epoch 9, Batch 2600, Avg Loss: 11.8911\n",
            "03/25/2025 19:29:15 - INFO - __main__ -   Epoch 9, Batch 2650, Avg Loss: 11.9261\n",
            "03/25/2025 19:29:22 - INFO - __main__ -   Epoch 9, Batch 2700, Avg Loss: 11.9120\n",
            "03/25/2025 19:29:29 - INFO - __main__ -   Epoch 9, Batch 2750, Avg Loss: 11.8971\n",
            "03/25/2025 19:29:36 - INFO - __main__ -   Epoch 9, Batch 2800, Avg Loss: 11.8821\n",
            "03/25/2025 19:29:43 - INFO - __main__ -   Epoch 9, Batch 2850, Avg Loss: 11.8721\n",
            "03/25/2025 19:29:50 - INFO - __main__ -   Epoch 9, Batch 2900, Avg Loss: 11.8562\n",
            "03/25/2025 19:29:57 - INFO - __main__ -   Epoch 9, Batch 2950, Avg Loss: 11.8375\n",
            "03/25/2025 19:30:04 - INFO - __main__ -   Epoch 9, Batch 3000, Avg Loss: 11.8237\n",
            "03/25/2025 19:30:11 - INFO - __main__ -   Epoch 9, Batch 3050, Avg Loss: 11.8142\n",
            "03/25/2025 19:30:18 - INFO - __main__ -   Epoch 9, Batch 3100, Avg Loss: 11.7982\n",
            "03/25/2025 19:30:25 - INFO - __main__ -   Epoch 9, Batch 3150, Avg Loss: 11.7883\n",
            "03/25/2025 19:30:32 - INFO - __main__ -   Epoch 9, Batch 3200, Avg Loss: 11.7803\n",
            "03/25/2025 19:30:39 - INFO - __main__ -   Epoch 9, Batch 3250, Avg Loss: 11.7739\n",
            "03/25/2025 19:30:46 - INFO - __main__ -   Epoch 9, Batch 3300, Avg Loss: 11.8135\n",
            "03/25/2025 19:30:53 - INFO - __main__ -   Epoch 9, Batch 3350, Avg Loss: 11.8191\n",
            "03/25/2025 19:31:00 - INFO - __main__ -   Epoch 9, Batch 3400, Avg Loss: 11.8404\n",
            "03/25/2025 19:31:07 - INFO - __main__ -   Epoch 9, Batch 3450, Avg Loss: 11.8355\n",
            "03/25/2025 19:31:14 - INFO - __main__ -   Epoch 9, Batch 3500, Avg Loss: 11.8442\n",
            "03/25/2025 19:31:21 - INFO - __main__ -   Epoch 9, Batch 3550, Avg Loss: 11.8521\n",
            "03/25/2025 19:31:28 - INFO - __main__ -   Epoch 9, Batch 3600, Avg Loss: 11.8580\n",
            "03/25/2025 19:31:35 - INFO - __main__ -   Epoch 9, Batch 3650, Avg Loss: 11.8733\n",
            "03/25/2025 19:31:42 - INFO - __main__ -   Epoch 9, Batch 3700, Avg Loss: 11.8864\n",
            "03/25/2025 19:31:49 - INFO - __main__ -   Epoch 9, Batch 3750, Avg Loss: 11.8821\n",
            "03/25/2025 19:31:56 - INFO - __main__ -   Epoch 9, Batch 3800, Avg Loss: 11.8846\n",
            "03/25/2025 19:32:03 - INFO - __main__ -   Epoch 9, Batch 3850, Avg Loss: 11.8923\n",
            "03/25/2025 19:32:10 - INFO - __main__ -   Epoch 9, Batch 3900, Avg Loss: 11.8968\n",
            "03/25/2025 19:32:17 - INFO - __main__ -   Epoch 9, Batch 3950, Avg Loss: 11.8996\n",
            "03/25/2025 19:32:24 - INFO - __main__ -   Epoch 9, Batch 4000, Avg Loss: 11.8962\n",
            "03/25/2025 19:32:31 - INFO - __main__ -   Epoch 9, Batch 4050, Avg Loss: 11.8983\n",
            "03/25/2025 19:32:38 - INFO - __main__ -   Epoch 9, Batch 4100, Avg Loss: 11.8970\n",
            "03/25/2025 19:32:45 - INFO - __main__ -   Epoch 9, Batch 4150, Avg Loss: 11.8953\n",
            "03/25/2025 19:32:52 - INFO - __main__ -   Epoch 9, Batch 4200, Avg Loss: 11.9114\n",
            "03/25/2025 19:32:59 - INFO - __main__ -   Epoch 9, Batch 4250, Avg Loss: 11.9196\n",
            "03/25/2025 19:33:06 - INFO - __main__ -   Epoch 9, Batch 4300, Avg Loss: 11.9296\n",
            "03/25/2025 19:33:13 - INFO - __main__ -   Epoch 9, Batch 4350, Avg Loss: 11.9247\n",
            "03/25/2025 19:33:20 - INFO - __main__ -   Epoch 9, Batch 4400, Avg Loss: 11.9244\n",
            "03/25/2025 19:33:27 - INFO - __main__ -   Epoch 9, Batch 4450, Avg Loss: 11.9319\n",
            "03/25/2025 19:33:34 - INFO - __main__ -   Epoch 9, Batch 4500, Avg Loss: 11.9337\n",
            "03/25/2025 19:33:41 - INFO - __main__ -   Epoch 9, Batch 4550, Avg Loss: 11.9391\n",
            "03/25/2025 19:33:48 - INFO - __main__ -   Epoch 9, Batch 4600, Avg Loss: 11.9366\n",
            "03/25/2025 19:33:55 - INFO - __main__ -   Epoch 9, Batch 4650, Avg Loss: 11.9293\n",
            "03/25/2025 19:34:02 - INFO - __main__ -   Epoch 9, Batch 4700, Avg Loss: 11.9304\n",
            "03/25/2025 19:34:09 - INFO - __main__ -   Epoch 9, Batch 4750, Avg Loss: 11.9305\n",
            "03/25/2025 19:34:16 - INFO - __main__ -   Epoch 9, Batch 4800, Avg Loss: 11.9466\n",
            "03/25/2025 19:34:23 - INFO - __main__ -   Epoch 9, Batch 4850, Avg Loss: 11.9481\n",
            "03/25/2025 19:34:30 - INFO - __main__ -   Epoch 9, Batch 4900, Avg Loss: 11.9543\n",
            "03/25/2025 19:34:37 - INFO - __main__ -   Epoch 9, Batch 4950, Avg Loss: 11.9529\n",
            "03/25/2025 19:34:44 - INFO - __main__ -   Epoch 9, Batch 5000, Avg Loss: 11.9567\n",
            "03/25/2025 19:34:51 - INFO - __main__ -   Epoch 9, Batch 5050, Avg Loss: 11.9551\n",
            "03/25/2025 19:34:58 - INFO - __main__ -   Epoch 9, Batch 5100, Avg Loss: 11.9619\n",
            "03/25/2025 19:35:05 - INFO - __main__ -   Epoch 9, Batch 5150, Avg Loss: 11.9751\n",
            "03/25/2025 19:35:12 - INFO - __main__ -   Epoch 9, Batch 5200, Avg Loss: 11.9727\n",
            "03/25/2025 19:35:19 - INFO - __main__ -   Epoch 9, Batch 5250, Avg Loss: 11.9632\n",
            "03/25/2025 19:35:27 - INFO - __main__ -   Epoch 9, Batch 5300, Avg Loss: 11.9608\n",
            "03/25/2025 19:35:34 - INFO - __main__ -   Epoch 9, Batch 5350, Avg Loss: 11.9532\n",
            "03/25/2025 19:35:41 - INFO - __main__ -   Epoch 9, Batch 5400, Avg Loss: 11.9524\n",
            "03/25/2025 19:35:48 - INFO - __main__ -   Epoch 9, Batch 5450, Avg Loss: 11.9536\n",
            "03/25/2025 19:35:55 - INFO - __main__ -   Epoch 9, Batch 5500, Avg Loss: 11.9494\n",
            "03/25/2025 19:36:02 - INFO - __main__ -   Epoch 9, Batch 5550, Avg Loss: 11.9498\n",
            "03/25/2025 19:36:09 - INFO - __main__ -   Epoch 9, Batch 5600, Avg Loss: 11.9451\n",
            "03/25/2025 19:36:16 - INFO - __main__ -   Epoch 9, Batch 5650, Avg Loss: 11.9540\n",
            "03/25/2025 19:36:23 - INFO - __main__ -   Epoch 9, Batch 5700, Avg Loss: 11.9509\n",
            "03/25/2025 19:36:30 - INFO - __main__ -   Epoch 9, Batch 5750, Avg Loss: 11.9413\n",
            "03/25/2025 19:36:37 - INFO - __main__ -   Epoch 9, Batch 5800, Avg Loss: 11.9359\n",
            "03/25/2025 19:36:44 - INFO - __main__ -   Epoch 9, Batch 5850, Avg Loss: 11.9321\n",
            "03/25/2025 19:36:51 - INFO - __main__ -   Epoch 9, Batch 5900, Avg Loss: 11.9266\n",
            "03/25/2025 19:36:58 - INFO - __main__ -   Epoch 9, Batch 5950, Avg Loss: 11.9286\n",
            "03/25/2025 19:37:05 - INFO - __main__ -   Epoch 9, Batch 6000, Avg Loss: 11.9368\n",
            "03/25/2025 19:37:12 - INFO - __main__ -   Epoch 9, Batch 6050, Avg Loss: 11.9338\n",
            "03/25/2025 19:37:19 - INFO - __main__ -   Epoch 9, Batch 6100, Avg Loss: 11.9320\n",
            "03/25/2025 19:37:26 - INFO - __main__ -   Epoch 9, Batch 6150, Avg Loss: 11.9304\n",
            "03/25/2025 19:37:33 - INFO - __main__ -   Epoch 9, Batch 6200, Avg Loss: 11.9300\n",
            "03/25/2025 19:37:40 - INFO - __main__ -   Epoch 9, Batch 6250, Avg Loss: 11.9327\n",
            "03/25/2025 19:37:47 - INFO - __main__ -   Epoch 9, Batch 6300, Avg Loss: 11.9243\n",
            "03/25/2025 19:37:54 - INFO - __main__ -   Epoch 9, Batch 6350, Avg Loss: 11.9289\n",
            "03/25/2025 19:38:01 - INFO - __main__ -   Epoch 9, Batch 6400, Avg Loss: 11.9306\n",
            "03/25/2025 19:38:08 - INFO - __main__ -   Epoch 9, Batch 6450, Avg Loss: 11.9313\n",
            "03/25/2025 19:38:15 - INFO - __main__ -   Epoch 9, Batch 6500, Avg Loss: 11.9274\n",
            "03/25/2025 19:38:22 - INFO - __main__ -   Epoch 9, Batch 6550, Avg Loss: 11.9453\n",
            "03/25/2025 19:38:29 - INFO - __main__ -   Epoch 9, Batch 6600, Avg Loss: 11.9451\n",
            "03/25/2025 19:38:36 - INFO - __main__ -   Epoch 9, Batch 6650, Avg Loss: 11.9466\n",
            "03/25/2025 19:38:43 - INFO - __main__ -   Epoch 9, Batch 6700, Avg Loss: 11.9507\n",
            "03/25/2025 19:38:50 - INFO - __main__ -   Epoch 9, Batch 6750, Avg Loss: 11.9543\n",
            "03/25/2025 19:38:57 - INFO - __main__ -   Epoch 9, Batch 6800, Avg Loss: 11.9551\n",
            "03/25/2025 19:39:04 - INFO - __main__ -   Epoch 9, Batch 6850, Avg Loss: 11.9576\n",
            "03/25/2025 19:39:11 - INFO - __main__ -   Epoch 9, Batch 6900, Avg Loss: 11.9604\n",
            "03/25/2025 19:39:18 - INFO - __main__ -   Epoch 9, Batch 6950, Avg Loss: 11.9571\n",
            "03/25/2025 19:39:25 - INFO - __main__ -   Epoch 9, Batch 7000, Avg Loss: 11.9634\n",
            "03/25/2025 19:39:32 - INFO - __main__ -   Epoch 9, Batch 7050, Avg Loss: 11.9691\n",
            "03/25/2025 19:39:39 - INFO - __main__ -   Epoch 9, Batch 7100, Avg Loss: 11.9653\n",
            "03/25/2025 19:39:46 - INFO - __main__ -   Epoch 9, Batch 7150, Avg Loss: 11.9659\n",
            "03/25/2025 19:39:53 - INFO - __main__ -   Epoch 9, Batch 7200, Avg Loss: 11.9698\n",
            "03/25/2025 19:40:00 - INFO - __main__ -   Epoch 9, Batch 7250, Avg Loss: 11.9796\n",
            "03/25/2025 19:40:07 - INFO - __main__ -   Epoch 9, Batch 7300, Avg Loss: 11.9829\n",
            "03/25/2025 19:40:14 - INFO - __main__ -   Epoch 9, Batch 7350, Avg Loss: 11.9823\n",
            "03/25/2025 19:40:21 - INFO - __main__ -   Epoch 9, Batch 7400, Avg Loss: 11.9843\n",
            "03/25/2025 19:40:28 - INFO - __main__ -   Epoch 9, Batch 7450, Avg Loss: 11.9941\n",
            "03/25/2025 19:40:35 - INFO - __main__ -   Epoch 9, Batch 7500, Avg Loss: 11.9920\n",
            "03/25/2025 19:40:42 - INFO - __main__ -   Epoch 9, Batch 7550, Avg Loss: 11.9872\n",
            "03/25/2025 19:40:49 - INFO - __main__ -   Epoch 9, Batch 7600, Avg Loss: 11.9918\n",
            "03/25/2025 19:40:56 - INFO - __main__ -   Epoch 9, Batch 7650, Avg Loss: 11.9867\n",
            "03/25/2025 19:41:03 - INFO - __main__ -   Epoch 9, Batch 7700, Avg Loss: 11.9830\n",
            "03/25/2025 19:41:10 - INFO - __main__ -   Epoch 9, Batch 7750, Avg Loss: 12.0022\n",
            "03/25/2025 19:41:17 - INFO - __main__ -   Epoch 9, Batch 7800, Avg Loss: 12.0067\n",
            "03/25/2025 19:41:24 - INFO - __main__ -   Epoch 9, Batch 7850, Avg Loss: 12.0060\n",
            "03/25/2025 19:41:27 - INFO - __main__ -   Epoch 9 completed, Avg Loss: 12.0076\n",
            "03/25/2025 19:41:32 - INFO - __main__ -   Hashing module training completed\n",
            "03/25/2025 19:41:33 - INFO - __main__ -   Skipping evaluation\n"
          ]
        }
      ],
      "source": [
        "DATA_DIR = \"/content/data\"\n",
        "MODEL_DIR = f\"/content/drive/MyDrive/CoSHC/CodeBERT/models/{LANG}_baseline/checkpoint-best-mrr\"\n",
        "OUT_DIR = \"/content/drive/MyDrive/CoSHC/CodeBERT\"\n",
        "\n",
        "!python run_coshc.py \\\n",
        "    --model_name_or_path={MODEL_DIR}/model.bin \\\n",
        "    --config_name=microsoft/codebert-base \\\n",
        "    --tokenizer_name=microsoft/codebert-base \\\n",
        "    --output_dir={OUT_DIR}/models/{LANG}_coshc \\\n",
        "    --train_data_file={DATA_DIR}/{LANG}/train.jsonl \\\n",
        "    --eval_data_file={DATA_DIR}/{LANG}/valid.jsonl \\\n",
        "    --test_data_file={DATA_DIR}/{LANG}/test.jsonl \\\n",
        "    --codebase_file={DATA_DIR}/{LANG}/codebase.jsonl \\\n",
        "    --do_train \\\n",
        "    --hash_dim=128 \\\n",
        "    --do_embed \\\n",
        "    --embedding_dir={OUT_DIR}/code_embedding/{LANG} \\\n",
        "    --num_clusters=10 \\\n",
        "    --total_recall=100 \\\n",
        "    --beta=0.6 \\\n",
        "    --eta=0.4 \\\n",
        "    --hash_epochs=10 \\\n",
        "    --class_epochs=5 \\\n",
        "    --train_batch_size=32 \\\n",
        "    --eval_batch_size=64 \\\n",
        "    --learning_rate=1e-5 \\\n",
        "    --seed=123456"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YDepeGBWwA5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vsh7cFOPyFZi"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}